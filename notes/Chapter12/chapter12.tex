%!TEX root = ../notebook.tex
% Chapter 12

\graphicspath{{Chapter12/Figs/}}

\chapter{Graphical Models \& Markov Network}
\label{chapter12}



\section{Graph Definitions}
\label{section12.1}

\subsection{Graph}

\paragraph{Graph}

A graph consists of nodes (vertices) and undirected or directed links (edges) between nodes.

\paragraph{Path}

A path from $X_i$ to $X_j$ is a sequence of connected nodes starting at $X_i$ and ending at $X_j$.

\subsection{Directed Graph}

\paragraph{Directed Graphs}

Graphs that all the edges are directed.
\begin{figure*}[h]
	\centering
	\includegraphics*[width=0.5\textwidth]{fig1.png}
\end{figure*}

\paragraph{Directed Acyclic Graph (DAG)}

Graph in which by following the direction of the arrows a node will never be visited more than once.

\paragraph{Parents and Children}

$X_i$ is a parent of $X_j$ if there is a link from $X_i$ to $X_j$. $X_i$ is a child of $X_j$ if there is a link from $X_j$ to $X_i$.

\paragraph{Ancestors and Descendants}

The ancestors of a node $X_i$ are the nodes with a directed path ending at $X_i$. The descendants of $X_i$ are the nodes with a directed path beginning at $X_i$.

\subsection{Undirected Graph}	

\paragraph{Undirected Graph}

Graph that all the edges are undirected.
\begin{figure*}[h]
	\centering
	\includegraphics*[width=0.25\textwidth]{fig2.png}
\end{figure*}

\paragraph{Clique}

A clique is a fully connected subset of nodes. ($X_1$, $X_2$, $X_4$) forms a (non-maximal) clique.

\paragraph{Maximal Clique}

Clique which is not a subset of a larger clique. ($X_1$, $X_2$, $X_3$, $X_4$) and ($X_2$, $X_3$, $X_5$) are both maximal cliques.

\subsection{Connectivity}

\paragraph{Connected Graph}

There is a path between every pair of vertices.

\paragraph{Connected Components}

In a non-connected graph, the connected components are the connected-subgraphs. ($X_1$, $X_2$, $X_4$) and ($X_3$, $X_5$) are the two connected components.
\begin{figure*}[h]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics*[width=0.5\textwidth]{fig3.png}
		\caption*{Connected Graph}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics*[width=0.5\textwidth]{fig4.png}
		\caption*{Connected Components}
	\end{subfigure}
\end{figure*}

\subsection{Connectedness}

\paragraph{Singly-connected}

There is only one path from any node $a$ to another node $b$.

\paragraph{Multiply-connected}

A graph is multiply-connected if it is not singly-connected.
\begin{figure*}[h]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics*[width=0.5\textwidth]{fig5.png}
		\caption*{Singly-connected}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics*[width=0.5\textwidth]{fig6.png}
		\caption*{Multiply-connected}
	\end{subfigure}
\end{figure*}



\section{Belief Networks}
\label{section12.2}

\subsection{Definition}

A belief network is a directed acyclic graph in which each node is associated with the conditional probability of the node given its parents. The joint distribution is obtained by taking the product of the conditional probabilities.

\begin{minipage}{0.4\textwidth}
	\centering
	\includegraphics*[width=0.6\textwidth]{fig7.png}
\end{minipage}
\begin{minipage}{0.6\textwidth}
	$\P{A,B,C,D,E}=\\ \P{A}\P{B}\P{C|A,B}\P{D|C}\P{E|B,C}$
\end{minipage}

\subsection{Uncertain Evidence}

\paragraph{Definition}

In soft/uncertain evidence the variable is in more than one state, with the strength of out belief about each state being given by probabilities. For example, if $y$ has the states $\dom(y)=\{red,blue,green\}$, the vector (0.6, 0.1, 0.3) could represent the probabilities of the respective states.

\paragraph{Hard Evidence}

We are certain that a variable is in a particular state. In this state, all the probability mass is in one of the vector components, (0, 0, 1).

\paragraph{Inference}

Inference with soft-evidence can be achieved using Bayes' rule. Writing the soft-evidence as $\ti{y}$, we have $\P{x|\ti{y}}=\sum_y\P{x|y}\P{y|\ti{y}}$, where $\P{y=i|\ti{y}}$ represents the probability that $y$ is in state $i$ under the soft-evidence.

\paragraph{Jeffrey's Rule}

For variables $x$, $y$ and $P_1(x,y)$, how do we form a joint distribution given soft-evidence $\ti{y}$? (a) From the conditional we first define $P_1(x|y)=\frac{P_1(x,y)}{\sum_xP_1(x,y)}$. (b) Define the joint. The soft-evidence $\P{y|\ti{y}}$ then defines a new joint distribution $P_2(x,y|\ti{y})=P_1(x|y)P_1(y|\ti{y})$. One can therefore view soft-evidence as defining a new joint distribution. We use a dashed circle to represent a variable in an uncertain state.
\begin{figure*}[h]
	\centering
	\includegraphics*[width=0.2\textwidth]{fig8.png}
\end{figure*}

\subsection{Independence}

\subsubsection*{Conditionally Independent}

\begin{figure*}[h]
	\centering
	\includegraphics*[width=\textwidth]{fig9.png}
\end{figure*}
In (a), (b) and (c), A, B are conditionally independent given C.
\begin{enumerate}[label=(\alph*)]
	\item $\P{A,B|C}=\frac{\P{A,B,C}}{\P{C}}=\frac{\P{A|C}\P{B|C}\P{C}}{\P{C}}=\P{A|C}\P{B|C}$
	\item $\P{A,B|C}=\frac{\P{A,B,C}}{\P{C}}=\frac{\P{A}\P{C|A}\P{B|C}}{\P{C}}=\frac{\P{A,C}\P{B|C}}{\P{C}}=\P{A|C}\P{B|C}$
	\item $\P{A,B|C}=\frac{\P{A,B,C}}{\P{C}}=\frac{\P{A|C}\P{C|B}\P{B}}{\P{C}}=\frac{\P{A|C}\P{B,C}}{\P{C}}=\P{A|C}\P{B|C}$
\end{enumerate}
In (d) the variables A, B are conditionally dependent given C, $\P{A,B|C}\prop\P{C|A,B}\P{A}\P{B}$.

\subsubsection*{Marginally Dependent}

\begin{figure*}[h]
	\centering
	\includegraphics*[width=\textwidth]{fig10.png}
\end{figure*}
In (a), (b) and (c), the variables A, B are marginally dependent.
In (d) the variables A, B are marginally independent.
\begin{align*}
\P{A,B}=\sum_C\P{A,B,C}=\sum_C\P{A}\P{B}\P{C|A,B}=\P{A}\P{B}
\end{align*}

\subsubsection*{Colliders}

\begin{minipage}{0.4\textwidth}
	\centering
	\includegraphics*[width=0.6\textwidth]{fig11.png}
\end{minipage}
\begin{minipage}{0.6\textwidth}
	If C has more than one incoming link, then $A\nindep B|C$. In this case C in called \textit{collider}.
\end{minipage}
\begin{minipage}{0.4\textwidth}
	\centering
	\includegraphics*[width=0.6\textwidth]{fig12.png}
\end{minipage}
\begin{minipage}{0.6\textwidth}
	If C has at most one incoming link, then $A\indep B|C$ and $A\nindep B$. In this case C is called \textit{non-collider}.
\end{minipage}

\subsection{General Rule for Independence in Belief Networks}

Given three sets of nodes $\cX$, $\cY$, $\cC$, if all paths from any element of $\cX$ to any element of $\cY$ are blocked by $\cC$, then $\cX$ and $\cY$ are conditionally independent given $\cC$.
A path $\cP$ is blocked by $\cC$ if at least one of the following conditions is satisfied:
\begin{enumerate}
	\item There is a collider in the path $\cP$ such that neither the collider nor any of its descendants is in the conditioning set $\cC$.
	\item There is a non-collider in the path $\cP$ that is in the conditioning set $\cC$.
\end{enumerate}

\paragraph{Independence of $\cX$ and $\cY$}

When the conditioning set is empty $\cC=\emptyset$, then a path $\cP$ from an element of $\cX$ to an element of $\cY$ is blocked if there is a collider on the path. Hence $\cX$ and $\cY$ are independent if every path from an element of $\cX$ to any element of $\cY$ has a collider.

\paragraph{d-connected}

We use the term that $\cX$ and $\cY$ are ``d-connected'' by $\cZ$ if there is any path from $\cX$ to $\cY$ that is not blocked by $\cZ$. If $\cZ$ is the empty set then we just say that $\cX$ and $\cY$ are d-connected.

\paragraph{Separation and Independence}

Note first that d-separation and connection are properties of the graph (not of the distribution). d-separation implies that $\cX\indep \cY|\cZ$, but d-connection does not necessarily imply conditional dependence. That is, for any distribution in which $\cX$ and $\cY$ are ``d-separated'' by $\cZ$, then no matter what the settings of the conditional tables are, then conditional independence holds, namely $\cX\indep \cY|\cZ$.

\subsection{Markov Equivalence}

\paragraph{Skeleton}

Formed from a graph by removing the arrows.

\paragraph{Immorality}

An immorality in a DAG is a configuration of three nodes, A,B,C such that C is a child of both A and B, with A and B not directly connected.

\paragraph{Markov Equivalence}

Markov equivalence
Two graphs represent the same set of independence assumptions if and only if they have the same skeleton and the same set of immoralities.

\section{Markov Networks}

\subsection{Definition}
A Markov Network is an undirected graph in which there is a potential (non-negative function) $\ps$ defined on each maximal clique.

\begin{minipage}{0.4\textwidth}
	\centering
	\includegraphics*[width=0.5\textwidth]{fig13.png}
\end{minipage}
\begin{minipage}{0.6\textwidth}
	\begin{align*}
	\P{A,B,C,D,E}=\frac{1}{Z}&\ps(A,C)\ps(C,D)\ps(B,C,E) \\
	Z=\sum_{A,B,C,D,E}&\ps(A,C)\ps(C,D)\ps(B,C,E)
	\end{align*}
\end{minipage}

\subsection{Examples}

\subsubsection*{Binary Image}

\begin{minipage}{0.4\textwidth}
	\centering
	\includegraphics*[width=0.8\textwidth]{fig14.png}
\end{minipage}
\begin{minipage}{0.6\textwidth}
	\begin{align*}
	&X=\{X_i,i=1,\dotsc,D\}\ X_i\in\{-1,1\}: \text{clean pixel} \\
	&Y=\{Y_i,i=1,\dotsc,D\}\ Y_i\in\{-1,1\}: \text{corrupted pixel} \\
	&\ph(Y_i,X_i)=e^{\ga X_i Y_i}: \text{encourage $Y_i$ and $X_i$ to be similar} \\
	&\ps(X_i,X_j)=e^{\be X_i X_j}: \text{encourage the image to be smooth} \\
	&\P{X,Y}\prop\left[\prod_{i=1}^D\ph(Y_i,X_i)\right]\left[\prod_{i\sim j}\ps(X_i,X_j)\right]
	\end{align*}
\end{minipage}

\subsubsection*{Boltzmann Machine}

\subsubsection*{The Ising Model}

\subsection{Independence}

\subsection{Expressiveness of Markov and Belief Networks}

\subsection{Factor Graphs}

\section{Markov Chains}

\section{Hidden Markov Models}

\section{Sampling with Monte Carlo Markov Chains (MCMC)}