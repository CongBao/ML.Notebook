%!TEX root = ../notebook.tex
% Chapter 2

\chapter{Mathematics Basics}
\label{chapter2}



\section{Probability}
\label{section2.1}

\subsection{Basic Rules}

\paragraph{Three Axioms of Probability}

Let $\Om$ be a sample space. A probability assigns a real number $\P{X}$ to each event $X \subseteq \Om$ in such a way that
\begin{enumerate}
	\item $\P{X} \geq 0, \forall X$
	\item If $X_1, X_2, \dotsc$ are pairwise disjoint events ($X_1 \cap X_2=\emptyset,\ i \ne j,\ i,j=1,2,\dotsc$), then $\P{\bigcup_{i=1}^{\infty}X_i}=\sum_{i=1}^{\infty}\P{X_i}$. (This property is called countable additivity.)
	\item $\P{\Om}=1$
\end{enumerate}

\paragraph{Joint Probability}

The probability both event A and B occur. $\P{X,Y}=\P{X \cap Y}$.

\paragraph{Marginalization}

The probability distribution of any variable in a joint distribution can be recovered by integrating (or summing) over the other variables.
\begin{enumerate}
	\item For continuous r.v. $\P{x}=\int\P{x,y}dy$ ; $\P{y}=\int\P{x,y}dx$.
	\item For discrete r.v. $\P{x}=\sum_y\P{x,y}$ ; $\P{y}=\sum_x\P{x,y}$.
	\item For mixed r.v. $\P{x,y}=\sum_w\int\P{w,x,y,z}dz$, where $w$ is discrete and $z$ is continuous.
\end{enumerate}

\paragraph{Conditional Probibility}

$\P{X=x|Y=y}$ is the probability $X=x$ occurs given the knowledge $Y=y$ occurs. Conditional probability can be extracted from joint probability that
\begin{align*}
	\P{x|y=y^{*}}=\frac{\P{x,y=y^{*}}}{\int\P{x,y=y^{*}}dx}=\frac{\P{x,y=y^{*}}}{\P{y=y^{*}}}
\end{align*}

Usually, the formula is written as $\P{x|y}=\frac{\P{x,y}}{\P{y}}$.

\paragraph{Product Rule}

The formula can be rearranged as $\P{x,y}=\P{x|y}\P{y}=\P{y|x}\P{x}$. In case of multiple variables
\begin{align*}
	\P{w,x,y,z}&=\P{w,x,y|z}\P{z} \\
			   &=\P{w,x|y,z}\P{y|z}\P{z} \\
			   &=\P{w|x,y,z}\P{x|y,z}\P{y|z}\P{z}
\end{align*}

\paragraph{Independence}

If two variables $x$ and $y$ are independent, then r.v. $x$ tells nothing about r.v. $y$ (and vice-versa)
\begin{align*}
	\P{x|y}&=\P{x} \\
	\P{y|x}&=\P{y} \\
	\P{x,y}&=\P{x}\P{y}
\end{align*}

\paragraph{Baye's Rule}

By rearranging formula in Product Rule, we have
\begin{align*}
	\P{y|x}&=\frac{\P{x|y}\P{y}}{\P{x}} \\
		   &=\frac{\P{x|y}\P{y}}{\int\P{x,y}dy} \\
		   &=\frac{\P{x|y}\P{y}}{\int\P{x|y}\P{y}dy}
\end{align*}

\paragraph{Expectation}

Expectation tells us the expected or average value of some function $f(x)$, taking into account the distribution of $x$.
\begin{align*}
	\E{f(x)}&=\sum_x f(x)\P{x} \\
	\E{f(x)}&=\int f(x)\P{x}dx
\end{align*}
Definition in two dimensions: $\E{f(x,y)}=\iint f(x,y)\P{x,y}dx\ dy$
\begin{table*}[!h]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Function $f(\bullet)$ & Expectation \\
		\hline\hline
		$x^k$ & $k^{th}$ moment about zero \\
		\hline
		$(x-\mu_x)^k$ & $k^{th}$ moment about the mean \\
		\hline
	\end{tabular}
	\
	\begin{tabular}{|c|c|}
		\hline
		Function $f(\bullet)$ & Expectation \\
		\hline\hline
		$x$ & mean, $\mu_x$ \\
		\hline
		$(x-\mu_x)^2$ & variance \\
		\hline
		$(x-\mu_x)^3$ & skew \\
		\hline
		$(x-\mu_x)^4$ & kurtosis \\
		\hline
		$(x-\mu_x)(x-\mu_y)$ & covariance of $x$ and $y$ \\
		\hline
	\end{tabular}
\end{table*}

Besides, Expectation has the following four rules
	\begin{enumerate}
		\item Expected value of a constant is the constant $\E{\kappa}=\kappa$.
		\item Expected value of constant times function is constant times excepted value of function $\E{kf(x)}=k\E{f(x)}$.
		\item Expectation of sum of functions is sum of expectation of functions \\ $\E{f(x)+g(y)}=\E{f(x)}+\E{g(x)}$.
		\item Expectation of product of functions in variables $x$ and $y$ is product of expectations of functions if $x$ and $y$ are independent $\E{f(x)g(y)}=\E{f(x)}\E{g(y)}, x \indep y$.
	\end{enumerate}

\subsection{Common Probability Distributions}

\paragraph{Bernoulli}

Bernoulli distribution describes situation where only two possible outcomes $y=0 / y=1$ or failure/success exist.
\begin{enumerate}
	\item $\P{x}=\Bern_x[\la]=\la^x(1-\la)^{1-x}$
	\item $x \in \{0,1\}$; $\la \in [0,1]$
	\item $\E{x}=\la$, $\V{x}=\la(1-\la)$
\end{enumerate}

\paragraph{Binomial}

Binomial distribution describes $n$ independent Bernoulli trials.
\begin{enumerate}
	\item $\P{x}=\Bin_x[n,\la]=\binom{n}{x}\la^x(1-\la)^{1-x}$
	\item $x\in\bN$; $n\in\bN$, $\la\in[0,1]$
	\item $\E{x}=n\la$, $\V{x}=n\la(1-\la)$
\end{enumerate}

\paragraph{Geometric}

Geometric distribution describes the number of independent Bernoulli trails until we record the first success.
\begin{enumerate}
	\item $\P{x}=\Geom_x[\la]=\la(1-\la)^{x-1}$
	\item $x\in\bZ_+$; $\la\in[0,1]$
	\item $\E{x}=\frac{1}{\la}$, $\V{x}=\frac{1-\la}{\la^2}$
\end{enumerate}

\paragraph{Negative Binomial}

Negative Binomial distribution describes the number of independent Bernoulli trails until we record the $r^\text{th}$ success.
\begin{enumerate}
	\item $\P{x}=\NBin_x[r,\la]=\binom{x+r-1}{x}\la^x(1-\la)^r$
	\item $x\in\bN$; $r\in\bZ_+$, $\la\in[0,1]$
	\item $\E{x}=\frac{r\la}{1-\la}$, $\V{x}=\frac{r\la}{(1-\la)^2}$
\end{enumerate} 

\paragraph{Beta}

Beta distribution is a common conjugate prior to Bernoulli, Binomial, Geometric, and Negative Binomial distributions.
\begin{enumerate}
	\item $\P{\la}=\Beta_\la[\al,\be]=\frac{\Ga(\al+\be)}{\Ga(\al)\Ga(\be)}\la^{\al-1}(1-\la)^{\be-1}$
	\item $\la \in \bR$; $\al \in \bR_+$,  $\be \in \bR_+$
	\item $\E{\la}=\frac{\al}{\al+\be}$, $\V{\la}=\frac{\al\be}{(\al+\be)^2(\al+\be+1)}$
\end{enumerate}

\paragraph{Poisson}

Poisson distribution describes the rate $\mu$ an event takes place rarely on an interval or surface over time or space.
\begin{enumerate}
	\item $\P{x}=\Poi_x[\la]=\frac{\la^x}{x!}e^{-\la}$
	\item $x\in\bN$; $\la\in\bR_+$
	\item $\E{x}=\la$, $\V{x}=\la$
\end{enumerate}

\paragraph{Exponential}

Exponential distribution describes the continuous time between events in Poisson process.
\begin{enumerate}
	\item $\P{x}=\Exp_x[\la]=\la e^{-\la x}$
	\item $x\in\bN$; $\la\in\bZ_+$
	\item $\E{x}=\frac{1}{\la}$, $\V{x}=\frac{1}{\la^2}$
\end{enumerate}

\paragraph{Gamma}

Gamma distribution describes the continuous time until the $\al^\text{th}$ event in Poisson process takes place. It is also a common conjugate prior to Poisson and Exponential distributions.
\begin{enumerate}
	\item $\P{\la}=\Gam_\la[\al,\be]=\frac{\be^\al}{\Ga(\al)}\la^{\al-1}e^{-\be\la}$
	\item $\la\in\bZ_+$; $\al\in\bZ_+$, $\be\in\bZ_+$
	\item $\E{\la}=\frac{\al}{\be}$, $\V{\la}=\frac{\al}{\be^2}$
\end{enumerate}

\begin{table*}[!h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& \multicolumn{2}{|c|}{Distribution} \\
		\hline
		Random Variables & Discrete time & continuous time \\
		\hline\hline
		No. of events & Binomial & Poisson \\
		\hline
		Time till first event & Geometric & Exponential \\
		\hline
		Time till $r^\text{th}$ event & Negative Binomial & Gamma \\
		\hline
	\end{tabular}
\end{table*}

\paragraph{Categorical}

Categorical distribution describes situation with K possible outcomes.
\begin{enumerate}
	\item $\P{x}=\Cat_x[\vec{\la}]$, $\P{x=k}=\la_k$,  $\P{\vec{x}=\vec{e}_k}=\prod_{j=1}^K\la_j^{x_j}=\la_k$
	\item $x \in \{1,2,\dotsc,K\}$; $\la_k \in [0,1]$ where $\sum_k\la_k=1$
	\item $\E{x_i}=\la_i$, $\V{x_i}=\la_i(1-\la_i)$, $\C{x_i,x_j}=-\la_i\la_j\ (i \neq j)$
\end{enumerate}

\paragraph{Dirichlet}

Dirichlet distribution is a common conjugate prior to Categorical distribution.
\begin{enumerate}
	\item $\P{\vec{\la}}=\Dir_{\vec{\la}}[\vec{\al}]=\frac{\Ga(\sum_{k=1}^K\al_k)}{\prod_{k=1}^K\Ga(\al_k)}\prod_{k=1}^K\la_k^{\al_k-1}$
	\item $\vec{\la}=[\la_1,\la_2,\dotsc,\la_K]\T$, $\la_k \in [0,1]$, $\sum_{k=1}^K\la_k=1$; $\vec{\al}=[\al_1,\al_2,\dotsc,\al_K]$, $\al_k \in \bR_+$
	\item $\E{\la_i}=\frac{\al_i}{\sum_k\al_k}$, $\V{\la_i}=\frac{\al_i(\sum_k\al_k-\al_i)}{(\sum_k\al_k)^2(\sum_k\al_k+1)}$, $\C{\la_i,\la_j}=\frac{-\al_i\al_j}{(\sum_k\al_k)^2(\sum_k\al_k+1)}\ (i \neq j)$
\end{enumerate}

\paragraph{Univariate Normal}

Univariate normal distribution describes single continuous variable.
\begin{enumerate}
	\item $\P{x}=\Norm_x[\mu,\si^2]=\frac{1}{\sqrt{2\pi\si^2}}\e{-\frac{(x-\mu)^2}{2\si^2}}$
	\item $x \in \bR$; $\mu \in \bR$, $\si^2 \in \bR_+$
	\item $\E{x}=\mu$, $\V{x}=\si^2$
\end{enumerate}

\paragraph{Normal Inverse Gamma}

Normal inverse gamma distribution is a common conjugate prior to Univariate Normal distribution.
\begin{enumerate}
	\item $\P{\mu,\si^2}=\NIG_{\mu,\si^2}[\al,\be,\ga,\de]=\frac{\sqrt{\ga}}{\sqrt{2\pi\si^2}}\frac{\be^\al}{\Ga(\al)}\left(\frac{1}{\si^2}\right)^{\al+1}\e{-\frac{2\be+\ga(\de-\mu)^2}{2\si^2}}$
	\item $\mu \in \bR$, $\si^2 \in \bR_+$; $\al \in \bR_+$, $\be \in \bR_+$, $\ga \in \bR_+$, $\de \in \bR$
	\item $\E{\mu}=\de$, $\E{\si^2}=\frac{\be}{\al-1}\ (\al>1)$, $\V{\mu}=\frac{\be}{(\al-1)\ga}\ (\al>1)$, $\V{\si^2}=\frac{\be^2}{(\al-1)^2(\al-2)}\ (\al>2)$, $\C{\mu,\si^2}=0\ (\al>1)$
\end{enumerate}

\paragraph{Multivariate Normal}

Multivariate normal distribution describes multiple continuous variables. It takes two parameters: a vector containing mean position $\vec{\mu}$, and a symmetric positive definite covariance matrix $\cov$.
\begin{enumerate}
	\item $\P{\vec{x}}=\Norm_{\vec{x}}[\vec{\mu},\cov]=\frac{1}{\sqrt{det(2\pi\cov)}}\e{-\half(\vec{x}-\vec{\mu})\T\cov\inv(\vec{x}-\vec{\mu})}$
	\item $\vec{x} \in \bR^K$; $\vec{\mu} \in \bR^K$, $\cov \in \bR^{K \times K}$ (positive semi-definite matrix)
	\item $\E{\vec{x}}=\vec{\mu}$, $\V{\vec{x}}=\cov$
\end{enumerate}

\paragraph{Normal Inverse Wishart}

Normal inverse wishart distribution is a common conjugate prior to Multivariate Normal distribution.
\begin{enumerate}
	\item $\P{\vec{\mu},\cov}=\NIW_{\vec{\mu},\cov}[\al,\vec{\Ps},\ga,\vec{\de}] \\ =\frac{\ga^{D/2}\det{\vec{\Ps}}^{\al/2}\det{\cov}^{-\frac{\al+D+2}{2}}}{(2\pi)^{D/2}2^{(\al\cov)/2}\Ga_D(\al/2)}\e{-\half(\Tr(\vec{\Ps}\cov\inv)+\ga(\vec{\mu}-\vec{\de})\T\cov\inv(\vec{\mu}-\vec{\de})}$
	\item $\vec{\mu} \in \bR^K$, $\cov \in \bR^{K \times K}$; $\al \in \bR_{>D-1}$, $\vec{\Ps} \in \bR^{K \times K}$, $\ga \in \bR_+$, $\vec{\de} \in \bR^K$
\end{enumerate}



\section{Linear Algebra}
\label{section2.2}

\subsection{Vectors}

\paragraph{Vectors Addition}

\begin{align*}
	\vec{v}+\vec{w}=\begin{pmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_n
	\end{pmatrix}+\begin{pmatrix}
	w_1 \\ w_2 \\ \vdots \\ w_n
	\end{pmatrix}=\begin{pmatrix}
	v_1+w_1 \\ v_2+w_2 \\ \vdots \\ v_n+w_n
	\end{pmatrix}
\end{align*}

\paragraph{Vectors Scaling}

\begin{align*}
	a\vec{v}=a\begin{pmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_n
	\end{pmatrix}=\begin{pmatrix}
	av_1 \\ av_2 \\ \vdots \\ av_n
	\end{pmatrix}
\end{align*}

\paragraph{Rules for Vectors Addition and Scaling}

\begin{enumerate}
	\item $\vec{u}+\left(\vec{v}+\vec{w}\right)=\left(\vec{u}+\vec{v}\right)+\vec{w}$
	\item $\vec{v}+\vec{w}=\vec{w}+\vec{v}$
	\item There is a vector $\vec{0}$ such that $\vec{0}+\vec{v}=\vec{v}$ for all $\vec{v}$
	\item For every vector $\vec{v}$ there is a vector $-\vec{v}$ so that $\vec{v}+\left(-\vec{v}\right)=\vec{0}$
	\item $a\left(b\vec{v}\right)=\left(ab\right)\vec{v}$
	\item $1\vec{v}=\vec{v}$
	\item $a\left(\vec{v}+\vec{w}\right)=a\vec{v}+a\vec{w}$
	\item $\left(a+b\right)\vec{v}=a\vec{v}+b\vec{v}$
\end{enumerate}

\paragraph{Linear Combination \& Span}

Linear combination (e.g. in 2D space):
\begin{align*}
	a\vec{v}+b\vec{w}
\end{align*}
The span of $\vec{v}$ and $\vec{w}$ is the set of all their linear combinations.

\paragraph{Representation of Basis}

In Euclidean:
\begin{align*}
\begin{pmatrix}
v_1 \\ v_2 \\ \vdots \\ v_n
\end{pmatrix}=v_1\begin{pmatrix}
1 \\ 0 \\ \vdots \\ 0
\end{pmatrix}+v_2\begin{pmatrix}
0 \\ 1 \\ \vdots \\ 0
\end{pmatrix}+\dots+v_n\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 1
\end{pmatrix}
\end{align*}
We can write this as
\begin{align*}
	\vec{v}=v_1\vec{e}^1+v_2\vec{e}^2+\dots+v_n\vec{e}^n
\end{align*}
In different basis, we choose other basis vector and then write the same vector
\begin{align*}
	\vec{v}=w_1\vec{b}^1+w_2\vec{b}^2+\dots+w_n\vec{b}^n
\end{align*}
If these basis vectors are orthonormal, $w_i=\vec{v}\T\vec{b}^i$

\paragraph{Linear Dependence}

\begin{enumerate}
	\item Linearly dependent: A set of vectors $\vec{v}^1,\dotsc,\vec{v}^n$ is linearly dependent if there exists a vector $\vec{v}^j$ that can be expressed as a linear combination of the other vectors. (The vector $\vec{v}^j$ is already located in the span of other vectors)
	\item Linearly Independent: Each vector really does add another dimension to the span. And the only solution to $\sum_{i=1}^na_i\vec{v}^i=\vec{0}$ is for all $a_i=0, i=1,\dotsc,n$.
\end{enumerate}

\paragraph{Dot Products}

\begin{align*}
	\vec{v}\cdot\vec{w}=\sum_{i=1}^nv_iw_i=\vec{v}\T\vec{w}
\end{align*}
The length of a vector is denoted as $\norm{\vec{v}}$, the squared length is given by
\begin{align*}
	\norm{\vec{v}}^2=\vec{v}\T\vec{v}=\vec{v}^2=v_1^2+v_2^2+\dots+v_n^2
\end{align*}
A natural geometric interpretation of dot products is
\begin{align*}
	\vec{v}\cdot\vec{w}=\norm{\vec{v}}\norm{\vec{w}}\cos\th
\end{align*}
where $\th$ is the angle between two vectors.

\subsection{Matrices (Square Matrices)}

\paragraph{Linear Transformation}

\begin{align*}
	\vec{v}'=\left(\mat{M}_n\dots\mat{M}_2\mat{M}_1\right)\vec{v}
\end{align*}
Linear transformation always maps linear subspaces onto linear subspaces (possibly of a lower dimension). Intuitively, linear transformation keeps the grid lines stay parallel and evenly spaced, and so that the origin remains fixed.

\paragraph{Basic Formulae}

\begin{enumerate}
	\item $\mat{A}(\mat{B}+\mat{C})=\mat{A}\mat{B}+\mat{A}\mat{C}$
	\item $(\mat{A}+\mat{B})\T=\mat{A}\T+\mat{B}\T$
	\item $(\mat{A}\mat{B})\T=\mat{B}\T\mat{A}\T$
	\item[] if individual inverses exist:
	\item $(\mat{A}\mat{B})\inv=\mat{B}\inv\mat{A}\inv$
	\item $(\mat{A}\inv)\T=(\mat{A}\T)\inv$
\end{enumerate}

\paragraph{Trace}

Trace is the sum of diagonal elements of matrix $\mat{M}$,
\begin{align*}
	\Tr(\mat{M})=\Tr(\mat{M}\T)=\sum_{i=1}^n\mat{M}_{ii}=\sum\text{eigenvalues of $\mat{M}$}
\end{align*}
Cyclic permutations in trace,
\begin{align*}
	\Tr(\mat{A}\mat{B}\mat{C})=\Tr(\mat{C}\mat{A}\mat{B})=\Tr(\mat{B}\mat{C}\mat{A})
\end{align*}

\paragraph{Determinants}

The determinant of a matrix $\mat{M}$, denoted as $\det{\mat{M}}$, is a function mapping matrices to scalars, measuring how much multiplication by the matrix expands or contracts space. If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is 1, then the transformation preserves volume.

Some properties of determinant:
\begin{enumerate}
	\item $\det{\mat{M}}=\prod\text{eigenvalues of $\mat{M}$}$
	\item $\det{\mat{A}\mat{B}}=\det{\mat{A}}\det{\mat{B}}$
	\item $\det{a}=a$
	\item $\det{a\mat{M}}=a^n\det{\mat{M}}$
	\item $\det{\mat{M}\inv}=\det{\mat{M}}^{-1}$
\end{enumerate}

\paragraph{Symmetric Matrix}

\begin{align*}
	\mat{M}=\mat{M}\T
\end{align*}

\paragraph{Orthogonal Matrix}

\begin{align*}
	\mat{M}\T\mat{M}&=\mat{M}\mat{M}\T=\mat{I} \\
	\mat{M}\inv&=\mat{M}\T
\end{align*}

\paragraph{Eigendecomposition}

If a matrix $\mat{M}$ satisfies
\begin{align*}
	\mat{M}\vec{v}=\la\vec{v}
\end{align*}
then $\vec{v}$ is the \emph{eigenvector} of $\mat{M}$, and $\la$ is the \emph{eigenvalue} of $\mat{M}$ (The vector $\vec{v}$ is just scaled by value $\la$ after a linear transformation $\mat{M}$). To solve the equation, we can rewrite it as
\begin{align*}
	(\mat{M}-\la\mat{I})\vec{v}=\vec{0}
\end{align*}
When $\det{\mat{M}-\la\mat{I}}=0$, it means at least one dimension is contracted by the linear transformation $\mat{M}-\la\mat{I}$ so that there exists at least one non-zero vector to be transformed to zero, which gives us a solution to $\la$.

Suppose matrix $\mat{M}$ has $n$ linearly independent eigenvectors, $\{\vec{v}^{(1)},\dotsc,\vec{v}^{(n)}\}$, with corresponding eigenvalues $\{\la_1,\dotsc,\la_n\}$. We may concatenate all of the eigenvectors to form a matrix $\mat{V}$ with one eigenvector per column: $\mat{V}=[\vec{v}^{(1)},\dotsc,\vec{v}^{(n)}]$. Likewise, we can concatenate the eigenvalues to form a vector $\vec{\la}=[\la_1,\dotsc,\la_n]$. The eigendecomposition of $\mat{M}$ is then given by
\begin{align*}
	\mat{M}=\mat{V}\diag(\vec{\la})\mat{V}\inv
\end{align*}

Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:
\begin{align*}
	\mat{M}=\mat{Q}\mat{\La}\mat{Q}\T
\end{align*}
where $\mat{Q}$ is an orthogonal matrix composed of eigenvectors of $\mat{M}$, and $\mat{\La}$ is a diagonal matrix.

A matrix is \emph{singular} if and only if any of the eigenvalues are zero ($\det{\mat{M}}=0$). A matrix whose eigenvalues are all positive is called \emph{positive definite}. A matrix whose eigenvalues are all positive or zero-valued (non-negative) is called \emph{positive semi-definite}. Likewise, if all eigenvalues are negative, the matrix is \emph{negative definite}, and if all eigenvalues are negative or zero-valued (non-positive), it is \emph{negative semi-definite}. Positive semi-definite matrices guarantee that $\forall\vec{v}, \vec{v}\T\mat{M}\vec{v}\geq0$. Positive definite matrices additionally guarantee that $\vec{v}\T\mat{M}\vec{v}=0\impl\vec{v}=\vec{0}$.

\paragraph{Singular Value Decomposition (SVD)}

The singular value decomposition of a $m\times n$ matrix $\mat{M}$ can be formed as
\begin{align*}
	\mat{M}=\mat{U}\mat{\Si}\mat{V}\T
\end{align*}
where $\mat{U}$ is a $m\times m$ orthogonal matrix, $\mat{\Si}$ is a $m\times n$ diagonal matrix, and $\mat{V}$ is a $n\times n$ orthogonal matrix. The elements along the diagonal of $\mat{\Si}$ are known as the \emph{singular values} of the matrix $\mat{M}$. The columns of $\mat{U}$ are known as the \emph{left-singular vectors}. The columns of $\mat{V}$ are known as as the \emph{right-singular vectors}.

There are relations between SVD and eigendecomposition. According to the definition of SVD, we have
\begin{align*}
	\mat{M}\T\mat{M}&=\mat{V}\mat{\Si}\T\mat{U}\T\mat{U}\mat{\Si}\mat{V}\T=\mat{V}(\mat{\Si}\T\mat{\Si})\mat{V}\T \\
	\mat{M}\mat{M}\T&=\mat{U}\mat{\Si}\mat{V}\T\mat{V}\mat{\Si}\T\mat{U}\T=\mat{U}(\mat{\Si}\mat{\Si}\T)\mat{U}\T
\end{align*}
Hence, we can conclude that
\begin{enumerate}
	\item The right-singular vectors of $\mat{M}$ are the eigenvectors of $\mat{M}\T\mat{M}$.
	\item The left-singular vectors of $\mat{M}$ are the eigenvectors of $\mat{M}\mat{M}\T$.
	\item The non-zero singular values of $\mat{M}$ are the square roots of the eigenvalues of $\mat{M}\T\mat{M}$ or $\mat{M}\mat{M}\T$.
\end{enumerate}

\paragraph{Moore-Penrose Pseudoinverse}

SVD can be applied to compute the pseudoinverse of a matrix $\mat{M}=\mat{U}\mat{\Si}\mat{V}\T$, given by
\begin{align*}
	\mat{M}\pinv=\mat{V}\mat{\Si}\pinv\mat{U}\T
\end{align*}
where $\mat{\Si}\pinv$ is the pseudoinverse of $\mat{\Si}$, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix.



\section{Calculus}
\label{section2.3}

\subsection{Differentiation}

\paragraph{Basic Formulae}

Here, $f(x)$ and $g(x)$ are differentiable functions (the derivative exists), $c$ and $n$ are any real numbers.
\begin{enumerate}
	\item $(cf)'=cf'(x)$
	\item $(f\pm g)'=f'(x)\pm g'(x)$
	\item $(fg)'=f'g+fg'$ (Product Rule)
	\item $(\frac{f}{g})'=\frac{f'g-fg'}{g^2}$ (Quotient Rule)
	\item $\der{x}(c)=0$
	\item $\der{x}(x^n)=nx^{n-1}$ (Power Rule)
	\item $\der{x}(f(g(x)))=f'(g(x))g'(x)$ (Chain Rule)
\end{enumerate}

\paragraph{Mean Value Theorem}

If $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$ then there is a number $a<c<b$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$.

\paragraph{Newton's Method}

If $x_n$ is the $n^\text{th}$ guess for the root/solution of $f(x)=0$ then $(n+1)^\text{st}$ guess is $x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$ provided $f'(x_n)$ exists.

\paragraph{Taylor Series}

The Taylor series of a real or complex-valued function $f(x)$ that is infinitely differentiable at a real or complex number a is the power series
\begin{align*}
	f(a)+\frac{f'(a)}{1!}(x-a)+\frac{f''(a)}{2!}(x-a)^2+\dots+\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{align*}
where $n!$ denotes the factorial of $n$ and $f^{n}(a)$ denotes the $n^\text{th}$ derivative of $f$ evaluated at the point $a$. Sometimes, we can represent it as
\begin{align*}
	f(x+h)=f(x)+hf'(x)+\frac{h^2}{2}f''(x)+\cdots+\frac{h^{r-1}}{(r-1)!}f^{(r-1)}(x)+\O{h^r}
\end{align*}
If we set the function as $f(\vec{x}+h\vec{v})$ for some vector $\vec{v}$, then
\begin{align*}
	f(\vec{x}+h\vec{v})=f(\vec{x})+h\grad f\T\vec{v}+\frac{h^2}{2}\vec{v}\T \hess_f\vec{v}+\O{h^3}
\end{align*}
where $\grad f$ is the gradient vector and $H_f$ is the Hessian matrix.

\subsection{Integration}

\paragraph{Properties}

\begin{enumerate}
	\item $\int f(x)\pm g(x)\ds{x}=\int f(x)\ds{x}\pm\int g(x)\ds{x}$
	\item $\int_a^b f(x)\pm g(x)\ds{x}=\int_a^b f(x)\ds{x}\pm\int_a^b g(x)\ds{x}$
	\item $\int_a^a f(x)\ds{x}=0$
	\item $\int_a^b=-\int_b^a f(x)\ds{x}$
	\item $\int_a^b f(x)\ds{x}=\int_a^c f(x)\ds{x}+\int_c^b f(x)\ds{x}$
	\item $\int kf(x)\ds{x}=k\int f(x)\ds{x}$
	\item $\int_a^b kf(x)\ds{x}=k\int_a^b f(x)\ds{x}$
	\item $\int_a^b k\ds{x}=k(b-a)$
	\item $\ab{\int_a^b f(x)\ds{x}}\leq\int_a^b\ab{f(x)}\ds{x}$
	\item If $f(x)\geq g(x)$ on $a\leq x\leq b$ then $\int_a^b f(x)\ds{x}\geq\int_a^b g(x)\ds{x}$
	\item If $f(x)\geq0$ on $a\leq x\leq b$ then $\int_a^b f(x)\ds{x}\geq0$
	\item If $m\leq f(x)\leq M$ on $a\leq x\leq b$ then $m(b-a)\leq\int_a^b f(x)\ds{x}\leq M(b-a)$
\end{enumerate}

\paragraph{Average Function Value}

The average value of $f(x)$ on $a\leq x\leq b$ is
\begin{align*}
	f_{avg}=\frac{1}{b-a}\int_a^b f(x)\ds{x}
\end{align*}

\paragraph{Jensen's Inequality}

If $\vph$ is a convex function,
\begin{align*}
	\vph\left(\frac{1}{b-a}\int_a^b f(x)\ds{x}\right)\leq\frac{1}{b-a}\int_a^b\vph\left(f(x)\right)\ds{x}
\end{align*}
Additionally, if $X$ is an integrable real-valued random variable, then
\begin{align*}
	\vph\left(\E{X}\right)\leq\E{\vph(X)}
\end{align*}

\subsection{Multivariate Calculus}

\paragraph{Partial Derivatives}

Partial derivatives are simply holding all other variables constant (and act like constants for the derivative) and only taking the derivative with respect to a given variable.
\begin{align*}
	f_x(x,y)=f_x=\frac{\pa f}{\pa x}=\frac{\pa}{\pa x}f(x,y)=D_x f
\end{align*}

\paragraph{Clairaut's Theorem}

If the function $f_{xy}$ and $f_{yx}$ are both continuous, then $f_{xy}(a,b)=f_{yx}(a,b)$.

\paragraph{Chain Rule}

If $z=f(x,y)$, $x=g(t)$, and $y=h(t)$, then
\begin{align*}
	\frac{dz}{dt}=\frac{\pa z}{\pa x}\frac{dx}{dt}+\frac{\pa z}{\pa y}\frac{dy}{dt}
\end{align*}

\paragraph{Gradient}

\begin{align*}
	\grad f(x,y)=\left[f_x(x,y)\ f_y(x,y)\right]\T
\end{align*}

\paragraph{Directional Derivative}

\begin{align*}
	D_{\vec{u}}f(x,y)=\grad f(x,y)\cdot\vec{u}
\end{align*}
where $\vec{u}$ is an unit vector. The maximum value of the directional derivative $D_{\vec{u}}f(x,y)$ is $\ab{\grad f(x,y)}$, and it occurs when $\vec{u}$ has the same direction as the gradient vector $\grad f(x,y)$.

\paragraph{Lagrange Multipliers}

To find the maximum and minimum values of $f(x,y,z)$ subject to the constraint $g(x,y,z)=k$,
\begin{enumerate}
	\item Find all values of $x,y,z$ and $\la$ such that
	\begin{align*}
		\grad f(x,y,z)&=\la\grad g(x,y,z) \\
		g(x,y,z)&=k
	\end{align*}
	\item Evaluate $f$ at all of these points. The largest is the maximum value, and the smallest is the minimum value of $f$ subject to the constraint $g$.
\end{enumerate}

\subsection{Matrix Calculus}

\paragraph{Vector by Scalar}

The derivative of a vector $\vec{y}=[y_1\ y_2\ \cdots\ y_n]\T$, by a scalar $x$:
\begin{align*}
	\frac{\pa\vec{y}}{\pa x}=\left[\frac{\pa y_1}{\pa x}\ \frac{\pa y_2}{\pa x}\ \cdots\ \frac{\pa y_n}{\pa x}\right]\TT
\end{align*}

\paragraph{Scalar by Vector (Gradient)}

The derivative of a scalar $y$, by a vector $\vec{x}=[x_1\ x_2\ \cdots\ x_n]\T$:
\begin{align*}
	\frac{\pa y}{\pa\vec{x}}=\left[\frac{\pa y}{\pa x_1}\ \frac{\pa y}{\pa x_2}\ \cdots\ \frac{\pa y}{\pa x_n}\right]\TT
\end{align*}

\paragraph{Vector by Vector (Jacobian Matrix)}

The derivative of a vector $\vec{y}=[y_1\ y_2\ \cdots\ y_m]\T$, by another vector $\vec{x}=[x_1\ x_2\ \cdots\ x_n]\T$:
\begin{align*}
	\frac{\pa\vec{y}}{\pa\vec{x}}=\matfill{
		\frac{\pa y_1}{\pa x_1}&\frac{\pa y_1}{\pa x_2}&\cdots&\frac{\pa y_1}{\pa x_n} \\
		\frac{\pa y_2}{\pa x_1}&\frac{\pa y_2}{\pa x_2}&\cdots&\frac{\pa y_2}{\pa x_n} \\
		\vdots&\vdots&\ddots&\vdots \\
		\frac{\pa y_m}{\pa x_1}&\frac{\pa y_m}{\pa x_2}&\cdots&\frac{\pa y_m}{\pa x_n}
	}
\end{align*}

\paragraph{Matrix by Scalar}

The derivative of a matrix $\mat{Y}\in\bR^{m\times n}$, by a scalar $x$:
\begin{align*}
	\frac{\pa\mat{Y}}{\pa x}=\matfill{
	\frac{\pa y_{11}}{\pa x}&\frac{\pa y_{12}}{\pa x}&\cdots&\frac{\pa y_{1n}}{\pa x} \\
	\frac{\pa y_{21}}{\pa x}&\frac{\pa y_{22}}{\pa x}&\cdots&\frac{\pa y_{2n}}{\pa x} \\
	\vdots&\vdots&\ddots&\vdots \\
	\frac{\pa y_{m1}}{\pa x}&\frac{\pa y_{m2}}{\pa x}&\cdots&\frac{\pa y_{mn}}{\pa x}
}
\end{align*}

\paragraph{Scalar by Matrix (Gradient Matrix)}

The derivative of a scalar $y$, by a matrix $\mat{X}\in\bR^{m\times n}$:
\begin{align*}
	\frac{\pa y}{\pa\mat{X}}=\matfill{
	\frac{\pa y}{\pa x_{11}}&\frac{\pa y}{\pa x_{21}}&\cdots&\frac{\pa y}{\pa x_{m1}} \\
	\frac{\pa y}{\pa x_{12}}&\frac{\pa y}{\pa x_{22}}&\cdots&\frac{\pa y}{\pa x_{m2}} \\
	\vdots&\vdots&\ddots&\vdots \\
	\frac{\pa y}{\pa x_{1n}}&\frac{\pa y}{\pa x_{2n}}&\cdots&\frac{\pa y}{\pa x_{mn}}
}
\end{align*}

\subsection{Backpropagation Modules}

\paragraph{Backpropagation}

An application of chain rule. For a simple nested function: $y=f(g(x))$:
\begin{align*}
	\frac{dy}{dx}=\frac{df}{dg}\frac{dg}{dx}
\end{align*}
For multivariate function $y=f\left(g^{(1)}(x),\dotsc,g^{(n)}(x)\right)$:
\begin{align*}
	\frac{\pa y}{\pa x}=\sum_{i=1}^n\frac{\pa f}{\pa g^{(i)}}\frac{\pa g^{(i)}}{\pa x}
\end{align*}

\paragraph{Linear Module}

\begin{enumerate}
	\item Forward pass: $\vec{y}=\mat{W}\vec{x}+\vec{b}$
	\item Backward pass: $\pfrac{\pa L}{\pa\vec{x}}=\ppfrac{\pa L}{\pa\vec{y}}\mat{W}$
	\item Parameter gradient: $\pfrac{\pa L}{\pa\mat{W}}=\ppfrac{\pa L}{\pa\vec{y}}\T\vec{x}\T$, $\pfrac{\pa L}{\pa\vec{b}}=\pfrac{\pa L}{\pa\vec{y}}$
\end{enumerate}

\paragraph{ReLU Module}

\begin{enumerate}
	\item Forward pass: $\vec{y}=relu(\vec{x})$, $y_i=max(0,x_i)$
	\item backward pass: $\pfrac{\pa L}{\pa x_i}=(y_i>0)\ppfrac{\pa L}{\pa y_i}$
\end{enumerate}

\paragraph{Softmax Module}

\begin{enumerate}
	\item Forward pass: $\vec{y}=softmax(\vec{x})$, $y_n=\pfrac{e^{x_n}}{\sum_m e^{x_m}}$
	\item Backward pass: $\pfrac{\pa L}{\pa\vec{x}}=\vec{s}-\vec{y}\sum_i s_i$, where $s_i=\ppfrac{\pa L}{\pa y_i}y_i$
\end{enumerate}

\paragraph{Cross-entropy Module}

\begin{enumerate}
	\item Forward pass: $y=L=crossentropy(\vec{p},\vec{x})$, $y=-\sum_i^C p_i\log x_i$
	\item Backward pass: $\pfrac{\pa L}{\pa x_i}=-\pfrac{p_i}{x_i}$
\end{enumerate}

\paragraph{Cross-entropy with Logits Module}

\begin{enumerate}
	\item Forward pass: $y=L=crossentropy(\vec{p},softmax(\vec{x}))$, $y=-\sum_i^Cp_i\log\ppfrac{e^{x_i}}{\sum_m e^{x_m}}$
	\item Backward pass: $\pfrac{\pa L}{\pa\vec{x}}=\vec{y}-\vec{p}$
\end{enumerate}




\section{Information Theory}
\label{section2.4}

\subsection{Self-information}

Properties of information quantification:
\begin{enumerate}
	\item Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.
	\item Less likely events should have higher information content.
	\item Independent events should have additive information.
\end{enumerate}
In order to satisfy all three of these properties, we define the self-information of an event $\mathtt{x}=x$ to be $I(x)=-\log\P{x}$.

\subsection{Entropy}
Self-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the Shannon entropy:
\begin{align*}
	\H{x}=\Ep{x\sim P}{I(x)}=-\Ep{x\sim P}{\log\P{x}}=-\sum_i p_i\log p_i
\end{align*}

\subsection{Kullback-Leibler (KL) Divergence}

If we have two separate probability distributions $\P{x}$ and $Q\left(x\right)$ over the same random variable $x$, we can measure how different these two distribution are using the Kullback-Leibler (KL) divergence:
\begin{align*}
	\KL{P}{Q}=\Ep{x\sim P}{\log\frac{\P{x}}{Q\left(x\right)}}=\Ep{x\sim P}{\log\P{x}-\log Q\left(x\right)}
\end{align*}
KL divergence is non-negative, and it is not symmetric for some $P$ and $Q$: $\KL{P}{Q}\neq\KL{Q}{P}$. The KL divergence is 0 if and only if $P$ and $Q$ are the same distribution in the case of discrete variables, or equal ``almost everywhere'' in the case of continuous variables.

\subsection{Cross-entropy}

A quantity that is closely related to the KL divergence is the cross-entropy, which is similar to the KL divergence but lacking the term on the left:
\begin{align*}
	\H{P,Q}=\H{P}+\KL{P}{Q}=-\Ep{x\sim P}{\log Q\left(x\right)}
\end{align*}



\section{Optimization}
\label{section2.5}

\subsection{One-dimensional Minimization}

\paragraph{Sufficient Conditions for a Minimum}

For $x^*$ to be a local minimum of a univariate function $f(x)$, we require that $f(x^*)\leq f(x)$ for all $x$ which are sufficiently close to $x^*$. More precisely, for $x^*$ to be a local minimum, there must exist a positive constant $\De$ such that the inequality holds for all $x$ satisfying $\ab{x-x^*}<\De$. Further, $x^*$ is a global minimum if the inequality holds for all $x$, i.e. one can choose $\De=\infty$.

Now assume that $f(x)$ can be differentiated three times at some point $x^*$. Then we have the Taylor expansion:
\begin{align*}
	f(x^*+h)=f(x^*)+f'(x^*)h+\frac{f''(x^*)}{2}h^2+\O{h^3}
\end{align*}
Choosing a small positive $\ep$ and setting $h=-f'(x^*)\ep$, we get $f(x^*+h)\approx f(x^*)-f'(x^*)^2\ep$ and thus $f(x^*+h)<f(x^*)$ unless $f'(x^*)=0$. Consequently $x^*$ can only be a local minimum if $f'(x^*)=0$. Let us assume that $f'(x^*)=0$ and that further $f''(x^*)$ is greater than zero. Then for small values of $h$ the Taylor expansion yields,
\begin{align*}
	f(x^*+h)\approx f(x^*)+\frac{f''(x^*)}{2}h^2
\end{align*}
and thus $f(x^*+h)>f(x^*)$. So in this case $x^*$ is a local minimum of $f$. In case that also $f''(x^*)=0$, one has to take into account the behavior of the higher order term $\O{h^3}$. To summarize, $f'(x^*)=0$, $f''(x^*)>0$ is a sufficient condition for $x^*$ to be a local minimum.

\paragraph{Rate of Convergence}

In the sequel we shall consider algorithms which compute a sequence of improving approximation $x_k$ to $x^*$. So, numerical considerations aside, $\lim_{k\to\infty}x_k=x^*$. One way of comparing different algorithms, is to consider the rate of convergence $p$ of the sequence $x_k$. The rate of convergence $p$ is defined to be the largest number for which it is possible to find a bount $C_p$ so that the following inequality holds for all $k$:
\begin{align*}
	\frac{\ab{x_{k+1}-x^*}}{\ab{x_k-x^*}^p}\leq C_p
\end{align*}
For example, the sequence $x_k=2^{-k}$ has rate of convergence $p=1$. Obviously $x^*=0$ and
\begin{align*}
	\frac{\ab{x_{k+1}-x^*}}{\ab{x_k-x^*}^p}=\frac{2^{-k-1}}{2^{-kp}}=2^{-k-1+kp}=2^{-1}2^{k(p-1)}
\end{align*}
So for $p=1$ we can use $C_1=2^{-1}$ whereas $2^{k(p-1)}$ diverges for $p>1$.

\paragraph{Brackets}

A bracket for minimizing a function $f$, is a triple $\{a,b,c\}$ of real numbers, such that $a<b<c$ and $f(a)\geq f(b)\leq f(c)$. Since the value of $f$ on the interior point $b$ is smaller or equal to the value on the boundary points $a$ and $c$, one will expect that there is a local minimum somewhere inside the interval $[a,c]$. Once we have bracket, it is straightforward to obtain a better approximation, by picking a point $x\neq b$ in the interval $(a,c)$ and comparing $f(x)$ to $f(b)$.

Assume we pick $x$ such that $a<x<b$, then either $f(x)\geq f(b)$, so $\{x,b,c\}$ is a new bracket; or $f(x)<f(b)$, so $\{a,x,b\}$ is a new bracket. In the first case, it is obvious that $\{x,b,c\}$ is a bracket. For the second case, $f(x)<f(b)$, note that $f(a)\geq f(b)$, since $\{a,b,c\}$ is a bracket. So $f(a)>f(x)$ and $\{a,x,b\}$ is indeed a bracket. In the case that we choose to pick $x$ such that $b<x<c$, it is either $f(x)\geq f(b)$, so $\{a,b,x\}$ is a new bracket; or $f(x)<f(b)$, so $\{b,x,c\}$ is a new bracket. So, by choosing an interior point $x$ in the current bracket and obtaining a new bracket, we arrive at better and better approximations to the location of the minimum.

\subsection{Gradient Descent}

\paragraph{Exact Line Search Condition}

The iterative search technique that we proceed towards the minimum $\vec{x}^*$ by a sequence of steps. On the $k^\text{th}$ step we take a step of length $\al_k$ in the direction $\vec{p}_k$,
\begin{align*}
	\vec{x}_{k+1}=\vec{x}_k+\al_k\vec{p}_k
\end{align*}
The length of the step can either be chosen using prior knowledge, or by carrying out a line search in the direction $\vec{p}_k$. At the $k^\text{th}$ step, we chose $\al_k$ to minimize $f(\vec{x}_k+\al_k\vec{p}_k)$. So setting $F(\la)=f(\vec{x}_k+\la\vec{p}_k)$, at this step we solve the one-dimensional minimization problem for $F(\la)$. Thus our choice of $\al_k=\la^*$ will satisfy $F'(\al_k)=0$. We can calculate that $F'(\al_k)=\grad f\TT(\vec{x}_{k+1})\vec{p}_k$. So $F'(\al_k)=0$ means the directional derivative in the search direction must vanish at the new point and this gives the exact line search condition: $\grad f\T(\vec{x}_{k+1})\vec{p}_k=0$.

For a quadratic function $f(\vec{x})=\half\vec{x}\T\mat{A}\vec{x}-\vec{b}\T\vec{x}+c$, with symmetric $\mat{A}$, we can use he condition to analytically calculate $\al_k$. Since $\grad f(\vec{x}_{k+1})=\mat{A}\vec{x}_k+\al_k\mat{A}\vec{p}_k-\vec{b}=\grad f(\vec{x}_k)+\al_k\mat{A}\vec{p}_k$, we find $\al_k=-\frac{\vec{p}_k\TT\grad f(\vec{x}_k)}{\vec{p}_k\TT\mat{A}\vec{p}_k}$.

\paragraph{Gradient Descent}

The simplest choice for $\vec{p}_k$ is to set it equal to $-\grad f(\vec{x}_k)$. If we find that $\grad f(\vec{x}_k)=\vec{0}$ we can stop. To see this, expand $f$ around $\vec{x}_k$ using Taylor's theorem:
\begin{align*}
	f(\vec{x}_k+\al_k\vec{p}_k)\approx f(\vec{x}_k)+\al_k\grad f\TT(\vec{x}_k)\vec{p}_k
\end{align*}
With $\vec{p}_k=-\grad f(\vec{x}_k)$ and for small positive $\al_k$, we see a guaranteed reduction:
\begin{align*}
	f(\vec{x}_k+\al_k\vec{p}_k)\approx f(\vec{x}_k)-\al_k\norm{\grad f(\vec{x}_k)}^2
\end{align*}

\subsection{Quadratic Functions}

\paragraph{Minimizing Quadratic Functions Using Line Search}

Consider a function $f(\vec{x})=\half\vec{x}\T\mat{A}\vec{x}-\vec{b}\T\vec{x}+c$ where $\mat{A}$ is positive definite and symmetric. One approach to finding minima is to search along a particular direction $\vec{p}$, and find a minimum along this direction. We can then search for a deeper minima by looking in different directions. That is, we can search along a line $\vec{x}^*=\vec{x}^0+\la\vec{p}$ such that the function attains a minimum. Differentiate the original function and set to zero, we have the solution
\begin{align*}
	\la=\frac{(\vec{b}-\mat{A}\vec{x}^0)\vec{p}}{\vec{p}\T\mat{A}\vec{p}}=\frac{-\grad f(\vec{x}^0)\vec{p}}{\vec{p}\T\mat{A}\vec{p}}
\end{align*}
It would seem sensible to choose successive line search directions $\vec{p}$ according to $\vec{p}^{new}=-\grad f(\vec{x}^*)$, so that each time we minimize the function along the line of steepest descent. However, this is far from the optimal choice in the case of minimizing quadratic functions.

\paragraph{Conjugate Vectors}

The vectors $\vec{p}_i, i=1,\dotsc,k$ are called conjugate to the matrix $\mat{A}$, iff for $i,j=1,\dotsc,k$ and $i\neq j$:
\begin{align*}
	\vec{p}_i\TT\mat{A}\vec{p}_j=0\quad\text{and}\quad\vec{p}_i\TT\mat{A}\vec{p}_i>0
\end{align*}
The two conditions guarantee that conjugate vectors are linearly independent: assume that
\begin{align*}
	\vec{0}=\sum_{j=1}^k\al_j\vec{p}_j=\sum_{j=1}^{i-1}\al_j\vec{p}_j+\al_i\vec{p}_i+\sum_{j=i+1}^k\al_j\vec{p}_j
\end{align*}
Now multiplying from the left with $\vec{p}_i\TT\mat{A}$ yields $0=\al_i\vec{p}_i\TT\mat{A}\vec{p}_i$. So $\al_i$ is zero since we know that $\vec{p}_i\TT\mat{A}\vec{p}_i>0$. As we can make this argument for any $i=1,\dotsc,k$, all of the $\al_i$ must be zero. That conjugate vectors are linearly independent, means that the matrix $\mat{P}=[\vec{p}_1\ \vec{p}_2\ \cdots\ \vec{p}_k]$ has full rank. In particular, if $k=n$, the matrix $\mat{P}$ will be invertible. If the symmetric $n\times n$ matrix $\mat{A}$ has $n$ conjugate directions, it is positive definite.

\paragraph{Gram-Schmidt Procedure}

We now turn to constructing conjugate vectors using a procedure which is analogous to Gram-Schmidt orthogonalization. Assume we already have $k$ conjugate vectors $\vec{p}_1,\dotsc,\vec{p}_k$ and let $\vec{v}$ be a vector which is linearly independent of $\vec{p}_1,\dotsc,\vec{p}_k$. We then set
\begin{align*}
	\vec{p}_{k+1}=\vec{v}-\sum_{j=1}^k\frac{\vec{p}_j\TT\mat{A}\vec{v}}{\vec{p}_j\TT\mat{A}\vec{p}_j}\vec{p}_j
\end{align*}
and claim that now the vectors $\vec{p}_1,\dotsc,\vec{p}_{k+1}$ are conjugate if $\mat{A}$ is positive definite: since $\vec{v}$ is linearly independent of $\vec{p}_1,\dotsc,\vec{p}_k$, the new vector $\vec{p}_{k+1}$ cannot be zero, and thus for positive definite $\mat{A}$, $\vec{p}_{k+1}\TT\mat{A}\vec{p}_{k+1}>0$. Also, $\vec{p}_i\TT\mat{A}\vec{p}_{k+1}=0$ that
\begin{align*}
	\vec{p}_i\TT\mat{A}\vec{p}_{k+1}&=\vec{p}_i\TT\mat{A}\vec{v}-\sum_{k=1}^k\frac{\vec{p}_j\TT\mat{A}\vec{v}}{\vec{p}_j\TT\mat{A}\vec{p}_j}\vec{p}_i\TT\mat{A}\vec{p}_j \\
	&=\vec{p}_i\TT\mat{A}\vec{v}-\frac{\vec{p}_i\TT\mat{A}\vec{v}}{\vec{p}_i\TT\mat{A}\vec{p}_i}\vec{p}_i\TT\mat{A}\vec{p}_i \\
	&=0
\end{align*}
An important property of the Gram-Schmidt procedure is that the vectors $\vec{p}_1,\dotsc,\vec{p}_{k+1}$ span the same subspace as the vectors $\vec{v}$ and $\vec{p}_1,\dotsc,\vec{p}_k$: any linear combination of $\vec{p}_1,\dotsc,\vec{p}_{k+1}$ can be rewritten as a linear combination of $\vec{v}$ and $\vec{p}_1,\dotsc,\vec{p}_k$ just by replacing $\vec{p}_{k+1}$ with the RHS of Gram-Schmidt equation. So the space spanned by $\vec{p}_1,\dotsc,\vec{p}_{k+1}$ is contained in the one spanned by $\vec{v}$ and $\vec{p}_1,\dotsc,\vec{p}_k$. But solving the Gram-Schmidt equation for $\vec{v}$ allows us to turn the argument around and so the two subspaces are the same.

Using the Gram-Schmidt procedure, we can construct $n$ conjugate vectors for a positive definite matrix in the following way. We start with $n$ linearly independent vectors $\vec{u}_1,\dotsc,\vec{u}_n$, e.g. we might choose $\vec{u}_i=\vec{e}_i$, the unit vector in the $i^\text{th}$ direction. We then set $\vec{p}_1=\vec{u}_1$ and use the equation to compute $\vec{p}_2$ from $\vec{p}_1$ and $\vec{v}=\vec{u}_2$. Next we set $\vec{v}=\vec{u}_3$ and compute $\vec{p}_3$ from $\vec{p}_1$, $\vec{p}_2$ and $\vec{v}$. Continuing in this manner we obtain $n$ conjugate vectors. Note that at each stage of the procedure the vectors $\vec{u}_1,\dotsc,\vec{u}_k$ span the same subspace as the vector $\vec{p}_1,\dotsc,\vec{p}_j$.

\paragraph{The Conjugate Vectors Algorithm}

Assume that when minimizing $f(\vec{x})=\half\vec{x}\T\mat{A}\vec{x}-\vec{b}\T\vec{x}+c$ we first construct $n$ vectors $\vec{p}_1,\dotsc,\vec{p}_n$ conjugate to $\mat{A}$ which we use as our search directions. So
\begin{align*}
	\vec{x}_{k+1}=\vec{x}_k+\al_k\vec{\vec{p}_k}
\end{align*}
At each step we choose $\al_k$ by an exact line search, thus
\begin{align*}
	\al_k=-\frac{\vec{p}_k\TT\grad f(\vec{x}_k)}{\vec{p}_k\TT\mat{A}\vec{p}_k}
\end{align*}
We call this procedure the conjugate vectors algorithm. According to Expanding Subspace Theorem, the directional derivative $D_{\vec{p}_i}f(\vec{x}_{k+1})=\grad f\TT(\vec{x}_{k+1})\vec{p}_i=0,\ i=1,\dotsc,k$, which means it is not only zero at the new point along the direction $\vec{p}_k$, but also zero along all the previous search direction $\vec{p}_1,\dotsc,\vec{p}_k$.

\paragraph{The Conjugate Gradients Algorithm}

The conjugate gradients algorithm is a special case of the conjugate vectors algorithm, where we choose vector $\vec{v}=-\grad f(\vec{x}_{k+1})$. According to subspace theorem the gradient at the new point $\vec{x}_{k+1}$ is orthogonal to $\vec{p}_i,\ i=1,\dotsc,k$ so it is linearly independent of $\vec{p}_1,\dotsc,\vec{p}_k$ and a valid choice for $\vec{v}$. The equation for the new search direction given by the Gram-Schmidt procedure is
\begin{align*}
	\vec{p}_{k+1}=-\grad f(\vec{x}_{k+1})+\sum_{i=1}^k\frac{\vec{p}_i\TT\mat{A}\grad f(\vec{x}_{k+1})}{\vec{p}_i\TT\mat{A}\vec{p}_i}\vec{p}_i
\end{align*}
Since $\grad f(\vec{x}_{k+1})$ is orthogonal to $\vec{p}_i,\ i=1,\dotsc,k$, by the subspace theorem we have $\vec{p}_{k+1}\TT\grad f(\vec{x}_{k+1})=-\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})$. So $\al_{k+1}$ can be written as
\begin{align*}
	\al_{k+1}=\frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\vec{p}_{k+1}\TT\mat{A}\vec{p}_{k+1}}
\end{align*}
We can show that
\begin{align*}
	\grad f(\vec{x}_{i+1})-\grad f(\vec{x}_i)&=\mat{A}\vec{x}_{i+1}-\vec{b}-(\mat{A}\vec{x}_i-b)=\mat{A}(\vec{x}_{i+1}-\vec{x_i})=\al_i\mat{A}\vec{p}_i \\
	\mat{A}\vec{p}_i&=\pfrac{(\grad f(\vec{x}_{i+1})-\grad f(\vec{x}_i))}{\al_i}\quad\text{since}\ \al_i\neq0
\end{align*}
Further,
\begin{align*}
	\vec{p}_i\TT\mat{A}\grad f(\vec{x}_{k+1})&=\grad f\TT(\vec{x}_{k+1})\mat{A}\vec{p}_i=\grad f\TT(\vec{x}_{k+1})\pfrac{(\grad f(\vec{x}_{i+1})-\grad f(\vec{x}_i))}{\al_i} \\
	&=\frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{i+1})-\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_i)}{\al_i} \\
	&=\casefill{0&\quad\text{if}\ 1\leq i<k \\ \frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\al_k}&\quad\text{if}\ i=k}
\end{align*}
Hence we can simplify the equations as
\begin{align*}
	\vec{p}_{k+1}&=-\grad f(\vec{x}_{k+1})+\frac{\pfrac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\al_k}}{\vec{p}_k\TT\mat{A}\vec{p}_k}\vec{p}_k \\
	&=-\grad f(\vec{x}_{k+1})+\frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\vec{p}_k\TT\mat{A}\vec{p}_k}\frac{\vec{p}_k\TT\mat{A}\vec{p}_k}{\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)}\vec{p}_k \\
	&=-\grad f(\vec{x}_{k+1})+\frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)}\vec{p}_k \\
	&=-\grad f(\vec{x}_{k+1})+\be_k\vec{p}_k\quad\text{where}\ \be_k=\frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)}
\end{align*}
We can conclude the conjugate gradients algorithm as
\begin{algorithm}[H]
	\caption{The Conjugate Gradients Algorithm}
	\begin{algorithmic}
		\State $k=1$
		\State input $\vec{x}_1$
		\State $\vec{p}_1=-\grad f(\vec{x}_1)$
		\While{$\grad f(\vec{x}_k)\neq0$}
			\State $\al_k:=-\pfrac{\grad f\TT(\vec{x}_k)\vec{p}_k}{\left(\vec{p}_k\TT\mat{A}\vec{p}_k\right)}$
			\State $\vec{x}_{k+1}:=\vec{x}_k+\al_k\vec{p}_k$
			\State $\be_k:=\pfrac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\left(\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)\right)}$
			\State $\vec{p}_{k+1}:=-\grad f(\vec{x}_{k+1})+\be_k\vec{p}_k$
			\State $k=k+1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}
An common alternative is the Polak-Ribiere formula:
\begin{align*}
	\be_k=\frac{\grad f\TT(\vec{x}_{k+1})\left(\grad f(\vec{x}_{k+1})-\grad f(\vec{x}_k)\right)}{\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)}
\end{align*}

\paragraph{Newton Methods}

Consider the Taylor expansion of a target function $f(\vec{x})$:
\begin{align*}
	f(\vec{x}+\ep\vec{v})=f(\vec{x})+\ep\grad f\T\vec{v}+\frac{\ep^2}{2}\vec{v}\T \hess_f\vec{v}+\O{\ep^3}
\end{align*}
Differentiating the RHS with respect to $\vec{v}$, we find the RHS has its lowest value when
\begin{align*}
	\grad f=-\ep\hess_f\vec{v}\impl\vec{v}=-\ep^{-1}\hess_f\inv\grad f
\end{align*}
Hence, an optimization routine to minimize $f$ is given by the Newton update
\begin{align*}
	\vec{x}_{k+1}=\vec{x}_k-\al_k\hess_f\inv\grad f
\end{align*}
Although Newton's method can help to find the minima in one step, for large scale problems the Hessian matrix is computationally demanding, especially if it is singular or nearly so.

\paragraph{Quasi-Newton Methods}

An alternative is to set up the iteration
\begin{align*}
	\vec{x}_{k+1}=\vec{x}_k-\al_k\mat{S}_k\grad f(\vec{x}_k)
\end{align*}
This is a very general form: if $\mat{S}_k=\hess_f\inv$ then we have Newton's method, while if $\mat{S}_k=\mat{I}$ we have steepest (gradient) descent. The idea behind most quasi-Newton methods is to try to construct an approximate inverse Hessian $\hess_k$ using information gathered as the descent progresses, and to set $\mat{S}_k=\hess_k$. As we have seen, for a quadratic optimization problem we have the relationship $\grad f(\vec{x}_{k+1})-\grad f(\vec{x}_k)=\mat{A}(\vec{x}_{k+1}-\vec{x}_k)$. Defining $\vec{s}_k=\vec{x}_{k+1}-\vec{x}_k$ and $\vec{y}_k=\grad f(\vec{x}_{k+1})-\grad f(\vec{x}_k)$, we see that the original equation becomes $\vec{y}_k=\mat{A}\vec{s}_k$. It is reasonable to demand that $\hess_{k+1}\vec{y}_i=\vec{s}_i,1\leq i\leq k$. After $n$ linearly independent steps we would then have $\hess_{n+1}=\mat{A}\inv$. For $k<n$ there are an infinity of solutions for $\hess_{k+1}$ satisfying the equation. We shall focus on one of the most popular, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update. This is given by
\begin{align*}
	\hess_{k+1}=\hess_k+\left(1+\frac{\vec{y}_k\TT\hess_k\vec{y}_k}{\vec{y}_k\TT\vec{s}_k}\right)\frac{\vec{s}_k\vec{s}_k\TT}{\vec{s}_k\TT\vec{y}_k}-\frac{\vec{s}_k\vec{y}_k\TT\hess_k+\hess_k\vec{y}_k\vec{s}_k\TT}{\vec{s}_k\TT\vec{y}_k}
\end{align*}
This is a rand-2 correction to $\hess_k$ constructed from the vectors $\vec{s}_k$ and $\hess_k\vec{y}_k$. The quasi-Newton algorithm is
\begin{algorithm}[H]
	\caption{The Quasi-Newton Algorithm}
	\begin{algorithmic}
		\State input $\vec{x}_1$
		\State set $\hess_i=\mat{I}$
		\For{$k=1,\dotsc,n$}
			\State $\vec{p}_k=-\hess_k\grad f(\vec{x}_k)$
			\State compute $\al_k$ to minimize $f(\vec{x}_k+\al_k\vec{p}_k)$
			\State $\vec{s}_k=\vec{x}_{k+1}-\vec{x}_k$, $\vec{y}_k=\grad f(\vec{x}_{k+1})-\grad f(\vec{x}_k)$, and update $\hess_{k+1}$
			\State $k=k+1$
		\EndFor
	\end{algorithmic}
\end{algorithm}
Properties of the BFGS update:
\begin{enumerate}
	\item All the $\hess$'s are symmetric.
	\item All the $\hess$'s are positive definite.
	\item The direction vectors $\vec{p}_1,\dotsc,\vec{p}_k$ produced by the algorithm obey $\vec{p}_i\TT\mat{A}\vec{p}_j=0,1\leq i<j\leq k$ and $\hess_{k+1}\mat{A}\vec{p}_i=\vec{p}_i,1\leq i\leq k$.
\end{enumerate}

\subsection{General Functions}

The basic strategy for the minimization of general (i.e. non-quadratic) functions is to apply the same algorithms as in the quadratic case, except that exact line-minimizations (the calculation of the $\al_k$'s) have to be replaced with inexact line-searches.

For the conjugate gradient (CG) method, the exact formula used to compute $\be_k$ may be important in this respect. For example, the two possible formulae are
\begin{align*}
	\be_k&=\frac{\grad f\TT(\vec{x}_{k+1})\grad f(\vec{x}_{k+1})}{\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)} && \text{Fletcher-Reeves} \\
	\be_k&=\frac{\grad f\TT(\vec{x}_{k+1})\left(\grad f(\vec{x}_{k+1})-\grad f(\vec{x}_k)\right)}{\grad f\TT(\vec{x}_k)\grad f(\vec{x}_k)} && \text{Polak-Ribiere}
\end{align*}
These formulae are equivalent for quadratic functions, however, they can have quite different effects in the non-quadratic case. If the algorithm is not making progress then we can expect $\grad f(\vec{x}_{k+1})\simeq\grad f(\vec{x}_k)$, and hence that $\be_k$ computed by the Polak-Ribiere formula will be near 0, and hence that $\vec{p}_{k+1}\simeq-\grad f(\vec{x}_{k+1})$. On the other hand, under the same circumstances the Fletcher-Reeves update will set $\be_k\simeq1$.

For general functions it can be shown that the BFGS quasi-Newton (QN) method:
\begin{enumerate}
	\item preserves positive definite matrices $\hess_k$, and hence that the descent property holds;
	\item requires $\O{n^2}$ multiplications per iteration;
	\item has superlinear order of convergence;
	\item has global convergence for strictly convex functions (with exact line searches).
\end{enumerate}

A comparison of the CG and QN algorithms shows that CG methods require only $\O{n}$ storage, as opposed to $\O{n^2}$ for QN methods. Typically this means that QN methods are preferred for problems of relatively small size, while CG methods are more suitable for very large problems. QN methods appear to be more tolerant of moderate precision line-searches compared to CG methods, and CG methods are less efficient and less robust than QN methods on problems to which they can both be applied.

\subsection{Optimization with Constraints}

\paragraph{Constraint Types}

There are two types of constraints:
\begin{enumerate}
	\item Equality (type A). $c(\vec{x})=0$. These reduce the dimension of the search space.
	\item Inequality (type B). $c(\vec{x})\geq0$ or $c(\vec{x})>0$. These reduce the volume of the search space, but do not affect the dimension.
\end{enumerate}

\paragraph{Transformation Methods}

For some type A constraints, it is possible to solve for one variable $x_i$ in terms of the other coordinates of $\vec{x}$. If this can be done, then $x_i$ can be eliminated from $f$, and the whole problem reduced to one of lower dimension with fewer constraints.

Some type B constraints can be removed by simple variable substitutions so that they are automatically satisfied. For example, $x_i\geq0$ can be removed by substituting $x_i=y_i^2$ or $x_i=e^{y_i}$, while $a<x_i<b$ can be removed by substituting $x_i=\pfrac{(a+b)}{2}+\ppfrac{(a-b)}{2}\tanh y_i$. Another useful substitution is the softmax function. Minimizing $f(\vec{x})$ subject to the constraint $\sum_{i=1}^nx_i=1$, where $0\leq x_i\leq1$, for $i=1,\dotsc,n$ can be transformed to minimizing $f$ with $x_i=\pfrac{e^{y_i}}{\sum_{j=1}^ne^{y_j}}$.

\paragraph{Lagrange Multipliers (Single Constraint)}

Consider the problem of minimizing $f(\vec{x})$ subject to a single constraint $c(\vec{x})=0$. Assume we already identified an $\vec{x}$ that satisfies the constraint, what we are allowed is to search for lower function values around this $\vec{x}$ in directions which are consistent with te constraint. That is, $c(\vec{x}+\vec{\de})=0$. We know that $c(\vec{x}+\vec{\de})\approx c(\vec{x})+\vec{\de}\cdot\grad c(\vec{x})$. Hence, in order that the constraint remains satisfies, we can only search in a direction such that $\vec{\de}\cdot\grad c(\vec{x})=0$, which means in the direction $\vec{\de}$ is orthogonal to $\grad c(\vec{x})$.

The change in $f$ along a direction $\vec{n}$ where $\vec{n}\cdot\grad c(\vec{x})=0$ is $f(\vec{x}+\ep\vec{n})\approx f(\vec{x})+\ep\grad f(\vec{x})\cdot\vec{n}$. Since we are looking for a point $\vec{x}$ that minimizes the function $f$, we require $\vec{x}$ to be a stationary point, $\grad f(\vec{x})\cdot\vec{n}=0$. That is, $\grad f(\vec{x})$ must lie parallel to $\grad c(\vec{x})$, so that $\grad f(\vec{x})=\la\grad c(\vec{x})$ for some $\la\in\bR$.

To solve the optimization problem therefore, we look for a point $\vec{x}$ such that $\grad f(\vec{x})=\la\grad c(\vec{x})$, for some $\la$, and for which $c(\vec{x})=0$. An alternative formulation of this dual requirement is to look for $\vec{x}$ and $\la$ that jointly minimize the \emph{Lagrangian}:
\begin{align*}
	\cL(\vec{x},\la)=f(\vec{x})-\la c(\vec{x})
\end{align*}
Differentiating with respect to $\vec{x}$, we get the requirement $\grad f(\vec{x})=\la\grad c(\vec{x})$, and differentiating with respect to $\la$, we get that $c(\vec{x})=0$.

\paragraph{Lagrange Multipliers (Multiple Constraints)}

Consider the problem of optimizing $f(\vec{x})$ subject to the constraints $c_i(\vec{x})=0,i=1,\dotsc,r<n$, where $n$ is the dimensionality of the space. We can apply the \emph{Lagrangian function} $\cL:\bR^{n+r}\to\bR$,
\begin{align*}
	\cL(\vec{x},\vec{\la})=f(\vec{x})-\sum_i\la_ic_i(\vec{x})
\end{align*}

\paragraph{Computational Approaches to Constrained Optimization}

A computational approach to the non-linear constrained optimization problem is the penalty and barrier function methods. Suppose we are minimizing a function $f(\vec{x})$ subject to constraints $c_i(\vec{x})=0,i=1,\dotsc,r$. We can construct a quadratic penalty function
\begin{align*}
	P_Q(\vec{x},\rh)=f(\vec{x})+\frac{\rh}{2}\vec{c}(\vec{x})\T\vec{c}(\vec{x})
\end{align*}
where $\vec{c}(\vec{x})$ is the vector of constraints $[c_1(\vec{x})\ \cdots\ c_r(\vec{x})]\T$ and $\rh$ is the penalty parameter. Note that the penalty term is continuously differentiable. Under mild technical conditions, if $\vec{x}^*(\rh)$ denotes an unconstrained munimum of $P_Q$ then it can be shown that $\lim_{\rh\to\infty}\vec{x}^*(\rh)=\vec{x}^*$ where $\vec{x}^*$ is the constrained minimum of $f$.