%!TEX root = ../notebook.tex
% Chapter 2

\chapter{Mathematics Basics}
\label{chapter2}

\section{Probability}
\label{section2.1}

\subsection{Basic Rules}

\begin{description}[leftmargin=0cm]
\item[Three Axioms of Probability] Let $\Om$ be a sample space. A probability assigns a real number $\P{X}$ to each event $X \subseteq \Om$ in such a way that
	\begin{enumerate}
	\item $\P{X} \geq 0, \forall X$
	\item If $X_1, X_2, \dotsc$ are pairwise disjoint events ($X_1 \cap X_2=\emptyset,\ i \ne j,\ i,j=1,2,\dotsc$), then $\P{\bigcup_{i=1}^{\infty}X_i}=\sum_{i=1}^{\infty}\P{X_i}$. (This property is called countable additivity.)
	\item $\P{\Om}=1$
	\end{enumerate}
\item[Joint Probability] The probability both event A and B occur. $\P{X,Y}=\P{X \cap Y}$.
\item[Marginalization] The probability distribution of any variable in a joint distribution can be recovered by integrating (or summing) over the other variables.
	\begin{enumerate}
	\item For continuous r.v. $\P{x}=\int\P{x,y}dy$ ; $\P{y}=\int\P{x,y}dx$.
	\item For discrete r.v. $\P{x}=\sum_y\P{x,y}$ ; $\P{y}=\sum_x\P{x,y}$.
	\item For mixed r.v. $\P{x,y}=\sum_w\int\P{w,x,y,z}dz$, where $w$ is discrete and $z$ is continuous.
	\end{enumerate}
\item[Conditional Probibility] $\P{X=x|Y=y}$ is the probability $X=x$ occurs given the knowledge $Y=y$ occurs. Conditional probability can be extracted from joint probability that
	\begin{align*}
	\P{x|y=y^{*}}=\frac{\P{x,y=y^{*}}}{\int\P{x,y=y^{*}}dx}=\frac{\P{x,y=y^{*}}}{\P{y=y^{*}}}
	\end{align*}

Usually, the formula is written as $\P{x|y}=\frac{\P{x,y}}{\P{y}}$.
\item[Product Rule] The formula can be rearranged as $\P{x,y}=\P{x|y}\P{y}=\P{y|x}\P{x}$. In case of multiple variables
	\begin{align*}
	\P{w,x,y,z}&=\P{w,x,y|z}\P{z} \\
			   &=\P{w,x|y,z}\P{y|z}\P{z} \\
			   &=\P{w|x,y,z}\P{x|y,z}\P{y|z}\P{z}
	\end{align*}
\item[Independence] If two variables $x$ and $y$ are independent, then r.v. $x$ tells nothing about r.v. $y$ (and vice-versa)
	\begin{align*}
	\P{x|y}&=\P{x} \\
	\P{y|x}&=\P{y} \\
	\P{x,y}&=\P{x}\P{y}
	\end{align*}
\item[Baye's Rule] By rearranging formula in Product Rule, we have
	\begin{align*}
	\P{y|x}&=\frac{\P{x|y}\P{y}}{\P{x}} \\
		   &=\frac{\P{x|y}\P{y}}{\int\P{x,y}dy} \\
		   &=\frac{\P{x|y}\P{y}}{\int\P{x|y}\P{y}dy}
	\end{align*}
\item[Expectation] Expectation tells us the excepted or average value of some function $f(x)$, taking into account the distribution of $x$.
	\begin{align*}
	\E{f(x)}&=\sum_x f(x)\P{x} \\
	\E{f(x)}&=\int f(x)\P{x}dx
	\end{align*}
Definition in two dimensions: $\E{f(x,y)}=\iint f(x,y)\P{x,y}dx\ dy$
	\begin{table*}[!h]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			Function $f(\bullet)$ & Expectation \\
			\hline
			$x^k$ & $k^{th}$ moment about zero \\
			\hline
			$(x-\mu_x)^k$ & $k^{th}$ moment about the mean \\
			\hline
		\end{tabular}
		\
		\begin{tabular}{|c|c|}
			\hline
			Function $f(\bullet)$ & Expectation \\
			\hline
			$x$ & mean, $\mu_x$ \\
			\hline
			$(x-\mu_x)^2$ & variance \\
			\hline
			$(x-\mu_x)^3$ & skew \\
			\hline
			$(x-\mu_x)^4$ & kurtosis \\
			\hline
			$(x-\mu_x)(x-\mu_y)$ & covariance of $x$ and $y$ \\
			\hline
		\end{tabular}
	\end{table*}

Besides, Expectation has the following four rules
	\begin{enumerate}
		\item Expected value of a constant is the constant $\E{\kappa}=\kappa$.
		\item Expected value of constant times function is constant times excepted value of function $\E{kf(x)}=k\E{f(x)}$.
		\item Expectation of sum of functions is sum of expectation of functions \\ $\E{f(x)+g(y)}=\E{f(x)}+\E{g(x)}$.
		\item Expectation of product of functions in variables $x$ and $y$ is product of expectations of functions if $x$ and $y$ are independent $\E{f(x)g(y)}=\E{f(x)}\E{g(y)}, x \indep y$.
	\end{enumerate}
\end{description}

\subsection{Common Probability Distributions}

\begin{description}[leftmargin=0cm]
\item[Bernoulli] Bernoulli distribution describes situation where only two possible outcomes $y=0 / y=1$ or failure/success.
	\begin{enumerate}
		\item $\P{x}=\Bern_x[\la]=\la^x(1-\la)^{1-x}$
		\item univariate, discrete, binary
		\item $x \in \{0,1\}$; $\la \in [0,1]$
		\item $\E{x}=\la$, $\V{x}=\la(1-\la)$
	\end{enumerate}
\item[Beta] Beta distribution is the conjugate distribution to Bernoulli distribution.
	\begin{enumerate}
		\item $\P{\la}=\Beta_\la[\al,\be]=\frac{\Ga(\al+\be)}{\Ga(\al)\Ga(\be)}\la^{\al-1}(1-\la)^{\be-1}$
		\item univariate, continuous, unbounded
		\item $\la \in \bR$; $\al \in \bR_+$,  $\be \in \bR_+$
		\item $\E{\la}=\frac{\al}{\al+\be}$, $\V{\la}=\frac{\al\be}{(\al+\be)^2(\al+\be+1)}$
	\end{enumerate}
\item[Categorical] Categorical distribution describes situation with K possible outcomes.
	\begin{enumerate}
		\item $\P{x}=\Cat_x[\vec{\la}]$, $\P{x=k}=\la_k$,  $\P{\vec{x}=\vec{e}_k}=\prod_{j=1}^K\la_j^{x_j}=\la_k$
		\item univariate, discrete, multi-valued
		\item $x \in \{1,2,\dotsc,K\}$; $\la_k \in [0,1]$ where $\sum_k\la_k=1$
		\item $\E{x_i}=\la_i$, $\V{x_i}=\la_i(1-\la_i)$, $\C{x_i,x_j}=-\la_i\la_j\ (i \neq j)$
	\end{enumerate}
\item[Dirichlet] Dirichlet distribution is the conjugate distribution to categorical distribution.
	\begin{enumerate}
		\item $\P{\vec{\la}}=\Dir_{\vec{\la}}[\vec{\al}]=\frac{\Ga(\sum_{k=1}^K\al_k)}{\prod_{k=1}^K\Ga(\al_k)}\prod_{k=1}^K\la_k^{\al_k-1}$
		\item multivariate, continuous, bounded, sums to one
		\item $\vec{\la}=[\la_1,\la_2,\dotsc,\la_K]\T$, $\la_k \in [0,1]$, $\sum_{k=1}^K\la_k=1$; $\vec{\al}=[\al_1,\al_2,\dotsc,\al_K]$, $\al_k \in \bR_+$
		\item $\E{\la_i}=\frac{\al_i}{\sum_k\al_k}$, $\V{\la_i}=\frac{\al_i(\sum_k\al_k-\al_i)}{(\sum_k\al_k)^2(\sum_k\al_k+1)}$, $\C{\la_i,\la_j}=\frac{-\al_i\al_j}{(\sum_k\al_k)^2(\sum_k\al_k+1)}\ (i \neq j)$
	\end{enumerate}
\item[Univariate Normal] Univariate normal distribution describes single continuous variable.
	\begin{enumerate}
		\item $\P{x}=\Norm_x[\mu,\si^2]=\frac{1}{\sqrt{2\pi\si^2}}\e{-\frac{(x-\mu)^2}{2\si^2}}$
		\item univariate, continuous, unbounded
		\item $x \in \bR$; $\mu \in \bR$, $\si^2 \in \bR_+$
		\item $\E{x}=\mu$, $\V{x}=\si^2$
	\end{enumerate}
\item[Normal Inverse Gamma] Normal inverse gamma distribution is a conjugate distribution to univariate normal distribution.
	\begin{enumerate}
		\item $\P{\mu,\si^2}=\NIG_{\mu,\si^2}[\al,\be,\ga,\de]=\frac{\sqrt{\ga}}{\sqrt{2\pi\si^2}}\frac{\be^\al}{\Ga(\al)}\left(\frac{1}{\si^2}\right)^{\al+1}\e{-\frac{2\be+\ga(\de-\mu)^2}{2\si^2}}$
		\item bivariate, continuous, $\mu$ unbounded, $\si^2$ bounded below
		\item $\mu \in \bR$, $\si^2 \in \bR_+$; $\al \in \bR_+$, $\be \in \bR_+$, $\ga \in \bR_+$, $\de \in \bR$
		\item $\E{\mu}=\de$, $\E{\si^2}=\frac{\be}{\al-1}\ (\al>1)$, $\V{\mu}=\frac{\be}{(\al-1)\ga}\ (\al>1)$, $\V{\si^2}=\frac{\be^2}{(\al-1)^2(\al-2)}\ (\al>2)$, $\C{\mu,\si^2}=0\ (\al>1)$
	\end{enumerate}
\item[Multivariate Normal] Multivariate normal distribution describes multiple continuous variables. It takes two parameters: a vector containing mean position $\vec{\mu}$, and a symmetric positive definite covariance matrix $\cov$.
	\begin{enumerate}
		\item $\P{\vec{x}}=\Norm_{\vec{x}}[\vec{\mu},\cov]=\frac{1}{\sqrt{det(2\pi\cov)}}\e{-\frac{1}{2}(\vec{x}-\vec{\mu})\T\cov\inv(\vec{x}-\vec{\mu})}$
		\item multivariate, continuous, unbounded
		\item $\vec{x} \in \bR^K$; $\vec{\mu} \in \bR^K$, $\cov \in \bR^{K \times K}$ (positive semi-definite matrix)
		\item $\E{\vec{x}}=\vec{\mu}$, $\V{\vec{x}}=\cov$
	\end{enumerate}
\item[Normal Inverse Wishart] Normal inverse wishart distribution is a conjugate distribution to multivariate normal distribution.
	\begin{enumerate}
		\item $\P{\vec{\mu},\cov}=\NIW_{\vec{\mu},\cov}[\al,\vec{\Ps},\ga,\vec{\de}] \\ =\frac{\ga^{D/2}\det{\vec{\Ps}}^{\al/2}\det{\cov}^{-\frac{\al+D+2}{2}}}{(2\pi)^{D/2}2^{(\al\cov)/2}\Ga_D(\al/2)}\e{-\frac{1}{2}(\Tr(\vec{\Ps}\cov\inv)+\ga(\vec{\mu}-\vec{\de})\T\cov\inv(\vec{\mu}-\vec{\de})}$
		\item multivariate, $\vec{\mu}$ unbounded, $\cov$ square, positive definite
		\item $\vec{\mu} \in \bR^K$, $\cov \in \bR^{K \times K}$; $\al \in \bR_{>D-1}$, $\vec{\Ps} \in \bR^{K \times K}$, $\ga \in \bR_+$, $\vec{\de} \in \bR^K$
	\end{enumerate}
\end{description}

\section{Linear Algebra}
\label{section2.2}

\subsection{Vectors}

\subsubsection*{Vectors Addition}

\begin{align*}
	\vec{v}+\vec{w}=\begin{pmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_n
	\end{pmatrix}+\begin{pmatrix}
	w_1 \\ w_2 \\ \vdots \\ w_n
	\end{pmatrix}=\begin{pmatrix}
	v_1+w_1 \\ v_2+w_2 \\ \vdots \\ v_n+w_n
	\end{pmatrix}
\end{align*}

\subsubsection*{Vectors Scaling}

\begin{align*}
	a\vec{v}=a\begin{pmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_n
	\end{pmatrix}=\begin{pmatrix}
	av_1 \\ av_2 \\ \vdots \\ av_n
	\end{pmatrix}
\end{align*}

\subsubsection*{Rules for Vectors Addition and Scaling}

\begin{enumerate}
	\item $\vec{u}+\left(\vec{v}+\vec{w}\right)=\left(\vec{u}+\vec{v}\right)+\vec{w}$
	\item $\vec{v}+\vec{w}=\vec{w}+\vec{v}$
	\item There is a vector $\vec{0}$ such that $\vec{0}+\vec{v}=\vec{v}$ for all $\vec{v}$
	\item For every vector $\vec{v}$ there is a vector $-\vec{v}$ so that $\vec{v}+\left(-\vec{v}\right)=\vec{0}$
	\item $a\left(b\vec{v}\right)=\left(ab\right)\vec{v}$
	\item $1\vec{v}=\vec{v}$
	\item $a\left(\vec{v}+\vec{w}\right)=a\vec{v}+a\vec{w}$
	\item $\left(a+b\right)\vec{v}=a\vec{v}+b\vec{v}$
\end{enumerate}

\subsubsection*{Linear Combination \& Span}

Linear combination (e.g. in 2D space):
\begin{align*}
	a\vec{v}+b\vec{w}
\end{align*}
The span of $\vec{v}$ and $\vec{w}$ is the set of all their linear combinations.

\subsubsection*{Representation of Basis}

In Euclidean:
\begin{align*}
\begin{pmatrix}
v_1 \\ v_2 \\ \vdots \\ v_n
\end{pmatrix}=v_1\begin{pmatrix}
1 \\ 0 \\ \vdots \\ 0
\end{pmatrix}+v_2\begin{pmatrix}
0 \\ 1 \\ \vdots \\ 0
\end{pmatrix}+\dots+v_n\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 1
\end{pmatrix}
\end{align*}
We can write this as
\begin{align*}
	\vec{v}=v_1\vec{e}^1+v_2\vec{e}^2+\dots+v_n\vec{e}^n
\end{align*}
In different basis, we choose other basis vector and then write the same vector
\begin{align*}
	\vec{v}=w_1\vec{b}^1+w_2\vec{b}^2+\dots+w_n\vec{b}^n
\end{align*}
If these basis vectors are orthonormal, $w_i=\vec{v}\T\vec{b}^i$

\subsubsection*{Linear Dependence}

\begin{enumerate}
	\item Linearly dependent: A set of vectors $\vec{v}^1,\dotsc,\vec{v}^n$ is linearly dependent if there exists a vector $\vec{v}^j$ that can be expressed as a linear combination of the other vectors. (The vector $\vec{v}^j$ is already located in the span of other vectors)
	\item Linearly Independent: Each vector really does add another dimension to the span. And the only solution to $\sum_{i=1}^na_i\vec{v}^i=\vec{0}$ is for all $a_i=0, i=1,\dotsc,n$.
\end{enumerate}

\subsubsection*{Dot Products}

\begin{align*}
	\vec{v}\cdot\vec{w}=\sum_{i=1}^nv_iw_i=\vec{v}\T\vec{w}
\end{align*}
The length of a vector is denoted as $\norm{\vec{v}}$, the squared length is given by
\begin{align*}
	\norm{\vec{v}}^2=\vec{v}\T\vec{v}=\vec{v}^2=v_1^2+v_2^2+\dots+v_n^2
\end{align*}
A natural geometric interpretation of dot products is
\begin{align*}
	\vec{v}\cdot\vec{w}=\norm{\vec{v}}\norm{\vec{w}}\cos\th
\end{align*}
where $\th$ is the angle between two vectors.

\subsection{Matrices (Square Matrices)}

\subsubsection*{Linear Transformation}

\begin{align*}
	\vec{v}'=\left(\mat{M}_n\dots\mat{M}_2\mat{M}_1\right)\vec{v}
\end{align*}
Linear transformation always maps linear subspaces onto linear subspaces (possibly of a lower dimension). Intuitively, linear transformation keeps the grid lines stay parallel and evenly spaced, and so that the origin remains fixed.

\subsubsection*{Basic Formulae}

\begin{enumerate}
	\item $\mat{A}(\mat{B}+\mat{C})=\mat{A}\mat{B}+\mat{A}\mat{C}$
	\item $(\mat{A}+\mat{B})\T=\mat{A}\T+\mat{B}\T$
	\item $(\mat{A}\mat{B})\T=\mat{B}\T\mat{A}\T$
	\item[] if individual inverses exist:
	\item $(\mat{A}\mat{B})\inv=\mat{B}\inv\mat{A}\inv$
	\item $(\mat{A}\inv)\T=(\mat{A}\T)\inv$
\end{enumerate}

\subsubsection*{Trace}

Trace is the sum of diagonal elements of matrix $\mat{M}$,
\begin{align*}
	\Tr(\mat{M})=\Tr(\mat{M}\T)=\sum_{i=1}^n\mat{M}_{ii}=\sum\text{eigenvalues of $\mat{M}$}
\end{align*}
Cyclic permutations in trace,
\begin{align*}
	\Tr(\mat{A}\mat{B}\mat{C})=\Tr(\mat{C}\mat{A}\mat{B})=\Tr(\mat{B}\mat{C}\mat{A})
\end{align*}

\subsubsection*{Determinants}

The determinant of a matrix $\mat{M}$, denoted as $\det{\mat{M}}$, is a function mapping matrices to scalars, measuring how much multiplication by the matrix expands or contracts space. If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is 1, then the transformation preserves volume.

Some properties of determinant:
\begin{enumerate}
	\item $\det{\mat{M}}=\prod\text{eigenvalues of $\mat{M}$}$
	\item $\det{\mat{A}\mat{B}}=\det{\mat{A}}\det{\mat{B}}$
	\item $\det{a}=a$
	\item $\det{a\mat{M}}=a^n\det{\mat{M}}$
	\item $\det{\mat{M}\inv}=\det{\mat{M}}^{-1}$
\end{enumerate}

\subsubsection*{Symmetric Matrix}

\begin{align*}
	\mat{M}=\mat{M}\T
\end{align*}

\subsubsection*{Orthogonal Matrix}

\begin{align*}
	\mat{M}\T\mat{M}&=\mat{M}\mat{M}\T=\mat{I} \\
	\mat{M}\inv&=\mat{M}\T
\end{align*}

\subsubsection*{Eigendecomposition}

If a matrix $\mat{M}$ satisfies
\begin{align*}
	\mat{M}\vec{v}=\la\vec{v}
\end{align*}
then $\vec{v}$ is the eigenvector of $\mat{M}$, and $\la$ is the eigenvalue of $\mat{M}$ (The vector $\vec{v}$ is just scaled by value $\la$ after a linear transformation $\mat{M}$). To solve the equation, we can rewrite it as
\begin{align*}
	(\mat{M}-\la\mat{I})\vec{v}=\vec{0}
\end{align*}
When $\det{\mat{M}-\la\mat{I}}=0$, it means at least one dimension is contracted by the linear transformation $\mat{M}-\la\mat{I}$ so that there exists at least one non-zero vector to be transformed to zero, which gives us a solution to $\la$.

Suppose matrix $\mat{M}$ has $n$ linearly independent eigenvectors, $\{\vec{v}^{(1)},\dotsc,\vec{v}^{(n)}\}$, with corresponding eigenvalues $\{\la_1,\dotsc,\la_n\}$. We may concatenate all of the eigenvectors to form a matrix $\mat{V}$ with one eigenvector per column: $\mat{V}=[\vec{v}^{(1)},\dotsc,\vec{v}^{(n)}]$. Likewise, we can concatenate the eigenvalues to form a vector $\vec{\la}=[\la_1,\dotsc,\la_n]$. The eigendecomposition of $\mat{M}$ is then given by
\begin{align*}
	\mat{M}=\mat{V}\diag(\vec{\la})\mat{V}\inv
\end{align*}

Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:
\begin{align*}
	\mat{M}=\mat{Q}\mat{\La}\mat{Q}\T
\end{align*}
where $\mat{Q}$ is an orthogonal matrix composed of eigenvectors of $\mat{M}$, and $\mat{\La}$ is a diagonal matrix.

A matrix is {\bf singular} if and only if any of the eigenvalues are zero ($\det{\mat{M}}=0$). A matrix whose eigenvalues are all positive is called {\bf positive definite}. A matrix whose eigenvalues are all positive or zero-valued (non-negative) is called {\bf positive semi-definite}. Likewise, if all eigenvalues are negative, the matrix is {\bf negative definite}, and if all eigenvalues are negative or zero-valued (non-positive), it is {\bf negative semi-definite}. Positive semi-definite matrices guarantee that $\forall\vec{v}, \vec{v}\T\mat{M}\vec{v}\geq0$. Positive definite matrices additionally guarantee that $\vec{v}\T\mat{M}\vec{v}=0\impl\vec{v}=\vec{0}$.

\subsubsection*{Singular Value Decomposition (SVD)}

The singular value decomposition of a $m\times n$ matrix $\mat{M}$ can be formed as
\begin{align*}
	\mat{M}=\mat{U}\mat{\Si}\mat{V}\T
\end{align*}
where $\mat{U}$ is a $m\times m$ orthogonal matrix, $\mat{\Si}$ is a $m\times n$ diagonal matrix, and $\mat{V}$ is a $n\times n$ orthogonal matrix. The elements along the diagonal of $\mat{\Si}$ are known as the {\bf singular values} of the matrix $\mat{M}$. The columns of $\mat{U}$ are known as the {\bf left-singular vectors}. The columns of $\mat{V}$ are known as as the {\bf right-singular vectors}.

There are relations between SVD and eigendecomposition. According to the definition of SVD, we have
\begin{align*}
	\mat{M}\T\mat{M}&=\mat{V}\mat{\Si}\T\mat{U}\T\mat{U}\mat{\Si}\mat{V}\T=\mat{V}(\mat{\Si}\T\mat{\Si})\mat{V}\T \\
	\mat{M}\mat{M}\T&=\mat{U}\mat{\Si}\mat{V}\T\mat{V}\mat{\Si}\T\mat{U}\T=\mat{U}(\mat{\Si}\mat{\Si}\T)\mat{U}\T
\end{align*}
Hence, we can conclude that
\begin{enumerate}
	\item The right-singular vectors of $\mat{M}$ are the eigenvectors of $\mat{M}\T\mat{M}$.
	\item The left-singular vectors of $\mat{M}$ are the eigenvectors of $\mat{M}\mat{M}\T$.
	\item The non-zero singular values of $\mat{M}$ are the square roots of the eigenvalues of $\mat{M}\T\mat{M}$ or $\mat{M}\mat{M}\T$.
\end{enumerate}

\subsubsection*{Moore-Penrose Pseudoinverse}

SVD can be applied to compute the pseudoinverse of a matrix $\mat{M}=\mat{U}\mat{\Si}\mat{V}\T$, given by
\begin{align*}
	\mat{M}\pinv=\mat{V}\mat{\Si}\pinv\mat{U}\T
\end{align*}
where $\mat{\Si}\pinv$ is the pseudoinverse of $\mat{\Si}$, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix.

\section{Calculus}
\label{section2.3}

\subsection{Differentiation}

\subsubsection*{Basic Formulae}

Here, $f(x)$ and $g(x)$ are differentiable functions (the derivative exists), $c$ and $n$ are any real numbers.
\begin{enumerate}
	\item $(cf)'=cf'(x)$
	\item $(f\pm g)'=f'(x)\pm g'(x)$
	\item $(fg)'=f'g+fg'$ (Product Rule)
	\item $(\frac{f}{g})'=\frac{f'g-fg'}{g^2}$ (Quotient Rule)
	\item $\der{x}(c)=0$
	\item $\der{x}(x^n)=nx^{n-1}$ (Power Rule)
	\item $\der{x}(f(g(x)))=f'(g(x))g'(x)$ (Chain Rule)
\end{enumerate}

\subsubsection*{Mean Value Theorem}

If $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$ then there is a number $a<c<b$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$.

\subsubsection*{Newton's Method}

If $x_n$ is the $n^\text{th}$ guess for the root/solution of $f(x)=0$ then $(n+1)^\text{st}$ guess is $x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$ provided $f'(x_n)$ exists.

\subsubsection*{Taylor Series}

The Taylor series of a real or complex-valued function $f(x)$ that is infinitely differentiable at a real or complex number a is the power series
\begin{align*}
	f(a)+\frac{f'(a)}{1!}(x-a)+\frac{f''(a)}{2!}(x-a)^2+\dots+\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{align*}
where $n!$ denotes the factorial of $n$ and $f^{n}(a)$ denotes the $n^\text{th}$ derivative of $f$ evaluated at the point $a$. In the more compact sigma notation, this can be written as 
\begin{align*}
	\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{align*}

\subsection{Integration}

\subsubsection*{Properties}

\begin{enumerate}
	\item $\int f(x)\pm g(x)\ds{x}=\int f(x)\ds{x}\pm\int g(x)\ds{x}$
	\item $\int_a^b f(x)\pm g(x)\ds{x}=\int_a^b f(x)\ds{x}\pm\int_a^b g(x)\ds{x}$
	\item $\int_a^a f(x)\ds{x}=0$
	\item $\int_a^b=-\int_b^a f(x)\ds{x}$
	\item $\int_a^b f(x)\ds{x}=\int_a^c f(x)\ds{x}+\int_c^b f(x)\ds{x}$
	\item $\int kf(x)\ds{x}=k\int f(x)\ds{x}$
	\item $\int_a^b kf(x)\ds{x}=k\int_a^b f(x)\ds{x}$
	\item $\int_a^b k\ds{x}=k(b-a)$
	\item $\ab{\int_a^b f(x)\ds{x}}\leq\int_a^b\ab{f(x)}\ds{x}$
	\item If $f(x)\geq g(x)$ on $a\leq x\leq b$ then $\int_a^b f(x)\ds{x}\geq\int_a^b g(x)\ds{x}$
	\item If $f(x)\geq0$ on $a\leq x\leq b$ then $\int_a^b f(x)\ds{x}\geq0$
	\item If $m\leq f(x)\leq M$ on $a\leq x\leq b$ then $m(b-a)\leq\int_a^b f(x)\ds{x}\leq M(b-a)$
\end{enumerate}

\subsubsection*{Average Function Value}

The average value of $f(x)$ on $a\leq x\leq b$ is
\begin{align*}
	f_{avg}=\frac{1}{b-a}\int_a^b f(x)\ds{x}
\end{align*}

\subsubsection*{Jensen's inequality}

If $\vph$ is a convex function,
\begin{align*}
	\vph\left(\frac{1}{b-a}\int_a^b f(x)\ds{x}\right)\leq\frac{1}{b-a}\int_a^b\vph\left(f(x)\right)\ds{x}
\end{align*}
Additionally, is $X$ is an integrable real-valued random variable, then
\begin{align*}
	\vph\left(\E{X}\right)\leq\E{\vph(X)}
\end{align*}

\subsection{Multivariate Calculus}

\section{Informatics}
\label{section2.4}

\subsection{Entropy}

\section{Optimization}
\label{section2.5}

\subsection{One-dimensional Minimization}

\subsection{Gradient Descent}

\subsection{Quadratic Functions}

\subsection{General Functions}

\subsection{Optimization with Constraints}