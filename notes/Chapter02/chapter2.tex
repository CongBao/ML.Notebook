%!TEX root = ../notebook.tex
% Chapter 2

\chapter{Mathematics Basics}
\label{chapter2}

\section{Probability}
\label{section2.1}

\subsection{Basic Rules}

\begin{description}[leftmargin=0cm]
\item[Three Axioms of Probability] Let $\Om$ be a sample space. A probability assigns a real number $\P{X}$ to each event $X \subseteq \Om$ in such a way that
	\begin{enumerate}
	\item $\P{X} \geq 0, \forall X$
	\item If $X_1, X_2, \dotsc$ are pairwise disjoint events ($X_1 \cap X_2=\emptyset,\ i \ne j,\ i,j=1,2,\dotsc$), then $\P{\bigcup_{i=1}^{\infty}X_i}=\sum_{i=1}^{\infty}\P{X_i}$. (This property is called countable additivity.)
	\item $\P{\Om}=1$
	\end{enumerate}
\item[Joint Probability] The probability both event A and B occur. $\P{X,Y}=\P{X \cap Y}$.
\item[Marginalization] The probability distribution of any variable in a joint distribution can be recovered by integrating (or summing) over the other variables.
	\begin{enumerate}
	\item For continuous r.v. $\P{x}=\int\P{x,y}dy$ ; $\P{y}=\int\P{x,y}dx$.
	\item For discrete r.v. $\P{x}=\sum_y\P{x,y}$ ; $\P{y}=\sum_x\P{x,y}$.
	\item For mixed r.v. $\P{x,y}=\sum_w\int\P{w,x,y,z}dz$, where $w$ is discrete and $z$ is continuous.
	\end{enumerate}
\item[Conditional Probibility] $\P{X=x|Y=y}$ is the probability $X=x$ occurs given the knowledge $Y=y$ occurs. Conditional probability can be extracted from joint probability that
	\begin{align*}
	\P{x|y=y^{*}}=\frac{\P{x,y=y^{*}}}{\int\P{x,y=y^{*}}dx}=\frac{\P{x,y=y^{*}}}{\P{y=y^{*}}}
	\end{align*}

Usually, the formula is written as $\P{x|y}=\frac{\P{x,y}}{\P{y}}$.
\item[Product Rule] The formula can be rearranged as $\P{x,y}=\P{x|y}\P{y}=\P{y|x}\P{x}$. In case of multiple variables
	\begin{align*}
	\P{w,x,y,z}&=\P{w,x,y|z}\P{z} \\
			   &=\P{w,x|y,z}\P{y|z}\P{z} \\
			   &=\P{w|x,y,z}\P{x|y,z}\P{y|z}\P{z}
	\end{align*}
\item[Independence] If two variables $x$ and $y$ are independent, then r.v. $x$ tells nothing about r.v. $y$ (and vice-versa)
	\begin{align*}
	\P{x|y}&=\P{x} \\
	\P{y|x}&=\P{y} \\
	\P{x,y}&=\P{x}\P{y}
	\end{align*}
\item[Baye's Rule] By rearranging formula in Product Rule, we have
	\begin{align*}
	\P{y|x}&=\frac{\P{x|y}\P{y}}{\P{x}} \\
		   &=\frac{\P{x|y}\P{y}}{\int\P{x,y}dy} \\
		   &=\frac{\P{x|y}\P{y}}{\int\P{x|y}\P{y}dy}
	\end{align*}
\item[Expectation] Expectation tells us the excepted or average value of some function $f(x)$, taking into account the distribution of $x$.
	\begin{align*}
	\E{f(x)}&=\sum_x f(x)\P{x} \\
	\E{f(x)}&=\int f(x)\P{x}dx
	\end{align*}
Definition in two dimensions: $\E{f(x,y)}=\iint f(x,y)\P{x,y}dx\ dy$
	\begin{table*}[!h]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			Function $f(\bullet)$ & Expectation \\
			\hline
			$x^k$ & $k^{th}$ moment about zero \\
			\hline
			$(x-\mu_x)^k$ & $k^{th}$ moment about the mean \\
			\hline
		\end{tabular}
		\
		\begin{tabular}{|c|c|}
			\hline
			Function $f(\bullet)$ & Expectation \\
			\hline
			$x$ & mean, $\mu_x$ \\
			\hline
			$(x-\mu_x)^2$ & variance \\
			\hline
			$(x-\mu_x)^3$ & skew \\
			\hline
			$(x-\mu_x)^4$ & kurtosis \\
			\hline
			$(x-\mu_x)(x-\mu_y)$ & covariance of $x$ and $y$ \\
			\hline
		\end{tabular}
	\end{table*}

Besides, Expectation has the following four rules
	\begin{enumerate}
		\item Expected value of a constant is the constant $\E{\kappa}=\kappa$.
		\item Expected value of constant times function is constant times excepted value of function $\E{kf(x)}=k\E{f(x)}$.
		\item Expectation of sum of functions is sum of expectation of functions \\ $\E{f(x)+g(y)}=\E{f(x)}+\E{g(x)}$.
		\item Expectation of product of functions in variables $x$ and $y$ is product of expectations of functions if $x$ and $y$ are independent $\E{f(x)g(y)}=\E{f(x)}\E{g(y)}, x \indep y$.
	\end{enumerate}
\end{description}

\subsection{Common Probability Distributions}

\begin{description}[leftmargin=0cm]
\item[Bernoulli] Bernoulli distribution describes situation where only two possible outcomes $y=0 / y=1$ or failure/success.
	\begin{enumerate}[nosep]
		\item $\P{x}=\Bern_x[\la]=\la^x(1-\la)^{1-x}$
		\item univariate, discrete, binary
		\item $x \in \{0,1\}$; $\la \in [0,1]$
		\item $\E{x}=\la$, $\V{x}=\la(1-\la)$
	\end{enumerate}
\item[Beta] Beta distribution is the conjugate distribution to Bernoulli distribution.
	\begin{enumerate}
		\item $\P{\la}=\Beta_\la[\al,\be]=\frac{\Ga(\al+\be)}{\Ga(\al)\Ga(\be)}\la^{\al-1}(1-\la)^{\be-1}$
		\item univariate, continuous, unbounded
		\item $\la \in \bR$; $\al \in \bR_+$,  $\be \in \bR_+$
		\item $\E{\la}=\frac{\al}{\al+\be}$, $\V{\la}=\frac{\al\be}{(\al+\be)^2(\al+\be+1)}$
	\end{enumerate}
\item[Categorical] Categorical distribution describes situation with K possible outcomes.
	\begin{enumerate}
		\item $\P{x}=\Cat_x[\vec{\la}]$, $\P{x=k}=\la_k$,  $\P{\vec{x}=\vec{e}_k}=\prod_{j=1}^K\la_j^{x_j}=\la_k$
		\item univariate, discrete, multi-valued
		\item $x \in \{1,2,\dotsc,K\}$; $\la_k \in [0,1]$ where $\sum_k\la_k=1$
		\item $\E{x_i}=\la_i$, $\V{x_i}=\la_i(1-\la_i)$, $\C{x_i,x_j}=-\la_i\la_j\ (i \neq j)$
	\end{enumerate}
\item[Dirichlet] Dirichlet distribution is the conjugate distribution to categorical distribution.
	\begin{enumerate}
		\item $\P{\vec{\la}}=\Dir_{\vec{\la}}[\vec{\al}]=\frac{\Ga(\sum_{k=1}^K\al_k)}{\prod_{k=1}^K\Ga(\al_k)}\prod_{k=1}^K\la_k^{\al_k-1}$
		\item multivariate, continuous, bounded, sums to one
		\item $\vec{\la}=[\la_1,\la_2,\dotsc,\la_K]\T$, $\la_k \in [0,1]$, $\sum_{k=1}^K\la_k=1$; $\vec{\al}=[\al_1,\al_2,\dotsc,\al_K]$, $\al_k \in \bR_+$
		\item $\E{\la_i}=\frac{\al_i}{\sum_k\al_k}$, $\V{\la_i}=\frac{\al_i(\sum_k\al_k-\al_i)}{(\sum_k\al_k)^2(\sum_k\al_k+1)}$, $\C{\la_i,\la_j}=\frac{-\al_i\al_j}{(\sum_k\al_k)^2(\sum_k\al_k+1)}\ (i \neq j)$
	\end{enumerate}
\item[Univariate Normal] Univariate normal distribution describes single continuous variable.
	\begin{enumerate}
		\item $\P{x}=\Norm_x[\mu,\si^2]=\frac{1}{\sqrt{2\pi\si^2}}\e{-\frac{(x-\mu)^2}{2\si^2}}$
		\item univariate, continuous, unbounded
		\item $x \in \bR$; $\mu \in \bR$, $\si^2 \in \bR_+$
		\item $\E{x}=\mu$, $\V{x}=\si^2$
	\end{enumerate}
\item[Normal Inverse Gamma] Normal inverse gamma distribution is a conjugate distribution to univariate normal distribution.
	\begin{enumerate}
		\item $\P{\mu,\si^2}=\NIG_{\mu,\si^2}[\al,\be,\ga,\de]=\frac{\sqrt{\ga}}{\sqrt{2\pi\si^2}}\frac{\be^\al}{\Ga(\al)}\left(\frac{1}{\si^2}\right)^{\al+1}\e{-\frac{2\be+\ga(\de-\mu)^2}{2\si^2}}$
		\item bivariate, continuous, $\mu$ unbounded, $\si^2$ bounded below
		\item $\mu \in \bR$, $\si^2 \in \bR_+$; $\al \in \bR_+$, $\be \in \bR_+$, $\ga \in \bR_+$, $\de \in \bR$
		\item $\E{\mu}=\de$, $\E{\si^2}=\frac{\be}{\al-1}\ (\al>1)$, $\V{\mu}=\frac{\be}{(\al-1)\ga}\ (\al>1)$, $\V{\si^2}=\frac{\be^2}{(\al-1)^2(\al-2)}\ (\al>2)$, $\C{\mu,\si^2}=0\ (\al>1)$
	\end{enumerate}
\item[Multivariate Normal] Multivariate normal distribution describes multiple continuous variables. It takes two parameters: a vector containing mean position $\vec{\mu}$, and a symmetric positive definite covariance matrix $\cov$.
	\begin{enumerate}
		\item $\P{\vec{x}}=\Norm_{\vec{x}}[\vec{\mu},\cov]=\frac{1}{\sqrt{det(2\pi\cov)}}\e{-\frac{1}{2}(\vec{x}-\vec{\mu})\T\cov\inv(\vec{x}-\vec{\mu})}$
		\item multivariate, continuous, unbounded
		\item $\vec{x} \in \bR^K$; $\vec{\mu} \in \bR^K$, $\cov \in \bR^{K \times K}$ (positive semi-definite matrix)
		\item $\E{\vec{x}}=\vec{\mu}$, $\V{\vec{x}}=\cov$
	\end{enumerate}
\item[Normal Inverse Wishart] Normal inverse wishart distribution is a conjugate distribution to multivariate normal distribution.
	\begin{enumerate}
		\item $\P{\vec{\mu},\cov}=\NIW_{\vec{\mu},\cov}[\al,\vec{\Ps},\ga,\vec{\de}] \\ =\frac{\ga^{D/2}\det{\vec{\Ps}}^{\al/2}\det{\cov}^{-\frac{\al+D+2}{2}}}{(2\pi)^{D/2}2^{(\al\cov)/2}\Ga_D(\al/2)}\e{-\frac{1}{2}(\Tr(\vec{\Ps}\cov\inv)+\ga(\vec{\mu}-\vec{\de})\T\cov\inv(\vec{\mu}-\vec{\de})}$
		\item multivariate, $\vec{\mu}$ unbounded, $\cov$ square, positive definite
		\item $\vec{\mu} \in \bR^K$, $\cov \in \bR^{K \times K}$; $\al \in \bR_{>D-1}$, $\vec{\Ps} \in \bR^{K \times K}$, $\ga \in \bR_+$, $\vec{\de} \in \bR^K$
	\end{enumerate}
\end{description}

\section{Linear Algebra}
\label{section2.2}

\section{Calculus}
\label{section2.3}

\section{Informatics}
\label{section2.4}

\section{Optimization}
\label{section2.5}