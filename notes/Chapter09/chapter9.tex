%!TEX root = ../notebook.tex
% Chapter 9

\chapter{Decision Tree \& Ensemble Methods}
\label{chapter9}



\section{Decision Tree}
\label{section9.1}

\subsection{Growing a Tree}

\paragraph{Tree Growing Algorithm}

We can define a split function to choose the best feature and the best value for that feature as follows
\begin{align*}
(j^*,t^*)=\argmin_{j\in\{1,\dotsc,D\}}\min_{t\in\cT_j}cost\left(\{\vec{x}_i,y_j:x_{ij}\leq t\}\right)+cost\left(\{\vec{x}_i,y_j:x_{ij}>t\}\right)
\end{align*}
where we compare a feature $x_{ij}$ to a threshold $t$. The set of possible thresholds $\cT_j$ for feature $j$ can be obtained by sorting the unique values of $x_{ij}$. The algorithm to grow tree is
\begin{algorithm}[H]
	\caption*{\bf Recursive Procedure to Grow a Tree}
	\begin{algorithmic}
		\Function{fitTree}{node,$\cD$,depth}
		\State $\text{node.prediction}=mean(y_i:i\in\cD)$
		\State $(j^*,t^*,\cD_L,\cD_R)=split(\cD)$
		\If{not $worthSplitting(depth,cost,\cD_L,\cD_R)$}
		\State \Return node
		\Else
		\State $\text{node.test}=\la\vec{x}.x_{j^*}<t^*$
		\State $\text{node.left}=fitTree(\text{node},\cD_L,\text{depth}+1)$
		\State $\text{node.right}=fitTree(\text{node},\cD_R,\text{depth}+1)$
		\State \Return node
		\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}
The function that checks if a node is worth splitting can use several stopping heuristics, such as the following:
\begin{enumerate}
	\item Is the reduction in cost too small? Typically we define the gain of using a feature ti be a normalized measure of the reduction in cost
	\begin{align*}
	\De=cost(\cD)-\left(\frac{\ab{\cD_L}}{\ab{\cD}}cost(\cD_L)+\frac{\ab{\cD_R}}{\ab{\cD}}cost(\cD_R)\right)
	\end{align*}
	\item Has the tree exceeded the maximum desired depth?
	\item Is the distribution of the response in either $\cD_L$ or $\cD_R$ sufficiently homogeneous (e.g. all labels are the same, so the distribution is pure)?
	\item Is the number of examples in either $\cD_L$ or $\cD_R$ too small?
\end{enumerate}

\paragraph{Regression Cost}

In the regression setting, we define the cost as follows
\begin{align*}
cost(\cD)=\sum_{i\in\cD}(y_i-\bar{y})^2
\end{align*}
where $\bar{y}=\frac{1}{\ab{\cD}}\sum_{i\in\cD}y_i$ is the mean of the response variable in the specified set of data. Alternatively, we can fit a linear regression model for each leaf, using as inputs the features that were chosen on the path fro the root, and then measure the residual error.

\paragraph{Classification Cost}

In the classification setting, there are several ways to measure the quality of a split. First, we fit a multinoulli model to the data in the leaf satisfying the test $X_j<t$ by estimating the class-conditional probabilities as follows
\begin{align*}
\hat{\pi}_c=\frac{1}{\ab{\cD}}\sum_{i\in\cD}\bI(y_i=c)
\end{align*}
where $\cD$ is the data in the leaf. Given this, there are several common error measures for evaluating a proposed partition

\subparagraph{Misclassification Rate}

We define the most probable class label as $\hat{y}=\argmax_c\hat{\pi}_c$. The corresponding error is then
\begin{align*}
\frac{1}{\ab{\cD}}\sum_{i\in\cD}\bI(y_i\neq\hat{y})=1-\hat{\pi}_{\hat{y}}
\end{align*}

\subparagraph{Entropy}

\begin{align*}
\H{\hat{\vec{\pi}}}=-\sum_{c=1}^C\hat{\pi}_c\log\hat{\pi}_c
\end{align*}
Note that minimizing the entropy is equivalent to maximizing the information gain (used in ID3) between test $X_j<t$ and the class label $Y$, defined by
\begin{align*}
infoGain(X_j<t,Y)&=\H{Y}-\H{Y|X_j<t} \\
&=\left(-\sum_c\p{y=c}\log\p{y=c}\right) \\
&\quad+\left(\sum_c\p{y=c|X_j<t}\log\p{c|X_j<t}\right)
\end{align*}
And the information gain (ratio used in C4.5).
\begin{align*}
infoGainRatio(X_j<t,Y)=\frac{infoGain(X_j,Y)}{\H{Y}}
\end{align*}

\subparagraph{Gini Index}

\begin{align*}
\sum_{c=1}^C\hat{\pi}_c(1-\hat{\pi}_c)=\sum_c\hat{\pi}_c-\sum_c\hat{\pi}_c^2=1-\sum_c\hat{\pi}_c^2
\end{align*}
This is the expected error rate. To see this, note that $\hat{\pi}_c$ is the probability a random entry in the leaf belongs to class $c$, and $1-\hat{\pi}_c$ is the probability it would be misclassified.

\subsection{Pruning a Tree}

To prevent overfitting, we can stop growing the tree if the decrease in the error is not sufficient to justify the extra complexity of adding an extra subtree. However, this tends to be too myopic. The standard approach is therefore to grow a ``full'' tree, and then to perform pruning. This can be dine using a scheme that prunes the branches giving the least increase in the error. To determine how far to prune back, we can evaluate the cross-validated error on each such subtree, and then pick the tree whose CV error is within 1 standard error of the minimum.

\subsection{Limitation of Decision Tree}

Decision trees such as CART have some disadvantages. The primary one is that they do not predict very accurately compared to other kinds of model. This is in part due to the greedy nature of the tree construction algorithm. A related problem is that trees are unstable: small changes to the input data can have large effects on the structure of the tree, due to the hierarchical nature of the tree-growing process, causing errors at the top to affect the rest of the tree. In frequentist terminology, we say that trees are high variance estimators.



\section{Bagging}
\label{section9.2}

\subsection{Bagging Basics}

Suppose there are $N$ predictors that make errors independently
\begin{align*}
\hat{y}_i^n=y_i+\ep_i^n
\end{align*}
where $\ep_i^n$ is the error following a Gaussian distribution with zero mean and one variance. Bagging is then the technique that we train many models and try to decorrelate them
\begin{align*}
\hat{y}_i=\frac{1}{N}\sum_{n=1}^N\hat{y}_i^n
\end{align*}
The expectation and variance of the new model are now
\begin{align*}
\E{\hat{y}_i}&=\frac{1}{N}\left(Ny_i+\E{\sum_{n=1}^N\ep_i^n}\right)=y_i \\
\V{\hat{y}_i}&=\E{\hat{y}_i^2}-\Es{\hat{y}_i} \\
&=\E{\frac{1}{N^2}\left(\sum_{n=1}^N\left(y_i+\ep_i^n\right)\right)^2}-y_i^2 \\
&=\E{\frac{1}{N^2}\left(Ny_i+\sum_{n=1}^N\ep_i^n\right)^2}-y_i^2 \\
&=\E{y_i^2}+\E{\frac{1}{N^2}\left(\sum_{n=1}^N\ep_i^n\right)^2}+\E{\frac{2}{N}y_i\sum_{n=1}^N\ep_i^n}-y_i^2 \\
&=\frac{1}{N^2}\E{\left(\sum_{n=1}^N\left(\hat{y}_i-y_i\right)\right)^2} \\
&=\frac{1}{N}\V{\hat{y}_i^n}
\end{align*}
This gives us the result that we can approach the mean of true distribution and reduce variance by $\pfrac{1}{N}$.

\subsection{Random Forest}

Random forest is an instance of bagging methods that we train a number of trees, where each tree is trained on a sample of the same size as the original dataset sampled with replacement. At each splitting point we restrict a search to a subspace of features. This gives a wonderful parallelizable procedure.

Another nice feature of random forests is that they provide an estimation of generalization error for free, without validation set. This is called Out-Of-Bag (OOB) error. For each example we compute the prediction using only the trees that were built without this example.



\section{Boosting}
\label{section9.3}

\subsection{Boosting Basics}

Boosting is a greedy algorithm for fitting adaptive basis function models of the form
\begin{align*}
f(\vec{x})=\al_0+\sum_{t=1}^T\al_th_t(\vec{x})
\end{align*}
where the $h(\bullet)$ are generated by an algorithm called a weak learner or a base learner. The algorithm works by applying the weak learner sequentially to weighted versions of the data, where more weight is given to examples that were misclassified by earlier rounds.

\subsection{Adaboost}

\begin{algorithm}[H]
	\caption*{\bf The Adaboost Algorithm}
	\begin{algorithmic}
		\State input dataset $(\vec{x}_i,y_i),\vec{x}_i\in\cX,y_i\in\{-1,1\},i=1,\dotsc,N$
		\State initialize distribution on samples $D_1=\frac{1}{N}$
		\For{$t=1,\dotsc,T$}
		\State find classifier $h_t:\cX\to\{-1,1\}$ to the training set using weight $D_t$
		\State compute the classification error $\ep_t=\pfrac{\sum_{i=1}^ND_{t,i}\bI\left[y_i\neq h_t(\vec{x}_i)\right]}{\sum_{i=1}^ND_{t,i}}$
		\State compute $\al_t=\ppfrac{1}{2}\log\left(\pfrac{(1-\ep_t)}{\ep_t}\right)$
		\State update distribution $D_{t+1,i}=\ppfrac{D_{t,i}}{Z_t}\e{-\al_ty_ih_t(\vec{x}_i)}$ where $Z_t$ is the normalization term that $Z_t=\sum_iD_{t,i}\e{-\al_ty_ih_t(\vec{x}_i)}$
		\EndFor
		\State the final adaptive function $f(\vec{x})=\sum_t\al_th_t(\vec{x})$
		\State the final classifier is $H(\vec{x})=sign\left(f(\vec{x})\right)=sign\left(\sum_t\al_th_t(\vec{x})\right)$
	\end{algorithmic}
\end{algorithm}
For a binary classification problem, we can show that Adaboost has a final error bound that
\begin{align*}
\sum_{i=1}^N\frac{1}{N}\bI\left[y_i\neq H(\vec{x}_i)\right]\leq\prod_{t=1}^TZ_t\leq\e{-2\sum_{t=1}^T\ga_t^2}
\end{align*}
where $\ga_t=\half-\ep_t$ is the edge. We can prove this result that
\begin{align*}
\sum_{i=1}^N\frac{1}{N}\bI\left[y_i\neq H(\vec{x}_i)\right]&=\frac{1}{N}\sum_{i=1}^N\casefill{1\ &y_if(\vec{x}_i)\leq 0\\0\ &y_if(\vec{x}_i)>0} \\
&\leq\sum_{i=1}^N\frac{1}{N}\e{-y_if(\vec{x}_i)} \\
&=\frac{1}{N}\sum_{i=1}^N\e{-\sum_{t=1}^T\al_ty_ih_t(\vec{x}_i)} \\
&=\sum_{i=1}^ND_{1,i}\prod_{t=1}^T\e{-\al_ty_ih_t(\vec{x}_i)} \\
&=Z_1\sum_{i=1}^ND_{2,i}\prod_{t=2}^T\e{-\al_ty_ih_t(\vec{x}_i)} \\
&=Z_1Z_2\sum_{i=1}^ND_{3,i}\prod_{t=3}^T\e{-\al_ty_ih_t(\vec{x}_i)} \\
&=\cdots \\
&=Z_1Z_2\cdots Z_{T-1}\sum_{i=1}^ND_{T,i}\e{-\al_Ty_ih_T(\vec{x}_i)} \\
&=\prod_{t=1}^TZ_t \\
&=\prod_{t=1}^T\sum_{i=1}^ND_{t,i}\e{-\al_ty_ih_t(\vec{x}_i)} \\
&=\prod_{t=1}^T\left(\sum_{y_i=h_t(\vec{x}_i)}D_{t,i}e^{-\al_t}+\sum_{y_i\neq h_t(\vec{x}_i)}D_{t,i}e^{\al_t}\right) \\
&=\prod_{t=1}^T\left((1-\ep_t)e^{-\al_t}+\ep_te^{\al_t}\right) \\
&=\prod_{t=1}^T2\sqrt{\ep_t(1-\ep_t)}=\prod_{t=1}^T\sqrt{1-4\ga_t^2}
\end{align*}
We can expand a Taylor series on $e^x$ and $\sqrt{1-x}$ around $x=0$ to have the inequality $\sqrt{1-4\ga_t^2}\leq\e{-2\ga_t^2}$, then we can have
\begin{align*}
\prod_{t=1}^T\sqrt{1-4\ga_t^2}\leq\e{-2\sum_{t=1}^T\ga_t^2}
\end{align*}
Hence, we can conclude that if $\ga_t\geq\ga>0,\forall t$, we have
\begin{align*}
\sum_{i=1}^N\frac{1}{N}\bI\left[y_i\neq H(\vec{x}_i)\right]\leq\e{-2T\ga^2}
\end{align*}

\subsection{Gradient Boosting}

Instead of building lots of independent predictors, we build an ensemble in stagewise fashion. Each new element of the ensemble tried to correct the errors of the previous ones.

For regression and MSE the model like this. We start with predicting mean of the data $f_0(\vec{x})=\bar{y}$ (this is the best constant prediction for MSE). Then for each iteration:
\begin{enumerate}
	\item Compute the residuals $r_{i,t}=y_i-f_t(\vec{x}_i)$
	\item fit a new model $h_t(\vec{x})$ to the residuals
	\item alter the original model $f_{t+1}(\vec{x})=f_t(\vec{x})+\al h_t(\vec{x})$
\end{enumerate}
If we have a look at the loss function $\cL(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y}_i)^2$, the gradient w.r.t. the predictions $\hat{y}_i$ is $\frac{\pa}{\pa\hat{y}_i}\cL(y,\hat{y})=-\frac{2}{N}(y_i-\hat{y}_i)\prop-r_i$. So we can change the predictions in such a way to move the function towards loss-function minimum, doing gradient descent.
\begin{algorithm}[H]
	\caption*{\bf The Gradient Boosting Algorithm}
	\begin{algorithmic}
		\State initialize $f_0(\vec{x})=\argmin_{\vec{\th}}\sum_{i=1}^N\cL(y_i,h_0(\vec{x}_i;\vec{\th}))$
		\For{$t=1,\dotsc,T$}
		\State compute the gradient residual $r_{i,t}=-\left[\frac{\pa\cL(y_i,f(\vec{x}_i))}{\pa f(\vec{x}_i)}\right]_{f(\vec{x}_i)=f_{t-1}(\vec{x}_i)}$
		\State use weak learner to compute $\vec{\th}_t=\argmin_{\vec{\th}}\sum_{i=1}^N\left(r_{i,t}-h_t(\vec{x}_i;\vec{\th}_t)\right)^2$
		\State update $f_t(\vec{x})=f_{t-1}(\vec{x})+\al h_t(\vec{x};\vec{\th}_t)$
		\EndFor
		\State \Return $f(\vec{x})=f_T(\vec{x})$
	\end{algorithmic}
\end{algorithm}
