\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}% 
\contentsline {section}{\numberline {1.1}About this Notebook}{1}{section.1.1}% 
\contentsline {section}{\numberline {1.2}Policy of Use}{1}{section.1.2}% 
\contentsline {chapter}{\numberline {2}Mathematics Basics}{2}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Probability}{2}{section.2.1}% 
\contentsline {subsection}{\numberline {2.1.1}Basic Rules}{2}{subsection.2.1.1}% 
\contentsline {paragraph}{Three Axioms of Probability}{2}{section*.2}% 
\contentsline {paragraph}{Joint Probability}{2}{section*.3}% 
\contentsline {paragraph}{Marginalization}{2}{section*.4}% 
\contentsline {paragraph}{Conditional Probibility}{2}{section*.5}% 
\contentsline {paragraph}{Product Rule}{3}{section*.6}% 
\contentsline {paragraph}{Independence}{3}{section*.7}% 
\contentsline {paragraph}{Baye's Rule}{3}{section*.8}% 
\contentsline {paragraph}{Expectation}{3}{section*.9}% 
\contentsline {subsection}{\numberline {2.1.2}Common Probability Distributions}{4}{subsection.2.1.2}% 
\contentsline {paragraph}{Bernoulli}{4}{section*.11}% 
\contentsline {paragraph}{Binomial}{4}{section*.12}% 
\contentsline {paragraph}{Geometric}{4}{section*.13}% 
\contentsline {paragraph}{Negative Binomial}{5}{section*.14}% 
\contentsline {paragraph}{Beta}{5}{section*.15}% 
\contentsline {paragraph}{Poisson}{5}{section*.16}% 
\contentsline {paragraph}{Exponential}{5}{section*.17}% 
\contentsline {paragraph}{Gamma}{5}{section*.18}% 
\contentsline {paragraph}{Categorical}{6}{section*.20}% 
\contentsline {paragraph}{Dirichlet}{6}{section*.21}% 
\contentsline {paragraph}{Univariate Normal}{6}{section*.22}% 
\contentsline {paragraph}{Normal Inverse Gamma}{6}{section*.23}% 
\contentsline {paragraph}{Multivariate Normal}{7}{section*.24}% 
\contentsline {paragraph}{Normal Inverse Wishart}{7}{section*.25}% 
\contentsline {section}{\numberline {2.2}Linear Algebra}{7}{section.2.2}% 
\contentsline {subsection}{\numberline {2.2.1}Vectors}{7}{subsection.2.2.1}% 
\contentsline {paragraph}{Vectors Addition}{7}{section*.26}% 
\contentsline {paragraph}{Vectors Scaling}{7}{section*.27}% 
\contentsline {paragraph}{Rules for Vectors Addition and Scaling}{7}{section*.28}% 
\contentsline {paragraph}{Linear Combination \& Span}{8}{section*.29}% 
\contentsline {paragraph}{Representation of Basis}{8}{section*.30}% 
\contentsline {paragraph}{Linear Dependence}{8}{section*.31}% 
\contentsline {paragraph}{Dot Products}{9}{section*.32}% 
\contentsline {subsection}{\numberline {2.2.2}Matrices (Square Matrices)}{9}{subsection.2.2.2}% 
\contentsline {paragraph}{Linear Transformation}{9}{section*.33}% 
\contentsline {paragraph}{Basic Formulae}{9}{section*.34}% 
\contentsline {paragraph}{Trace}{9}{section*.35}% 
\contentsline {paragraph}{Determinants}{10}{section*.36}% 
\contentsline {paragraph}{Symmetric Matrix}{10}{section*.37}% 
\contentsline {paragraph}{Orthogonal Matrix}{10}{section*.38}% 
\contentsline {paragraph}{Eigendecomposition}{10}{section*.39}% 
\contentsline {paragraph}{Singular Value Decomposition (SVD)}{11}{section*.40}% 
\contentsline {paragraph}{Moore-Penrose Pseudoinverse}{12}{section*.41}% 
\contentsline {section}{\numberline {2.3}Calculus}{12}{section.2.3}% 
\contentsline {subsection}{\numberline {2.3.1}Differentiation}{12}{subsection.2.3.1}% 
\contentsline {paragraph}{Basic Formulae}{12}{section*.42}% 
\contentsline {paragraph}{Mean Value Theorem}{13}{section*.43}% 
\contentsline {paragraph}{Newton's Method}{13}{section*.44}% 
\contentsline {paragraph}{Taylor Series}{13}{section*.45}% 
\contentsline {subsection}{\numberline {2.3.2}Integration}{13}{subsection.2.3.2}% 
\contentsline {paragraph}{Properties}{13}{section*.46}% 
\contentsline {paragraph}{Average Function Value}{14}{section*.47}% 
\contentsline {paragraph}{Jensen's Inequality}{14}{section*.48}% 
\contentsline {subsection}{\numberline {2.3.3}Multivariate Calculus}{14}{subsection.2.3.3}% 
\contentsline {paragraph}{Partial Derivatives}{14}{section*.49}% 
\contentsline {paragraph}{Clairaut's Theorem}{14}{section*.50}% 
\contentsline {paragraph}{Chain Rule}{14}{section*.51}% 
\contentsline {paragraph}{Gradient}{14}{section*.52}% 
\contentsline {paragraph}{Directional Derivative}{15}{section*.53}% 
\contentsline {paragraph}{Lagrange Multipliers}{15}{section*.54}% 
\contentsline {subsection}{\numberline {2.3.4}Matrix Calculus}{15}{subsection.2.3.4}% 
\contentsline {paragraph}{Vector by Scalar}{15}{section*.55}% 
\contentsline {paragraph}{Scalar by Vector (Gradient)}{15}{section*.56}% 
\contentsline {paragraph}{Vector by Vector (Jacobian Matrix)}{15}{section*.57}% 
\contentsline {paragraph}{Matrix by Scalar}{16}{section*.58}% 
\contentsline {paragraph}{Scalar by Matrix (Gradient Matrix)}{16}{section*.59}% 
\contentsline {subsection}{\numberline {2.3.5}Backpropagation Modules}{16}{subsection.2.3.5}% 
\contentsline {paragraph}{Backpropagation}{16}{section*.60}% 
\contentsline {paragraph}{Linear Module}{16}{section*.61}% 
\contentsline {paragraph}{ReLU Module}{16}{section*.62}% 
\contentsline {paragraph}{Softmax Module}{17}{section*.63}% 
\contentsline {paragraph}{Cross-entropy Module}{17}{section*.64}% 
\contentsline {paragraph}{Cross-entropy with Logits Module}{17}{section*.65}% 
\contentsline {section}{\numberline {2.4}Information Theory}{17}{section.2.4}% 
\contentsline {subsection}{\numberline {2.4.1}Self-information}{17}{subsection.2.4.1}% 
\contentsline {subsection}{\numberline {2.4.2}Entropy}{17}{subsection.2.4.2}% 
\contentsline {subsection}{\numberline {2.4.3}Kullback-Leibler (KL) Divergence}{18}{subsection.2.4.3}% 
\contentsline {subsection}{\numberline {2.4.4}Cross-entropy}{18}{subsection.2.4.4}% 
\contentsline {section}{\numberline {2.5}Optimization}{18}{section.2.5}% 
\contentsline {subsection}{\numberline {2.5.1}One-dimensional Minimization}{18}{subsection.2.5.1}% 
\contentsline {paragraph}{Sufficient Conditions for a Minimum}{18}{section*.66}% 
\contentsline {paragraph}{Rate of Convergence}{19}{section*.67}% 
\contentsline {paragraph}{Brackets}{19}{section*.68}% 
\contentsline {subsection}{\numberline {2.5.2}Gradient Descent}{20}{subsection.2.5.2}% 
\contentsline {paragraph}{Exact Line Search Condition}{20}{section*.69}% 
\contentsline {paragraph}{Gradient Descent}{20}{section*.70}% 
\contentsline {subsection}{\numberline {2.5.3}Quadratic Functions}{20}{subsection.2.5.3}% 
\contentsline {paragraph}{Minimizing Quadratic Functions using Line Search}{20}{section*.71}% 
\contentsline {paragraph}{Conjugate Vectors}{21}{section*.72}% 
\contentsline {paragraph}{Gram-Schmidt Procedure}{21}{section*.73}% 
\contentsline {paragraph}{The Conjugate Vectors Algorithm}{22}{section*.74}% 
\contentsline {paragraph}{The Conjugate Gradients Algorithm}{22}{section*.75}% 
\contentsline {paragraph}{Newton Methods}{24}{section*.76}% 
\contentsline {paragraph}{Quasi-Newton Methods}{24}{section*.77}% 
\contentsline {subsection}{\numberline {2.5.4}General Functions}{25}{subsection.2.5.4}% 
\contentsline {subsection}{\numberline {2.5.5}Optimization with Constraints}{26}{subsection.2.5.5}% 
\contentsline {paragraph}{Constraint Types}{26}{section*.78}% 
\contentsline {paragraph}{Transformation Methods}{26}{section*.79}% 
\contentsline {paragraph}{Lagrange Multipliers (Single Constraint)}{27}{section*.80}% 
\contentsline {paragraph}{Lagrange Multipliers (Multiple Constraints)}{27}{section*.81}% 
\contentsline {paragraph}{Computational Approaches to Constrained Optimization}{27}{section*.82}% 
\contentsline {chapter}{\numberline {3}Machine Learning Basics}{28}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Regularization}{28}{section.3.1}% 
\contentsline {subsection}{\numberline {3.1.1}Under-fitting \& Over-fitting}{28}{subsection.3.1.1}% 
\contentsline {subsection}{\numberline {3.1.2}Bias \& Variance}{28}{subsection.3.1.2}% 
\contentsline {subsection}{\numberline {3.1.3}Vector Norm}{28}{subsection.3.1.3}% 
\contentsline {subsection}{\numberline {3.1.4}Penalize Complexity}{29}{subsection.3.1.4}% 
\contentsline {section}{\numberline {3.2}Cross-Validation}{29}{section.3.2}% 
\contentsline {section}{\numberline {3.3}Bayesian Learning}{29}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Bayes' Rule Terminology}{29}{subsection.3.3.1}% 
\contentsline {subsection}{\numberline {3.3.2}Maximum Likelihood}{30}{subsection.3.3.2}% 
\contentsline {subsection}{\numberline {3.3.3}Maximum a Posterior (MAP)}{30}{subsection.3.3.3}% 
\contentsline {subsection}{\numberline {3.3.4}Bayesian Approach}{30}{subsection.3.3.4}% 
\contentsline {subsection}{\numberline {3.3.5}Example: Univariate Normal Distribution}{31}{subsection.3.3.5}% 
\contentsline {subsection}{\numberline {3.3.6}Example: Categorical Distribution}{33}{subsection.3.3.6}% 
\contentsline {section}{\numberline {3.4}Machine Learning Models}{36}{section.3.4}% 
\contentsline {subsection}{\numberline {3.4.1}Learning and Inference}{36}{subsection.3.4.1}% 
\contentsline {subsection}{\numberline {3.4.2}Three Types of Model}{36}{subsection.3.4.2}% 
\contentsline {subsection}{\numberline {3.4.3}Example: Regression}{37}{subsection.3.4.3}% 
\contentsline {subsection}{\numberline {3.4.4}Example: Classification}{39}{subsection.3.4.4}% 
\contentsline {section}{\numberline {3.5}Overview of Common Algorithms}{40}{section.3.5}% 
\contentsline {chapter}{\numberline {4}Matrix Factorization}{41}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Principal Component Analysis (PCA)}{41}{section.4.1}% 
\contentsline {subsection}{\numberline {4.1.1}The PCA Algorithm}{41}{subsection.4.1.1}% 
\contentsline {subsection}{\numberline {4.1.2}PCA Interpretation}{41}{subsection.4.1.2}% 
\contentsline {section}{\numberline {4.2}Singular Value Decomposition (SVD)}{44}{section.4.2}% 
\contentsline {subsection}{\numberline {4.2.1}The SVD Algorithm}{44}{subsection.4.2.1}% 
\contentsline {subsection}{\numberline {4.2.2}Relation to PCA}{44}{subsection.4.2.2}% 
\contentsline {section}{\numberline {4.3}Non-negative Matrix Factorization ((N)NMF)}{44}{section.4.3}% 
\contentsline {subsection}{\numberline {4.3.1}Standard NMF}{44}{subsection.4.3.1}% 
\contentsline {subsection}{\numberline {4.3.2}Anchor Words Assumption}{45}{subsection.4.3.2}% 
\contentsline {chapter}{\numberline {5}K-Nearest Neighbors}{47}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Simple K-NN}{47}{section.5.1}% 
\contentsline {section}{\numberline {5.2}Fast K-NN Computation}{48}{section.5.2}% 
\contentsline {subsection}{\numberline {5.2.1}Metric Distances Methods}{48}{subsection.5.2.1}% 
\contentsline {paragraph}{Triangle Inequality}{48}{section*.100}% 
\contentsline {paragraph}{Orchard's Algorithm}{48}{section*.101}% 
\contentsline {paragraph}{Approximating and Eliminating Search Algorithm (AESA)}{49}{section*.102}% 
\contentsline {paragraph}{Pre-elimination using Buoys}{49}{section*.103}% 
\contentsline {paragraph}{AESA with Buoys}{49}{section*.104}% 
\contentsline {paragraph}{Orchard with Buoys}{50}{section*.105}% 
\contentsline {subsection}{\numberline {5.2.2}K-dimensional (KD) Tree}{50}{subsection.5.2.2}% 
\contentsline {chapter}{\numberline {6}Linear Regression}{52}{chapter.6}% 
\contentsline {section}{\numberline {6.1}Non-probabilistic Model}{52}{section.6.1}% 
\contentsline {section}{\numberline {6.2}Probabilistic Model}{52}{section.6.2}% 
\contentsline {section}{\numberline {6.3}Bayesian Regression}{53}{section.6.3}% 
\contentsline {section}{\numberline {6.4}Non-linear Regression}{54}{section.6.4}% 
\contentsline {paragraph}{Basic Idea}{54}{section*.106}% 
\contentsline {paragraph}{Polynomial Regression}{54}{section*.107}% 
\contentsline {paragraph}{Radial Basis Functions (RBF)}{54}{section*.108}% 
\contentsline {paragraph}{Arc Tan Functions}{54}{section*.109}% 
\contentsline {section}{\numberline {6.5}Kernel Trick \& Gaussian Processes}{55}{section.6.5}% 
\contentsline {section}{\numberline {6.6}Sparse Linear Regression}{56}{section.6.6}% 
\contentsline {section}{\numberline {6.7}Dual Linear Regression}{58}{section.6.7}% 
\contentsline {section}{\numberline {6.8}Relevance Vector Regression}{59}{section.6.8}% 
\contentsline {chapter}{\numberline {7}Logistic Regression}{60}{chapter.7}% 
\contentsline {section}{\numberline {7.1}Binary Classification}{60}{section.7.1}% 
\contentsline {section}{\numberline {7.2}Bayesian Logistic Regression}{61}{section.7.2}% 
\contentsline {section}{\numberline {7.3}Non-linear Logistic Regression}{62}{section.7.3}% 
\contentsline {section}{\numberline {7.4}Kernel Trick \& Gaussian Process Classification}{62}{section.7.4}% 
\contentsline {section}{\numberline {7.5}Relevance Vector Classification}{63}{section.7.5}% 
\contentsline {section}{\numberline {7.6}Incremental Fitting}{64}{section.7.6}% 
\contentsline {section}{\numberline {7.7}Multi-class Classification}{64}{section.7.7}% 
\contentsline {chapter}{\numberline {8}Support Vector Machines}{65}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Functional \& Geometric Margins}{65}{section.8.1}% 
\contentsline {section}{\numberline {8.2}The Large Margin Principle}{66}{section.8.2}% 
\contentsline {section}{\numberline {8.3}Slack Variables}{68}{section.8.3}% 
\contentsline {section}{\numberline {8.4}Hinge Loss}{71}{section.8.4}% 
\contentsline {section}{\numberline {8.5}Non-linear SVMs \& Kernel Trick}{71}{section.8.5}% 
\contentsline {section}{\numberline {8.6}Sequential Minimal Optimization (SMO)}{72}{section.8.6}% 
\contentsline {chapter}{\numberline {9}EM Algorithm \& Variational Inference}{73}{chapter.9}% 
\contentsline {section}{\numberline {9.1}Expectation Maximization}{73}{section.9.1}% 
\contentsline {subsection}{\numberline {9.1.1}The EM Algorithm}{73}{subsection.9.1.1}% 
\contentsline {subsection}{\numberline {9.1.2}EM Algorithm for Mixture Models}{73}{subsection.9.1.2}% 
\contentsline {paragraph}{Mixture of Gaussians}{73}{section*.112}% 
\contentsline {paragraph}{Example: t-distributions}{73}{section*.113}% 
\contentsline {paragraph}{Example: Factor Analysis}{73}{section*.114}% 
\contentsline {section}{\numberline {9.2}Variational Inference}{73}{section.9.2}% 
\contentsline {subsection}{\numberline {9.2.1}Mean Field Approximation}{73}{subsection.9.2.1}% 
\contentsline {subsection}{\numberline {9.2.2}Variational Autoencoders}{73}{subsection.9.2.2}% 
\contentsline {chapter}{\numberline {10}Decision Tree \& Ensemble Methods}{74}{chapter.10}% 
\contentsline {section}{\numberline {10.1}Decision Tree}{74}{section.10.1}% 
\contentsline {subsection}{\numberline {10.1.1}Growing a Tree}{74}{subsection.10.1.1}% 
\contentsline {paragraph}{Tree Growing Algorithm}{74}{section*.115}% 
\contentsline {paragraph}{Regression Cost}{75}{section*.116}% 
\contentsline {paragraph}{Classification Cost}{75}{section*.117}% 
\contentsline {subparagraph}{Misclassification Rate}{75}{section*.118}% 
\contentsline {subparagraph}{Entropy}{75}{section*.119}% 
\contentsline {subparagraph}{Gini Index}{76}{section*.120}% 
\contentsline {subsection}{\numberline {10.1.2}Pruning a Tree}{76}{subsection.10.1.2}% 
\contentsline {subsection}{\numberline {10.1.3}Limitation of Decision Tree}{76}{subsection.10.1.3}% 
\contentsline {section}{\numberline {10.2}Bagging}{77}{section.10.2}% 
\contentsline {subsection}{\numberline {10.2.1}Bagging Basics}{77}{subsection.10.2.1}% 
\contentsline {subsection}{\numberline {10.2.2}Random Forest}{78}{subsection.10.2.2}% 
\contentsline {section}{\numberline {10.3}Boosting}{78}{section.10.3}% 
\contentsline {subsection}{\numberline {10.3.1}Boosting Basics}{78}{subsection.10.3.1}% 
\contentsline {subsection}{\numberline {10.3.2}Adaboost}{78}{subsection.10.3.2}% 
\contentsline {subsection}{\numberline {10.3.3}Gradient Boosting}{80}{subsection.10.3.3}% 
\contentsline {chapter}{\numberline {11}Clustering}{81}{chapter.11}% 
\contentsline {section}{\numberline {11.1}K-Means}{81}{section.11.1}% 
\contentsline {subsection}{\numberline {11.1.1}The K-Means Algorithm}{81}{subsection.11.1.1}% 
\contentsline {subsection}{\numberline {11.1.2}K-Means Limitations}{82}{subsection.11.1.2}% 
\contentsline {paragraph}{Hard Assignment}{82}{section*.121}% 
\contentsline {paragraph}{Outlier Sensitivity}{82}{section*.122}% 
\contentsline {paragraph}{Shape/Density Issues}{82}{section*.123}% 
\contentsline {paragraph}{Computational Cost}{82}{section*.124}% 
\contentsline {paragraph}{Representation Sensitivity}{82}{section*.125}% 
\contentsline {paragraph}{Missing Data}{82}{section*.126}% 
\contentsline {section}{\numberline {11.2}Spectral Clustering}{82}{section.11.2}% 
\contentsline {subsection}{\numberline {11.2.1}Similarity Graphs}{82}{subsection.11.2.1}% 
\contentsline {paragraph}{$\epsilon $-Neighborhood Graph}{82}{section*.127}% 
\contentsline {paragraph}{$k$-Nearest Neighbor Graphs}{82}{section*.128}% 
\contentsline {paragraph}{Fully Connected Graph}{83}{section*.129}% 
\contentsline {subsection}{\numberline {11.2.2}Graph Laplacian}{83}{subsection.11.2.2}% 
\contentsline {paragraph}{Basic Graph Definitions}{83}{section*.130}% 
\contentsline {paragraph}{Unnormalized Graph Laplacian}{83}{section*.131}% 
\contentsline {paragraph}{Normalized Graph Laplacian}{84}{section*.132}% 
\contentsline {subsection}{\numberline {11.2.3}Spectral Clustering Algorithms}{84}{subsection.11.2.3}% 
\contentsline {subsection}{\numberline {11.2.4}NCut and RatioCut Approximations}{85}{subsection.11.2.4}% 
\contentsline {subsection}{\numberline {11.2.5}Random Walk Viewpoint}{87}{subsection.11.2.5}% 
\contentsline {chapter}{\numberline {12}Graphical Models}{88}{chapter.12}% 
\contentsline {section}{\numberline {12.1}Graph Definitions}{88}{section.12.1}% 
\contentsline {subsection}{\numberline {12.1.1}Graph}{88}{subsection.12.1.1}% 
\contentsline {paragraph}{Graph}{88}{section*.133}% 
\contentsline {paragraph}{Path}{88}{section*.134}% 
\contentsline {subsection}{\numberline {12.1.2}Directed Graph}{88}{subsection.12.1.2}% 
\contentsline {paragraph}{Directed Graphs}{88}{section*.135}% 
\contentsline {paragraph}{Directed Acyclic Graph (DAG)}{88}{section*.137}% 
\contentsline {paragraph}{Parents and Children}{88}{section*.138}% 
\contentsline {paragraph}{Ancestors and Descendants}{89}{section*.139}% 
\contentsline {subsection}{\numberline {12.1.3}Undirected Graph}{89}{subsection.12.1.3}% 
\contentsline {paragraph}{Undirected Graph}{89}{section*.140}% 
\contentsline {paragraph}{Clique}{89}{section*.142}% 
\contentsline {paragraph}{Maximal Clique}{89}{section*.143}% 
\contentsline {subsection}{\numberline {12.1.4}Connectivity}{89}{subsection.12.1.4}% 
\contentsline {paragraph}{Connected Graph}{89}{section*.144}% 
\contentsline {paragraph}{Connected Components}{89}{section*.145}% 
\contentsline {subsection}{\numberline {12.1.5}Connectedness}{89}{subsection.12.1.5}% 
\contentsline {paragraph}{Singly-connected}{89}{section*.147}% 
\contentsline {paragraph}{Multiply-connected}{90}{section*.148}% 
\contentsline {section}{\numberline {12.2}Belief Networks}{90}{section.12.2}% 
\contentsline {subsection}{\numberline {12.2.1}Definition}{90}{subsection.12.2.1}% 
\contentsline {subsection}{\numberline {12.2.2}Uncertain Evidence}{90}{subsection.12.2.2}% 
\contentsline {paragraph}{Definition}{90}{section*.150}% 
\contentsline {paragraph}{Hard Evidence}{90}{section*.151}% 
\contentsline {paragraph}{Inference}{90}{section*.152}% 
\contentsline {paragraph}{Jeffrey's Rule}{90}{section*.153}% 
\contentsline {subsection}{\numberline {12.2.3}Independence}{91}{subsection.12.2.3}% 
\contentsline {subsection}{\numberline {12.2.4}General Rule for Independence in Belief Networks}{92}{subsection.12.2.4}% 
\contentsline {paragraph}{Independence of $\mathcal {X}$ and $\mathcal {Y}$}{92}{section*.160}% 
\contentsline {paragraph}{d-connected}{92}{section*.161}% 
\contentsline {paragraph}{Separation and Independence}{92}{section*.162}% 
\contentsline {subsection}{\numberline {12.2.5}Markov Equivalence}{93}{subsection.12.2.5}% 
\contentsline {paragraph}{Skeleton}{93}{section*.163}% 
\contentsline {paragraph}{Immorality}{93}{section*.164}% 
\contentsline {paragraph}{Markov Equivalence}{93}{section*.165}% 
\contentsline {section}{\numberline {12.3}Markov Networks}{93}{section.12.3}% 
\contentsline {subsection}{\numberline {12.3.1}Definition}{93}{subsection.12.3.1}% 
\contentsline {subsection}{\numberline {12.3.2}Examples}{93}{subsection.12.3.2}% 
\contentsline {subsection}{\numberline {12.3.3}Independence}{94}{subsection.12.3.3}% 
\contentsline {subsection}{\numberline {12.3.4}Expressiveness of Markov and Belief Networks}{95}{subsection.12.3.4}% 
\contentsline {subsection}{\numberline {12.3.5}Factor Graphs}{96}{subsection.12.3.5}% 
\contentsline {section}{\numberline {12.4}Markov Models}{96}{section.12.4}% 
\contentsline {subsection}{\numberline {12.4.1}Basic Model}{96}{subsection.12.4.1}% 
\contentsline {subsection}{\numberline {12.4.2}Markov Chains}{97}{subsection.12.4.2}% 
\contentsline {subsection}{\numberline {12.4.3}Hidden Markov Models (HMM)}{97}{subsection.12.4.3}% 
\contentsline {section}{\numberline {12.5}Inference}{97}{section.12.5}% 
\contentsline {subsection}{\numberline {12.5.1}Sum-Product Algorithm}{97}{subsection.12.5.1}% 
\contentsline {subsection}{\numberline {12.5.2}Max-Product Algorithm}{99}{subsection.12.5.2}% 
\contentsline {subsection}{\numberline {12.5.3}The Junction Tree Algorithm}{99}{subsection.12.5.3}% 
\contentsline {paragraph}{Clique Graph}{99}{section*.185}% 
\contentsline {paragraph}{Absorption}{100}{section*.186}% 
\contentsline {paragraph}{Running Intersection Property}{101}{section*.187}% 
\contentsline {paragraph}{Constructing a Junction Tree for Singly-Connected Distributions}{101}{section*.188}% 
\contentsline {paragraph}{Constructing a Junction Tree for Multiply-Connected Distributions}{102}{section*.190}% 
\contentsline {paragraph}{The Junction Algorithm}{102}{section*.192}% 
\contentsline {subsection}{\numberline {12.5.4}Graph Cut}{103}{subsection.12.5.4}% 
\contentsline {section}{\numberline {12.6}Sampling}{103}{section.12.6}% 
\contentsline {subsection}{\numberline {12.6.1}Basic Idea}{103}{subsection.12.6.1}% 
\contentsline {subsection}{\numberline {12.6.2}Importance Sampling}{105}{subsection.12.6.2}% 
\contentsline {subsection}{\numberline {12.6.3}Rejection Sampling}{105}{subsection.12.6.3}% 
\contentsline {subsection}{\numberline {12.6.4}Markov Chain Monte Carlo (MCMC)}{105}{subsection.12.6.4}% 
\contentsline {chapter}{\numberline {A}Bayesian Statistics}{106}{appendix.A}% 
\contentsline {section}{\numberline {A.1}Bayesian Inference}{106}{section.A.1}% 
\contentsline {section}{\numberline {A.2}Prior Distributions}{106}{section.A.2}% 
\contentsline {chapter}{\numberline {B}Statistical Assessment}{107}{appendix.B}% 
\contentsline {section}{\numberline {B.1}Hypothesis Testing}{107}{section.B.1}% 
\contentsline {subsection}{\numberline {B.1.1}Testing Basics}{107}{subsection.B.1.1}% 
\contentsline {subsection}{\numberline {B.1.2}Testing Procedure}{107}{subsection.B.1.2}% 
\contentsline {subsection}{\numberline {B.1.3}Power Investigation}{108}{subsection.B.1.3}% 
\contentsline {subsection}{\numberline {B.1.4}Useful Tests}{108}{subsection.B.1.4}% 
\contentsline {section}{\numberline {B.2}Confidence Intervals}{108}{section.B.2}% 
\contentsline {section}{\numberline {B.3}Bootstrap}{108}{section.B.3}% 
