\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}% 
\contentsline {section}{\numberline {1.1}About this Notebook}{1}{section.1.1}% 
\contentsline {section}{\numberline {1.2}Policy of Use}{1}{section.1.2}% 
\contentsline {chapter}{\numberline {2}Mathematics Basics}{2}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Probability}{2}{section.2.1}% 
\contentsline {subsection}{\numberline {2.1.1}Basic Rules}{2}{subsection.2.1.1}% 
\contentsline {paragraph}{Three Axioms of Probability}{2}{section*.2}% 
\contentsline {paragraph}{Joint Probability}{2}{section*.3}% 
\contentsline {paragraph}{Marginalization}{2}{section*.4}% 
\contentsline {paragraph}{Conditional Probibility}{2}{section*.5}% 
\contentsline {paragraph}{Product Rule}{3}{section*.6}% 
\contentsline {paragraph}{Independence}{3}{section*.7}% 
\contentsline {paragraph}{Baye's Rule}{3}{section*.8}% 
\contentsline {paragraph}{Expectation}{3}{section*.9}% 
\contentsline {subsection}{\numberline {2.1.2}Common Probability Distributions}{4}{subsection.2.1.2}% 
\contentsline {paragraph}{Bernoulli}{4}{section*.11}% 
\contentsline {paragraph}{Binomial}{4}{section*.12}% 
\contentsline {paragraph}{Geometric}{4}{section*.13}% 
\contentsline {paragraph}{Negative Binomial}{5}{section*.14}% 
\contentsline {paragraph}{Beta}{5}{section*.15}% 
\contentsline {paragraph}{Poisson}{5}{section*.16}% 
\contentsline {paragraph}{Exponential}{5}{section*.17}% 
\contentsline {paragraph}{Gamma}{5}{section*.18}% 
\contentsline {paragraph}{Categorical}{6}{section*.20}% 
\contentsline {paragraph}{Dirichlet}{6}{section*.21}% 
\contentsline {paragraph}{Univariate Normal}{6}{section*.22}% 
\contentsline {paragraph}{Normal Inverse Gamma}{6}{section*.23}% 
\contentsline {paragraph}{Multivariate Normal}{7}{section*.24}% 
\contentsline {paragraph}{Normal Inverse Wishart}{7}{section*.25}% 
\contentsline {section}{\numberline {2.2}Linear Algebra}{7}{section.2.2}% 
\contentsline {subsection}{\numberline {2.2.1}Vectors}{7}{subsection.2.2.1}% 
\contentsline {paragraph}{Vectors Addition}{7}{section*.26}% 
\contentsline {paragraph}{Vectors Scaling}{7}{section*.27}% 
\contentsline {paragraph}{Rules for Vectors Addition and Scaling}{7}{section*.28}% 
\contentsline {paragraph}{Linear Combination \& Span}{8}{section*.29}% 
\contentsline {paragraph}{Representation of Basis}{8}{section*.30}% 
\contentsline {paragraph}{Linear Dependence}{8}{section*.31}% 
\contentsline {paragraph}{Dot Products}{9}{section*.32}% 
\contentsline {subsection}{\numberline {2.2.2}Matrices (Square Matrices)}{9}{subsection.2.2.2}% 
\contentsline {paragraph}{Linear Transformation}{9}{section*.33}% 
\contentsline {paragraph}{Basic Formulae}{9}{section*.34}% 
\contentsline {paragraph}{Trace}{9}{section*.35}% 
\contentsline {paragraph}{Determinants}{10}{section*.36}% 
\contentsline {paragraph}{Symmetric Matrix}{10}{section*.37}% 
\contentsline {paragraph}{Orthogonal Matrix}{10}{section*.38}% 
\contentsline {paragraph}{Eigendecomposition}{10}{section*.39}% 
\contentsline {paragraph}{Singular Value Decomposition (SVD)}{11}{section*.40}% 
\contentsline {paragraph}{Moore-Penrose Pseudoinverse}{12}{section*.41}% 
\contentsline {section}{\numberline {2.3}Calculus}{12}{section.2.3}% 
\contentsline {subsection}{\numberline {2.3.1}Differentiation}{12}{subsection.2.3.1}% 
\contentsline {paragraph}{Basic Formulae}{12}{section*.42}% 
\contentsline {paragraph}{Mean Value Theorem}{13}{section*.43}% 
\contentsline {paragraph}{Newton's Method}{13}{section*.44}% 
\contentsline {paragraph}{Taylor Series}{13}{section*.45}% 
\contentsline {subsection}{\numberline {2.3.2}Integration}{13}{subsection.2.3.2}% 
\contentsline {paragraph}{Properties}{13}{section*.46}% 
\contentsline {paragraph}{Average Function Value}{14}{section*.47}% 
\contentsline {paragraph}{Jensen's Inequality}{14}{section*.48}% 
\contentsline {subsection}{\numberline {2.3.3}Multivariate Calculus}{14}{subsection.2.3.3}% 
\contentsline {paragraph}{Partial Derivatives}{14}{section*.49}% 
\contentsline {paragraph}{Clairaut's Theorem}{14}{section*.50}% 
\contentsline {paragraph}{Chain Rule}{14}{section*.51}% 
\contentsline {paragraph}{Gradient}{14}{section*.52}% 
\contentsline {paragraph}{Directional Derivative}{15}{section*.53}% 
\contentsline {paragraph}{Lagrange Multipliers}{15}{section*.54}% 
\contentsline {subsection}{\numberline {2.3.4}Matrix Calculus}{15}{subsection.2.3.4}% 
\contentsline {paragraph}{Vector by Scalar}{15}{section*.55}% 
\contentsline {paragraph}{Scalar by Vector (Gradient)}{15}{section*.56}% 
\contentsline {paragraph}{Vector by Vector (Jacobian Matrix)}{15}{section*.57}% 
\contentsline {paragraph}{Matrix by Scalar}{16}{section*.58}% 
\contentsline {paragraph}{Scalar by Matrix (Gradient Matrix)}{16}{section*.59}% 
\contentsline {subsection}{\numberline {2.3.5}Backpropagation Modules}{16}{subsection.2.3.5}% 
\contentsline {paragraph}{Backpropagation}{16}{section*.60}% 
\contentsline {paragraph}{Linear Module}{16}{section*.61}% 
\contentsline {paragraph}{ReLU Module}{16}{section*.62}% 
\contentsline {paragraph}{Softmax Module}{17}{section*.63}% 
\contentsline {paragraph}{Cross-entropy Module}{17}{section*.64}% 
\contentsline {paragraph}{Cross-entropy with Logits Module}{17}{section*.65}% 
\contentsline {section}{\numberline {2.4}Information Theory}{17}{section.2.4}% 
\contentsline {subsection}{\numberline {2.4.1}Self-information}{17}{subsection.2.4.1}% 
\contentsline {subsection}{\numberline {2.4.2}Entropy}{17}{subsection.2.4.2}% 
\contentsline {subsection}{\numberline {2.4.3}Kullback-Leibler (KL) Divergence}{18}{subsection.2.4.3}% 
\contentsline {subsection}{\numberline {2.4.4}Cross-entropy}{18}{subsection.2.4.4}% 
\contentsline {section}{\numberline {2.5}Optimization}{18}{section.2.5}% 
\contentsline {subsection}{\numberline {2.5.1}One-dimensional Minimization}{18}{subsection.2.5.1}% 
\contentsline {paragraph}{Sufficient Conditions for a Minimum}{18}{section*.66}% 
\contentsline {paragraph}{Rate of Convergence}{19}{section*.67}% 
\contentsline {paragraph}{Brackets}{19}{section*.68}% 
\contentsline {subsection}{\numberline {2.5.2}Gradient Descent}{20}{subsection.2.5.2}% 
\contentsline {paragraph}{Exact Line Search Condition}{20}{section*.69}% 
\contentsline {paragraph}{Gradient Descent}{20}{section*.70}% 
\contentsline {subsection}{\numberline {2.5.3}Quadratic Functions}{20}{subsection.2.5.3}% 
\contentsline {paragraph}{Minimizing Quadratic Functions Using Line Search}{20}{section*.71}% 
\contentsline {paragraph}{Conjugate Vectors}{21}{section*.72}% 
\contentsline {paragraph}{Gram-Schmidt Procedure}{21}{section*.73}% 
\contentsline {paragraph}{The Conjugate Vectors Algorithm}{22}{section*.74}% 
\contentsline {paragraph}{The Conjugate Gradients Algorithm}{22}{section*.75}% 
\contentsline {paragraph}{Newton Methods}{24}{section*.76}% 
\contentsline {paragraph}{Quasi-Newton Methods}{24}{section*.77}% 
\contentsline {subsection}{\numberline {2.5.4}General Functions}{25}{subsection.2.5.4}% 
\contentsline {subsection}{\numberline {2.5.5}Optimization with Constraints}{26}{subsection.2.5.5}% 
\contentsline {paragraph}{Constraint Types}{26}{section*.78}% 
\contentsline {paragraph}{Transformation Methods}{26}{section*.79}% 
\contentsline {paragraph}{Lagrange Multipliers (Single Constraint)}{27}{section*.80}% 
\contentsline {paragraph}{Lagrange Multipliers (Multiple Constraints)}{27}{section*.81}% 
\contentsline {paragraph}{Computational Approaches to Constrained Optimization}{27}{section*.82}% 
\contentsline {chapter}{\numberline {3}Machine Learning Basics}{28}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Regularization}{28}{section.3.1}% 
\contentsline {subsection}{\numberline {3.1.1}Under-fitting \& Over-fitting}{28}{subsection.3.1.1}% 
\contentsline {subsection}{\numberline {3.1.2}Bias \& Variance}{28}{subsection.3.1.2}% 
\contentsline {subsection}{\numberline {3.1.3}Vector Norm}{28}{subsection.3.1.3}% 
\contentsline {subsection}{\numberline {3.1.4}Penalize Complexity}{29}{subsection.3.1.4}% 
\contentsline {section}{\numberline {3.2}Cross-Validation}{29}{section.3.2}% 
\contentsline {section}{\numberline {3.3}Bayesian Learning}{29}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Bayes' Rule Terminology}{29}{subsection.3.3.1}% 
\contentsline {subsection}{\numberline {3.3.2}Maximum Likelihood}{30}{subsection.3.3.2}% 
\contentsline {subsection}{\numberline {3.3.3}Maximum a Posterior (MAP)}{30}{subsection.3.3.3}% 
\contentsline {subsection}{\numberline {3.3.4}Bayesian Approach}{30}{subsection.3.3.4}% 
\contentsline {subsection}{\numberline {3.3.5}Example: Univariate Normal Distribution}{31}{subsection.3.3.5}% 
\contentsline {subsection}{\numberline {3.3.6}Example: Categorical Distribution}{33}{subsection.3.3.6}% 
\contentsline {section}{\numberline {3.4}Machine Learning Models}{36}{section.3.4}% 
\contentsline {subsection}{\numberline {3.4.1}Learning and Inference}{36}{subsection.3.4.1}% 
\contentsline {subsection}{\numberline {3.4.2}Three Types of Model}{36}{subsection.3.4.2}% 
\contentsline {subsection}{\numberline {3.4.3}Example: Regression}{37}{subsection.3.4.3}% 
\contentsline {subsection}{\numberline {3.4.4}Example: Classification}{39}{subsection.3.4.4}% 
\contentsline {section}{\numberline {3.5}Overview of Common Algorithms}{40}{section.3.5}% 
\contentsline {chapter}{\numberline {4}Matrix Factorization}{41}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Principal Component Analysis (PCA)}{41}{section.4.1}% 
\contentsline {subsection}{\numberline {4.1.1}The PCA Algorithm}{41}{subsection.4.1.1}% 
\contentsline {subsection}{\numberline {4.1.2}PCA Interpretation}{41}{subsection.4.1.2}% 
\contentsline {section}{\numberline {4.2}Singular Value Decomposition (SVD)}{44}{section.4.2}% 
\contentsline {subsection}{\numberline {4.2.1}The SVD Algorithm}{44}{subsection.4.2.1}% 
\contentsline {subsection}{\numberline {4.2.2}Relation to PCA}{44}{subsection.4.2.2}% 
\contentsline {section}{\numberline {4.3}Non-negative Matrix Factorization ((N)NMF)}{44}{section.4.3}% 
\contentsline {subsection}{\numberline {4.3.1}Standard NMF}{44}{subsection.4.3.1}% 
\contentsline {subsection}{\numberline {4.3.2}Anchor Words Assumption}{45}{subsection.4.3.2}% 
\contentsline {chapter}{\numberline {5}K-Nearest Neighbors}{47}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Simple K-NN}{47}{section.5.1}% 
\contentsline {section}{\numberline {5.2}Fast K-NN Computation}{47}{section.5.2}% 
\contentsline {chapter}{\numberline {6}Linear Regression}{48}{chapter.6}% 
\contentsline {section}{\numberline {6.1}Basic Model}{48}{section.6.1}% 
\contentsline {section}{\numberline {6.2}Bayesian Regression}{48}{section.6.2}% 
\contentsline {section}{\numberline {6.3}Non-linear Regression}{50}{section.6.3}% 
\contentsline {section}{\numberline {6.4}Kernel Trick \& Gaussian Processes}{50}{section.6.4}% 
\contentsline {section}{\numberline {6.5}Sparse Linear Regression}{50}{section.6.5}% 
\contentsline {section}{\numberline {6.6}Dual Linear Regression}{50}{section.6.6}% 
\contentsline {section}{\numberline {6.7}Relevance Vector Regression}{50}{section.6.7}% 
\contentsline {chapter}{\numberline {7}Logistic Regression}{51}{chapter.7}% 
\contentsline {section}{\numberline {7.1}Logistic Regression}{51}{section.7.1}% 
\contentsline {section}{\numberline {7.2}Non-linear Logistic Regression}{51}{section.7.2}% 
\contentsline {section}{\numberline {7.3}Kernel Trick \& Gaussian Process Classification}{51}{section.7.3}% 
\contentsline {section}{\numberline {7.4}Multi-class Classification}{51}{section.7.4}% 
\contentsline {chapter}{\numberline {8}Support Vector Machines}{52}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Geometric Margins}{52}{section.8.1}% 
\contentsline {section}{\numberline {8.2}Primal \& Dual Problems}{52}{section.8.2}% 
\contentsline {section}{\numberline {8.3}Support Vectors}{52}{section.8.3}% 
\contentsline {section}{\numberline {8.4}Slack Variables}{52}{section.8.4}% 
\contentsline {section}{\numberline {8.5}Hinge Loss}{52}{section.8.5}% 
\contentsline {section}{\numberline {8.6}Non-linear SVMs}{52}{section.8.6}% 
\contentsline {section}{\numberline {8.7}Kernel Trick}{52}{section.8.7}% 
\contentsline {chapter}{\numberline {9}EM Algorithm}{53}{chapter.9}% 
\contentsline {section}{\numberline {9.1}Expectation Maximization}{53}{section.9.1}% 
\contentsline {section}{\numberline {9.2}Example: Mixture of Gaussians}{53}{section.9.2}% 
\contentsline {section}{\numberline {9.3}Example: t-distributions}{53}{section.9.3}% 
\contentsline {section}{\numberline {9.4}Example: Factor Analysis}{53}{section.9.4}% 
\contentsline {chapter}{\numberline {10}Bagging \& Boosting}{54}{chapter.10}% 
\contentsline {section}{\numberline {10.1}Ensemble Methods}{54}{section.10.1}% 
\contentsline {section}{\numberline {10.2}Bagging}{54}{section.10.2}% 
\contentsline {section}{\numberline {10.3}CART}{54}{section.10.3}% 
\contentsline {section}{\numberline {10.4}ID3}{54}{section.10.4}% 
\contentsline {section}{\numberline {10.5}C4.5}{54}{section.10.5}% 
\contentsline {section}{\numberline {10.6}Random Forest}{54}{section.10.6}% 
\contentsline {section}{\numberline {10.7}Boosting}{54}{section.10.7}% 
\contentsline {section}{\numberline {10.8}Adaboost}{54}{section.10.8}% 
\contentsline {chapter}{\numberline {11}Clustering}{55}{chapter.11}% 
\contentsline {section}{\numberline {11.1}K-Means}{55}{section.11.1}% 
\contentsline {section}{\numberline {11.2}Spectral Clustering}{55}{section.11.2}% 
\contentsline {chapter}{\numberline {12}Graphical Models \& Markov Network}{56}{chapter.12}% 
\contentsline {section}{\numberline {12.1}Graph Definitions}{56}{section.12.1}% 
\contentsline {subsection}{\numberline {12.1.1}Graph}{56}{subsection.12.1.1}% 
\contentsline {subsection}{\numberline {12.1.2}Directed Graph}{56}{subsection.12.1.2}% 
\contentsline {subsection}{\numberline {12.1.3}Undirected Graph}{57}{subsection.12.1.3}% 
\contentsline {subsection}{\numberline {12.1.4}Connectivity}{57}{subsection.12.1.4}% 
\contentsline {subsection}{\numberline {12.1.5}Connectedness}{57}{subsection.12.1.5}% 
\contentsline {section}{\numberline {12.2}Belief Networks}{58}{section.12.2}% 
\contentsline {subsection}{\numberline {12.2.1}Definition}{58}{subsection.12.2.1}% 
\contentsline {subsection}{\numberline {12.2.2}Uncertain Evidence}{58}{subsection.12.2.2}% 
\contentsline {subsection}{\numberline {12.2.3}Independence}{59}{subsection.12.2.3}% 
\contentsline {subsection}{\numberline {12.2.4}General Rule for Independence in Belief Networks}{60}{subsection.12.2.4}% 
\contentsline {subsection}{\numberline {12.2.5}Markov Equivalence}{61}{subsection.12.2.5}% 
\contentsline {section}{\numberline {12.3}Markov Networks}{61}{section.12.3}% 
\contentsline {subsection}{\numberline {12.3.1}Definition}{61}{subsection.12.3.1}% 
\contentsline {subsection}{\numberline {12.3.2}Examples}{61}{subsection.12.3.2}% 
\contentsline {subsection}{\numberline {12.3.3}Independence}{62}{subsection.12.3.3}% 
\contentsline {subsection}{\numberline {12.3.4}Expressiveness of Markov and Belief Networks}{62}{subsection.12.3.4}% 
\contentsline {subsection}{\numberline {12.3.5}Factor Graphs}{62}{subsection.12.3.5}% 
\contentsline {section}{\numberline {12.4}Markov Chains}{62}{section.12.4}% 
\contentsline {section}{\numberline {12.5}Hidden Markov Models}{62}{section.12.5}% 
\contentsline {chapter}{\numberline {A}Bayesian Statistics}{63}{appendix.A}% 
\contentsline {section}{\numberline {A.1}Bayesian Inference}{63}{section.A.1}% 
\contentsline {section}{\numberline {A.2}Prior Distributions}{63}{section.A.2}% 
\contentsline {chapter}{\numberline {B}Statistical Assessment}{64}{appendix.B}% 
\contentsline {section}{\numberline {B.1}Hypothesis Testing}{64}{section.B.1}% 
\contentsline {subsection}{\numberline {B.1.1}Testing Basics}{64}{subsection.B.1.1}% 
\contentsline {subsection}{\numberline {B.1.2}Testing Procedure}{64}{subsection.B.1.2}% 
\contentsline {subsection}{\numberline {B.1.3}Power Investigation}{65}{subsection.B.1.3}% 
\contentsline {subsection}{\numberline {B.1.4}Useful Tests}{65}{subsection.B.1.4}% 
\contentsline {section}{\numberline {B.2}Confidence Intervals}{65}{section.B.2}% 
\contentsline {section}{\numberline {B.3}Bootstrap}{65}{section.B.3}% 
