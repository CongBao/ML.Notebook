\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}% 
\contentsline {section}{\numberline {1.1}About this Notebook}{1}{section.1.1}% 
\contentsline {section}{\numberline {1.2}Policy of Use}{1}{section.1.2}% 
\contentsline {chapter}{\numberline {2}Mathematics Basics}{2}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Probability}{2}{section.2.1}% 
\contentsline {subsection}{\numberline {2.1.1}Basic Rules}{2}{subsection.2.1.1}% 
\contentsline {paragraph}{Three Axioms of Probability}{2}{section*.2}% 
\contentsline {paragraph}{Joint Probability}{2}{section*.3}% 
\contentsline {paragraph}{Marginalization}{2}{section*.4}% 
\contentsline {paragraph}{Conditional Probibility}{2}{section*.5}% 
\contentsline {paragraph}{Product Rule}{3}{section*.6}% 
\contentsline {paragraph}{Independence}{3}{section*.7}% 
\contentsline {paragraph}{Baye's Rule}{3}{section*.8}% 
\contentsline {paragraph}{Expectation}{3}{section*.9}% 
\contentsline {subsection}{\numberline {2.1.2}Common Probability Distributions}{4}{subsection.2.1.2}% 
\contentsline {paragraph}{Bernoulli}{4}{section*.11}% 
\contentsline {paragraph}{Binomial}{4}{section*.12}% 
\contentsline {paragraph}{Geometric}{4}{section*.13}% 
\contentsline {paragraph}{Negative Binomial}{5}{section*.14}% 
\contentsline {paragraph}{Beta}{5}{section*.15}% 
\contentsline {paragraph}{Poisson}{5}{section*.16}% 
\contentsline {paragraph}{Exponential}{5}{section*.17}% 
\contentsline {paragraph}{Gamma}{5}{section*.18}% 
\contentsline {paragraph}{Categorical}{6}{section*.20}% 
\contentsline {paragraph}{Dirichlet}{6}{section*.21}% 
\contentsline {paragraph}{Univariate Normal}{6}{section*.22}% 
\contentsline {paragraph}{Normal Inverse Gamma}{6}{section*.23}% 
\contentsline {paragraph}{Multivariate Normal}{7}{section*.24}% 
\contentsline {paragraph}{Normal Inverse Wishart}{7}{section*.25}% 
\contentsline {section}{\numberline {2.2}Linear Algebra}{7}{section.2.2}% 
\contentsline {subsection}{\numberline {2.2.1}Vectors}{7}{subsection.2.2.1}% 
\contentsline {paragraph}{Vectors Addition}{7}{section*.26}% 
\contentsline {paragraph}{Vectors Scaling}{7}{section*.27}% 
\contentsline {paragraph}{Rules for Vectors Addition and Scaling}{7}{section*.28}% 
\contentsline {paragraph}{Linear Combination \& Span}{8}{section*.29}% 
\contentsline {paragraph}{Representation of Basis}{8}{section*.30}% 
\contentsline {paragraph}{Linear Dependence}{8}{section*.31}% 
\contentsline {paragraph}{Dot Products}{9}{section*.32}% 
\contentsline {subsection}{\numberline {2.2.2}Matrices (Square Matrices)}{9}{subsection.2.2.2}% 
\contentsline {paragraph}{Linear Transformation}{9}{section*.33}% 
\contentsline {paragraph}{Basic Formulae}{9}{section*.34}% 
\contentsline {paragraph}{Trace}{9}{section*.35}% 
\contentsline {paragraph}{Determinants}{10}{section*.36}% 
\contentsline {paragraph}{Symmetric Matrix}{10}{section*.37}% 
\contentsline {paragraph}{Orthogonal Matrix}{10}{section*.38}% 
\contentsline {paragraph}{Eigendecomposition}{10}{section*.39}% 
\contentsline {paragraph}{Singular Value Decomposition (SVD)}{11}{section*.40}% 
\contentsline {paragraph}{Moore-Penrose Pseudoinverse}{12}{section*.41}% 
\contentsline {section}{\numberline {2.3}Calculus}{12}{section.2.3}% 
\contentsline {subsection}{\numberline {2.3.1}Differentiation}{12}{subsection.2.3.1}% 
\contentsline {paragraph}{Basic Formulae}{12}{section*.42}% 
\contentsline {paragraph}{Mean Value Theorem}{13}{section*.43}% 
\contentsline {paragraph}{Newton's Method}{13}{section*.44}% 
\contentsline {paragraph}{Taylor Series}{13}{section*.45}% 
\contentsline {subsection}{\numberline {2.3.2}Integration}{13}{subsection.2.3.2}% 
\contentsline {paragraph}{Properties}{13}{section*.46}% 
\contentsline {paragraph}{Average Function Value}{14}{section*.47}% 
\contentsline {paragraph}{Jensen's Inequality}{14}{section*.48}% 
\contentsline {subsection}{\numberline {2.3.3}Multivariate Calculus}{14}{subsection.2.3.3}% 
\contentsline {paragraph}{Partial Derivatives}{14}{section*.49}% 
\contentsline {paragraph}{Clairaut's Theorem}{14}{section*.50}% 
\contentsline {paragraph}{Chain Rule}{14}{section*.51}% 
\contentsline {paragraph}{Gradient}{14}{section*.52}% 
\contentsline {paragraph}{Directional Derivative}{15}{section*.53}% 
\contentsline {paragraph}{Lagrange Multipliers}{15}{section*.54}% 
\contentsline {subsection}{\numberline {2.3.4}Matrix Calculus}{15}{subsection.2.3.4}% 
\contentsline {paragraph}{Vector by Scalar}{15}{section*.55}% 
\contentsline {paragraph}{Scalar by Vector (Gradient)}{15}{section*.56}% 
\contentsline {paragraph}{Vector by Vector (Jacobian Matrix)}{15}{section*.57}% 
\contentsline {paragraph}{Matrix by Scalar}{16}{section*.58}% 
\contentsline {paragraph}{Scalar by Matrix (Gradient Matrix)}{16}{section*.59}% 
\contentsline {subsection}{\numberline {2.3.5}Backpropagation Modules}{16}{subsection.2.3.5}% 
\contentsline {paragraph}{Backpropagation}{16}{section*.60}% 
\contentsline {paragraph}{Linear Module}{16}{section*.61}% 
\contentsline {paragraph}{ReLU Module}{16}{section*.62}% 
\contentsline {paragraph}{Softmax Module}{17}{section*.63}% 
\contentsline {paragraph}{Cross-entropy Module}{17}{section*.64}% 
\contentsline {paragraph}{Cross-entropy with Logits Module}{17}{section*.65}% 
\contentsline {section}{\numberline {2.4}Information Theory}{17}{section.2.4}% 
\contentsline {subsection}{\numberline {2.4.1}Self-information}{17}{subsection.2.4.1}% 
\contentsline {subsection}{\numberline {2.4.2}Entropy}{17}{subsection.2.4.2}% 
\contentsline {subsection}{\numberline {2.4.3}Kullback-Leibler (KL) Divergence}{18}{subsection.2.4.3}% 
\contentsline {subsection}{\numberline {2.4.4}Cross-entropy}{18}{subsection.2.4.4}% 
\contentsline {section}{\numberline {2.5}Optimization}{18}{section.2.5}% 
\contentsline {subsection}{\numberline {2.5.1}One-dimensional Minimization}{18}{subsection.2.5.1}% 
\contentsline {paragraph}{Sufficient Conditions for a Minimum}{18}{section*.66}% 
\contentsline {paragraph}{Rate of Convergence}{19}{section*.67}% 
\contentsline {paragraph}{Brackets}{19}{section*.68}% 
\contentsline {subsection}{\numberline {2.5.2}Gradient Descent}{20}{subsection.2.5.2}% 
\contentsline {paragraph}{Exact Line Search Condition}{20}{section*.69}% 
\contentsline {paragraph}{Gradient Descent}{20}{section*.70}% 
\contentsline {subsection}{\numberline {2.5.3}Quadratic Functions}{20}{subsection.2.5.3}% 
\contentsline {paragraph}{Minimizing Quadratic Functions using Line Search}{20}{section*.71}% 
\contentsline {paragraph}{Conjugate Vectors}{21}{section*.72}% 
\contentsline {paragraph}{Gram-Schmidt Procedure}{21}{section*.73}% 
\contentsline {paragraph}{The Conjugate Vectors Algorithm}{22}{section*.74}% 
\contentsline {paragraph}{The Conjugate Gradients Algorithm}{22}{section*.75}% 
\contentsline {paragraph}{Newton Methods}{24}{section*.76}% 
\contentsline {paragraph}{Quasi-Newton Methods}{24}{section*.77}% 
\contentsline {subsection}{\numberline {2.5.4}General Functions}{25}{subsection.2.5.4}% 
\contentsline {subsection}{\numberline {2.5.5}Optimization with Constraints}{26}{subsection.2.5.5}% 
\contentsline {paragraph}{Constraint Types}{26}{section*.78}% 
\contentsline {paragraph}{Transformation Methods}{26}{section*.79}% 
\contentsline {paragraph}{Lagrange Multipliers (Single Constraint)}{27}{section*.80}% 
\contentsline {paragraph}{Lagrange Multipliers (Multiple Constraints)}{27}{section*.81}% 
\contentsline {paragraph}{Computational Approaches to Constrained Optimization}{27}{section*.82}% 
\contentsline {chapter}{\numberline {3}Machine Learning Basics}{28}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Regularization}{28}{section.3.1}% 
\contentsline {subsection}{\numberline {3.1.1}Under-fitting \& Over-fitting}{28}{subsection.3.1.1}% 
\contentsline {subsection}{\numberline {3.1.2}Bias \& Variance}{28}{subsection.3.1.2}% 
\contentsline {subsection}{\numberline {3.1.3}Vector Norm}{28}{subsection.3.1.3}% 
\contentsline {subsection}{\numberline {3.1.4}Penalize Complexity}{29}{subsection.3.1.4}% 
\contentsline {section}{\numberline {3.2}Cross-Validation}{29}{section.3.2}% 
\contentsline {section}{\numberline {3.3}Bayesian Learning}{29}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Bayes' Rule Terminology}{29}{subsection.3.3.1}% 
\contentsline {subsection}{\numberline {3.3.2}Maximum Likelihood}{30}{subsection.3.3.2}% 
\contentsline {subsection}{\numberline {3.3.3}Maximum a Posterior (MAP)}{30}{subsection.3.3.3}% 
\contentsline {subsection}{\numberline {3.3.4}Bayesian Approach}{30}{subsection.3.3.4}% 
\contentsline {subsection}{\numberline {3.3.5}Example: Univariate Normal Distribution}{31}{subsection.3.3.5}% 
\contentsline {subsection}{\numberline {3.3.6}Example: Categorical Distribution}{33}{subsection.3.3.6}% 
\contentsline {section}{\numberline {3.4}Machine Learning Models}{36}{section.3.4}% 
\contentsline {subsection}{\numberline {3.4.1}Learning and Inference}{36}{subsection.3.4.1}% 
\contentsline {subsection}{\numberline {3.4.2}Three Types of Model}{36}{subsection.3.4.2}% 
\contentsline {subsection}{\numberline {3.4.3}Example: Regression}{37}{subsection.3.4.3}% 
\contentsline {subsection}{\numberline {3.4.4}Example: Classification}{39}{subsection.3.4.4}% 
\contentsline {section}{\numberline {3.5}Overview of Common Algorithms}{40}{section.3.5}% 
\contentsline {chapter}{\numberline {4}Matrix Factorization}{41}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Principal Component Analysis (PCA)}{41}{section.4.1}% 
\contentsline {subsection}{\numberline {4.1.1}The PCA Algorithm}{41}{subsection.4.1.1}% 
\contentsline {subsection}{\numberline {4.1.2}PCA Interpretation}{41}{subsection.4.1.2}% 
\contentsline {section}{\numberline {4.2}Singular Value Decomposition (SVD)}{44}{section.4.2}% 
\contentsline {subsection}{\numberline {4.2.1}The SVD Algorithm}{44}{subsection.4.2.1}% 
\contentsline {subsection}{\numberline {4.2.2}Relation to PCA}{44}{subsection.4.2.2}% 
\contentsline {section}{\numberline {4.3}Non-negative Matrix Factorization ((N)NMF)}{44}{section.4.3}% 
\contentsline {subsection}{\numberline {4.3.1}Standard NMF}{44}{subsection.4.3.1}% 
\contentsline {subsection}{\numberline {4.3.2}Anchor Words Assumption}{45}{subsection.4.3.2}% 
\contentsline {chapter}{\numberline {5}K-Nearest Neighbors}{47}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Simple K-NN}{47}{section.5.1}% 
\contentsline {section}{\numberline {5.2}Fast K-NN Computation}{48}{section.5.2}% 
\contentsline {subsection}{\numberline {5.2.1}Metric Distances Methods}{48}{subsection.5.2.1}% 
\contentsline {paragraph}{Triangle Inequality}{48}{section*.100}% 
\contentsline {paragraph}{Orchard's Algorithm}{48}{section*.101}% 
\contentsline {paragraph}{Approximating and Eliminating Search Algorithm (AESA)}{49}{section*.102}% 
\contentsline {paragraph}{Pre-elimination using Buoys}{49}{section*.103}% 
\contentsline {paragraph}{AESA with Buoys}{49}{section*.104}% 
\contentsline {paragraph}{Orchard with Buoys}{50}{section*.105}% 
\contentsline {subsection}{\numberline {5.2.2}K-dimensional (KD) Tree}{50}{subsection.5.2.2}% 
\contentsline {chapter}{\numberline {6}Linear Regression}{52}{chapter.6}% 
\contentsline {section}{\numberline {6.1}Non-probabilistic Model}{52}{section.6.1}% 
\contentsline {section}{\numberline {6.2}Probabilistic Model}{52}{section.6.2}% 
\contentsline {section}{\numberline {6.3}Bayesian Regression}{53}{section.6.3}% 
\contentsline {section}{\numberline {6.4}Non-linear Regression}{54}{section.6.4}% 
\contentsline {paragraph}{Basic Idea}{54}{section*.106}% 
\contentsline {paragraph}{Polynomial Regression}{54}{section*.107}% 
\contentsline {paragraph}{Radial Basis Functions (RBF)}{54}{section*.108}% 
\contentsline {paragraph}{Arc Tan Functions}{54}{section*.109}% 
\contentsline {section}{\numberline {6.5}Kernel Trick \& Gaussian Processes}{55}{section.6.5}% 
\contentsline {section}{\numberline {6.6}Sparse Linear Regression}{56}{section.6.6}% 
\contentsline {section}{\numberline {6.7}Dual Linear Regression}{58}{section.6.7}% 
\contentsline {section}{\numberline {6.8}Relevance Vector Regression}{59}{section.6.8}% 
\contentsline {chapter}{\numberline {7}Logistic Regression}{60}{chapter.7}% 
\contentsline {section}{\numberline {7.1}Binary Classification}{60}{section.7.1}% 
\contentsline {section}{\numberline {7.2}Bayesian Logistic Regression}{61}{section.7.2}% 
\contentsline {section}{\numberline {7.3}Non-linear Logistic Regression}{62}{section.7.3}% 
\contentsline {section}{\numberline {7.4}Kernel Trick \& Gaussian Process Classification}{62}{section.7.4}% 
\contentsline {section}{\numberline {7.5}Relevance Vector Classification}{63}{section.7.5}% 
\contentsline {section}{\numberline {7.6}Incremental Fitting}{64}{section.7.6}% 
\contentsline {section}{\numberline {7.7}Multi-class Classification}{64}{section.7.7}% 
\contentsline {chapter}{\numberline {8}Support Vector Machines}{65}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Functional \& Geometric Margins}{65}{section.8.1}% 
\contentsline {section}{\numberline {8.2}The Large Margin Principle}{66}{section.8.2}% 
\contentsline {section}{\numberline {8.3}Slack Variables}{68}{section.8.3}% 
\contentsline {section}{\numberline {8.4}Hinge Loss}{71}{section.8.4}% 
\contentsline {section}{\numberline {8.5}Non-linear SVMs \& Kernel Trick}{71}{section.8.5}% 
\contentsline {section}{\numberline {8.6}Sequential Minimal Optimization (SMO)}{72}{section.8.6}% 
\contentsline {chapter}{\numberline {9}Decision Tree \& Ensemble Methods}{73}{chapter.9}% 
\contentsline {section}{\numberline {9.1}Decision Tree}{73}{section.9.1}% 
\contentsline {subsection}{\numberline {9.1.1}Growing a Tree}{73}{subsection.9.1.1}% 
\contentsline {paragraph}{Tree Growing Algorithm}{73}{section*.112}% 
\contentsline {paragraph}{Regression Cost}{74}{section*.113}% 
\contentsline {paragraph}{Classification Cost}{74}{section*.114}% 
\contentsline {subparagraph}{Misclassification Rate}{74}{section*.115}% 
\contentsline {subparagraph}{Entropy}{74}{section*.116}% 
\contentsline {subparagraph}{Gini Index}{75}{section*.117}% 
\contentsline {subsection}{\numberline {9.1.2}Pruning a Tree}{75}{subsection.9.1.2}% 
\contentsline {subsection}{\numberline {9.1.3}Limitation of Decision Tree}{75}{subsection.9.1.3}% 
\contentsline {section}{\numberline {9.2}Bagging}{76}{section.9.2}% 
\contentsline {subsection}{\numberline {9.2.1}Bagging Basics}{76}{subsection.9.2.1}% 
\contentsline {subsection}{\numberline {9.2.2}Random Forest}{77}{subsection.9.2.2}% 
\contentsline {section}{\numberline {9.3}Boosting}{77}{section.9.3}% 
\contentsline {subsection}{\numberline {9.3.1}Boosting Basics}{77}{subsection.9.3.1}% 
\contentsline {subsection}{\numberline {9.3.2}Adaboost}{77}{subsection.9.3.2}% 
\contentsline {subsection}{\numberline {9.3.3}Gradient Boosting}{79}{subsection.9.3.3}% 
\contentsline {chapter}{\numberline {10}Clustering}{80}{chapter.10}% 
\contentsline {section}{\numberline {10.1}K-Means}{80}{section.10.1}% 
\contentsline {subsection}{\numberline {10.1.1}The K-Means Algorithm}{80}{subsection.10.1.1}% 
\contentsline {subsection}{\numberline {10.1.2}K-Means Limitations}{81}{subsection.10.1.2}% 
\contentsline {paragraph}{Hard Assignment}{81}{section*.118}% 
\contentsline {paragraph}{Outlier Sensitivity}{81}{section*.119}% 
\contentsline {paragraph}{Shape/Density Issues}{81}{section*.120}% 
\contentsline {paragraph}{Computational Cost}{81}{section*.121}% 
\contentsline {paragraph}{Representation Sensitivity}{81}{section*.122}% 
\contentsline {paragraph}{Missing Data}{81}{section*.123}% 
\contentsline {section}{\numberline {10.2}Spectral Clustering}{81}{section.10.2}% 
\contentsline {subsection}{\numberline {10.2.1}Similarity Graphs}{81}{subsection.10.2.1}% 
\contentsline {paragraph}{$\epsilon $-Neighborhood Graph}{81}{section*.124}% 
\contentsline {paragraph}{$k$-Nearest Neighbor Graphs}{81}{section*.125}% 
\contentsline {paragraph}{Fully Connected Graph}{82}{section*.126}% 
\contentsline {subsection}{\numberline {10.2.2}Graph Laplacian}{82}{subsection.10.2.2}% 
\contentsline {paragraph}{Basic Graph Definitions}{82}{section*.127}% 
\contentsline {paragraph}{Unnormalized Graph Laplacian}{82}{section*.128}% 
\contentsline {paragraph}{Normalized Graph Laplacian}{83}{section*.129}% 
\contentsline {subsection}{\numberline {10.2.3}Spectral Clustering Algorithms}{83}{subsection.10.2.3}% 
\contentsline {subsection}{\numberline {10.2.4}NCut and RatioCut Approximations}{84}{subsection.10.2.4}% 
\contentsline {subsection}{\numberline {10.2.5}Random Walk Viewpoint}{86}{subsection.10.2.5}% 
\contentsline {chapter}{\numberline {11}Graphical Models}{87}{chapter.11}% 
\contentsline {section}{\numberline {11.1}Graph Definitions}{87}{section.11.1}% 
\contentsline {subsection}{\numberline {11.1.1}Graph}{87}{subsection.11.1.1}% 
\contentsline {paragraph}{Graph}{87}{section*.130}% 
\contentsline {paragraph}{Path}{87}{section*.131}% 
\contentsline {subsection}{\numberline {11.1.2}Directed Graph}{87}{subsection.11.1.2}% 
\contentsline {paragraph}{Directed Graphs}{87}{section*.132}% 
\contentsline {paragraph}{Directed Acyclic Graph (DAG)}{87}{section*.134}% 
\contentsline {paragraph}{Parents and Children}{87}{section*.135}% 
\contentsline {paragraph}{Ancestors and Descendants}{88}{section*.136}% 
\contentsline {subsection}{\numberline {11.1.3}Undirected Graph}{88}{subsection.11.1.3}% 
\contentsline {paragraph}{Undirected Graph}{88}{section*.137}% 
\contentsline {paragraph}{Clique}{88}{section*.139}% 
\contentsline {paragraph}{Maximal Clique}{88}{section*.140}% 
\contentsline {subsection}{\numberline {11.1.4}Connectivity}{88}{subsection.11.1.4}% 
\contentsline {paragraph}{Connected Graph}{88}{section*.141}% 
\contentsline {paragraph}{Connected Components}{88}{section*.142}% 
\contentsline {subsection}{\numberline {11.1.5}Connectedness}{88}{subsection.11.1.5}% 
\contentsline {paragraph}{Singly-connected}{88}{section*.144}% 
\contentsline {paragraph}{Multiply-connected}{89}{section*.145}% 
\contentsline {section}{\numberline {11.2}Belief Networks}{89}{section.11.2}% 
\contentsline {subsection}{\numberline {11.2.1}Definition}{89}{subsection.11.2.1}% 
\contentsline {subsection}{\numberline {11.2.2}Uncertain Evidence}{89}{subsection.11.2.2}% 
\contentsline {paragraph}{Definition}{89}{section*.147}% 
\contentsline {paragraph}{Hard Evidence}{89}{section*.148}% 
\contentsline {paragraph}{Inference}{89}{section*.149}% 
\contentsline {paragraph}{Jeffrey's Rule}{89}{section*.150}% 
\contentsline {subsection}{\numberline {11.2.3}Independence}{90}{subsection.11.2.3}% 
\contentsline {subsection}{\numberline {11.2.4}General Rule for Independence in Belief Networks}{91}{subsection.11.2.4}% 
\contentsline {paragraph}{Independence of $\mathcal {X}$ and $\mathcal {Y}$}{91}{section*.157}% 
\contentsline {paragraph}{d-connected}{91}{section*.158}% 
\contentsline {paragraph}{Separation and Independence}{91}{section*.159}% 
\contentsline {subsection}{\numberline {11.2.5}Markov Equivalence}{92}{subsection.11.2.5}% 
\contentsline {paragraph}{Skeleton}{92}{section*.160}% 
\contentsline {paragraph}{Immorality}{92}{section*.161}% 
\contentsline {paragraph}{Markov Equivalence}{92}{section*.162}% 
\contentsline {section}{\numberline {11.3}Markov Networks}{92}{section.11.3}% 
\contentsline {subsection}{\numberline {11.3.1}Definition}{92}{subsection.11.3.1}% 
\contentsline {subsection}{\numberline {11.3.2}Examples}{92}{subsection.11.3.2}% 
\contentsline {subsection}{\numberline {11.3.3}Independence}{93}{subsection.11.3.3}% 
\contentsline {subsection}{\numberline {11.3.4}Expressiveness of Markov and Belief Networks}{94}{subsection.11.3.4}% 
\contentsline {subsection}{\numberline {11.3.5}Factor Graphs}{95}{subsection.11.3.5}% 
\contentsline {section}{\numberline {11.4}Markov Models}{95}{section.11.4}% 
\contentsline {subsection}{\numberline {11.4.1}Basic Model}{95}{subsection.11.4.1}% 
\contentsline {subsection}{\numberline {11.4.2}Markov Chains}{96}{subsection.11.4.2}% 
\contentsline {subsection}{\numberline {11.4.3}Hidden Markov Models (HMM)}{96}{subsection.11.4.3}% 
\contentsline {section}{\numberline {11.5}Inference}{96}{section.11.5}% 
\contentsline {subsection}{\numberline {11.5.1}Sum-Product Algorithm}{96}{subsection.11.5.1}% 
\contentsline {subsection}{\numberline {11.5.2}Max-Product Algorithm}{98}{subsection.11.5.2}% 
\contentsline {subsection}{\numberline {11.5.3}The Junction Tree Algorithm}{98}{subsection.11.5.3}% 
\contentsline {paragraph}{Clique Graph}{98}{section*.182}% 
\contentsline {paragraph}{Absorption}{99}{section*.183}% 
\contentsline {paragraph}{Running Intersection Property}{100}{section*.184}% 
\contentsline {paragraph}{Constructing a Junction Tree for Singly-Connected Distributions}{100}{section*.185}% 
\contentsline {paragraph}{Constructing a Junction Tree for Multiply-Connected Distributions}{101}{section*.187}% 
\contentsline {paragraph}{The Junction Algorithm}{101}{section*.189}% 
\contentsline {subsection}{\numberline {11.5.4}Graph Cut}{102}{subsection.11.5.4}% 
\contentsline {chapter}{\numberline {12}Approximate Inference}{103}{chapter.12}% 
\contentsline {section}{\numberline {12.1}Sampling}{103}{section.12.1}% 
\contentsline {subsection}{\numberline {12.1.1}Basic Monte Carlo}{103}{subsection.12.1.1}% 
\contentsline {subsection}{\numberline {12.1.2}Importance Sampling}{104}{subsection.12.1.2}% 
\contentsline {subsection}{\numberline {12.1.3}Rejection Sampling}{105}{subsection.12.1.3}% 
\contentsline {subsection}{\numberline {12.1.4}Markov Chain Monte Carlo (MCMC)}{105}{subsection.12.1.4}% 
\contentsline {paragraph}{MCMC Idea}{105}{section*.190}% 
\contentsline {paragraph}{Gibbs Sampling}{106}{section*.191}% 
\contentsline {paragraph}{Metropolis-Hastings Sampling}{106}{section*.192}% 
\contentsline {section}{\numberline {12.2}Expectation Maximization (EM)}{106}{section.12.2}% 
\contentsline {subsection}{\numberline {12.2.1}Hidden Variables}{106}{subsection.12.2.1}% 
\contentsline {subsection}{\numberline {12.2.2}Variational EM}{106}{subsection.12.2.2}% 
\contentsline {paragraph}{Lower Bound}{106}{section*.193}% 
\contentsline {paragraph}{The EM Algorithm}{107}{section*.194}% 
\contentsline {subsection}{\numberline {12.2.3}EM Algorithm for Mixture Models}{108}{subsection.12.2.3}% 
\contentsline {paragraph}{Mixture of Gaussians (MoG)}{108}{section*.195}% 
\contentsline {paragraph}{Student t-distributions}{110}{section*.196}% 
\contentsline {paragraph}{Factor Analysis}{111}{section*.197}% 
\contentsline {section}{\numberline {12.3}Variational Inference}{113}{section.12.3}% 
\contentsline {subsection}{\numberline {12.3.1}Mean Field Approximation}{113}{subsection.12.3.1}% 
\contentsline {subsection}{\numberline {12.3.2}Variational Autoencoders}{113}{subsection.12.3.2}% 
\contentsline {chapter}{\numberline {A}Bayesian Statistics}{114}{appendix.A}% 
\contentsline {section}{\numberline {A.1}Bayesian Inference}{114}{section.A.1}% 
\contentsline {section}{\numberline {A.2}Prior Distributions}{114}{section.A.2}% 
\contentsline {chapter}{\numberline {B}Statistical Assessment}{115}{appendix.B}% 
\contentsline {section}{\numberline {B.1}Hypothesis Testing}{115}{section.B.1}% 
\contentsline {subsection}{\numberline {B.1.1}Testing Basics}{115}{subsection.B.1.1}% 
\contentsline {subsection}{\numberline {B.1.2}Testing Procedure}{115}{subsection.B.1.2}% 
\contentsline {subsection}{\numberline {B.1.3}Power Investigation}{116}{subsection.B.1.3}% 
\contentsline {subsection}{\numberline {B.1.4}Useful Tests}{116}{subsection.B.1.4}% 
\contentsline {section}{\numberline {B.2}Confidence Intervals}{116}{section.B.2}% 
\contentsline {section}{\numberline {B.3}Bootstrap}{116}{section.B.3}% 
