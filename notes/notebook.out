\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 1
\BOOKMARK [1][]{section.1.1}{1.1 About this Notebook}{chapter.1}% 2
\BOOKMARK [1][]{section.1.2}{1.2 Policy of Use}{chapter.1}% 3
\BOOKMARK [0][]{chapter.2}{2 Mathematics Basics}{}% 4
\BOOKMARK [1][]{section.2.1}{2.1 Probability}{chapter.2}% 5
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Basic Rules}{section.2.1}% 6
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 Common Probability Distributions}{section.2.1}% 7
\BOOKMARK [1][]{section.2.2}{2.2 Linear Algebra}{chapter.2}% 8
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 Vectors}{section.2.2}% 9
\BOOKMARK [2][]{subsection.2.2.2}{2.2.2 Matrices \(Square Matrices\)}{section.2.2}% 10
\BOOKMARK [1][]{section.2.3}{2.3 Calculus}{chapter.2}% 11
\BOOKMARK [2][]{subsection.2.3.1}{2.3.1 Differentiation}{section.2.3}% 12
\BOOKMARK [2][]{subsection.2.3.2}{2.3.2 Integration}{section.2.3}% 13
\BOOKMARK [2][]{subsection.2.3.3}{2.3.3 Multivariate Calculus}{section.2.3}% 14
\BOOKMARK [2][]{subsection.2.3.4}{2.3.4 Matrix Calculus}{section.2.3}% 15
\BOOKMARK [2][]{subsection.2.3.5}{2.3.5 Backpropagation Modules}{section.2.3}% 16
\BOOKMARK [1][]{section.2.4}{2.4 Information Theory}{chapter.2}% 17
\BOOKMARK [2][]{subsection.2.4.1}{2.4.1 Self-information}{section.2.4}% 18
\BOOKMARK [2][]{subsection.2.4.2}{2.4.2 Entropy}{section.2.4}% 19
\BOOKMARK [2][]{subsection.2.4.3}{2.4.3 Kullback-Leibler \(KL\) Divergence}{section.2.4}% 20
\BOOKMARK [2][]{subsection.2.4.4}{2.4.4 Cross-entropy}{section.2.4}% 21
\BOOKMARK [1][]{section.2.5}{2.5 Optimization}{chapter.2}% 22
\BOOKMARK [2][]{subsection.2.5.1}{2.5.1 One-dimensional Minimization}{section.2.5}% 23
\BOOKMARK [2][]{subsection.2.5.2}{2.5.2 Gradient Descent}{section.2.5}% 24
\BOOKMARK [2][]{subsection.2.5.3}{2.5.3 Quadratic Functions}{section.2.5}% 25
\BOOKMARK [2][]{subsection.2.5.4}{2.5.4 General Functions}{section.2.5}% 26
\BOOKMARK [2][]{subsection.2.5.5}{2.5.5 Optimization with Constraints}{section.2.5}% 27
\BOOKMARK [0][]{chapter.3}{3 Machine Learning Basics}{}% 28
\BOOKMARK [1][]{section.3.1}{3.1 Regularization}{chapter.3}% 29
\BOOKMARK [2][]{subsection.3.1.1}{3.1.1 Under-fitting \046 Over-fitting}{section.3.1}% 30
\BOOKMARK [2][]{subsection.3.1.2}{3.1.2 Bias \046 Variance}{section.3.1}% 31
\BOOKMARK [2][]{subsection.3.1.3}{3.1.3 Vector Norm}{section.3.1}% 32
\BOOKMARK [2][]{subsection.3.1.4}{3.1.4 Penalize Complexity}{section.3.1}% 33
\BOOKMARK [1][]{section.3.2}{3.2 Cross-Validation}{chapter.3}% 34
\BOOKMARK [1][]{section.3.3}{3.3 Bayesian Learning}{chapter.3}% 35
\BOOKMARK [2][]{subsection.3.3.1}{3.3.1 Bayes' Rule Terminology}{section.3.3}% 36
\BOOKMARK [2][]{subsection.3.3.2}{3.3.2 Maximum Likelihood}{section.3.3}% 37
\BOOKMARK [2][]{subsection.3.3.3}{3.3.3 Maximum a Posterior \(MAP\)}{section.3.3}% 38
\BOOKMARK [2][]{subsection.3.3.4}{3.3.4 Bayesian Approach}{section.3.3}% 39
\BOOKMARK [2][]{subsection.3.3.5}{3.3.5 Example: Univariate Normal Distribution}{section.3.3}% 40
\BOOKMARK [2][]{subsection.3.3.6}{3.3.6 Example: Categorical Distribution}{section.3.3}% 41
\BOOKMARK [1][]{section.3.4}{3.4 Machine Learning Models}{chapter.3}% 42
\BOOKMARK [2][]{subsection.3.4.1}{3.4.1 Learning and Inference}{section.3.4}% 43
\BOOKMARK [2][]{subsection.3.4.2}{3.4.2 Three Types of Model}{section.3.4}% 44
\BOOKMARK [2][]{subsection.3.4.3}{3.4.3 Example: Regression}{section.3.4}% 45
\BOOKMARK [2][]{subsection.3.4.4}{3.4.4 Example: Classification}{section.3.4}% 46
\BOOKMARK [1][]{section.3.5}{3.5 Overview of Common Algorithms}{chapter.3}% 47
\BOOKMARK [0][]{chapter.4}{4 Matrix Factorization}{}% 48
\BOOKMARK [1][]{section.4.1}{4.1 Principal Component Analysis \(PCA\)}{chapter.4}% 49
\BOOKMARK [2][]{subsection.4.1.1}{4.1.1 The PCA Algorithm}{section.4.1}% 50
\BOOKMARK [2][]{subsection.4.1.2}{4.1.2 PCA Interpretation}{section.4.1}% 51
\BOOKMARK [1][]{section.4.2}{4.2 Singular Value Decomposition \(SVD\)}{chapter.4}% 52
\BOOKMARK [2][]{subsection.4.2.1}{4.2.1 The SVD Algorithm}{section.4.2}% 53
\BOOKMARK [2][]{subsection.4.2.2}{4.2.2 Relation to PCA}{section.4.2}% 54
\BOOKMARK [1][]{section.4.3}{4.3 Non-negative Matrix Factorization \(\(N\)NMF\)}{chapter.4}% 55
\BOOKMARK [2][]{subsection.4.3.1}{4.3.1 Standard NMF}{section.4.3}% 56
\BOOKMARK [2][]{subsection.4.3.2}{4.3.2 Anchor Words Assumption}{section.4.3}% 57
\BOOKMARK [0][]{chapter.5}{5 K-Nearest Neighbors}{}% 58
\BOOKMARK [1][]{section.5.1}{5.1 Simple K-NN}{chapter.5}% 59
\BOOKMARK [1][]{section.5.2}{5.2 Fast K-NN Computation}{chapter.5}% 60
\BOOKMARK [2][]{subsection.5.2.1}{5.2.1 Metric Distances Methods}{section.5.2}% 61
\BOOKMARK [2][]{subsection.5.2.2}{5.2.2 K-dimensional \(KD\) Tree}{section.5.2}% 62
\BOOKMARK [0][]{chapter.6}{6 Linear Regression}{}% 63
\BOOKMARK [1][]{section.6.1}{6.1 Non-probabilistic Model}{chapter.6}% 64
\BOOKMARK [1][]{section.6.2}{6.2 Probabilistic Model}{chapter.6}% 65
\BOOKMARK [1][]{section.6.3}{6.3 Bayesian Regression}{chapter.6}% 66
\BOOKMARK [1][]{section.6.4}{6.4 Non-linear Regression}{chapter.6}% 67
\BOOKMARK [1][]{section.6.5}{6.5 Kernel Trick \046 Gaussian Processes}{chapter.6}% 68
\BOOKMARK [1][]{section.6.6}{6.6 Sparse Linear Regression}{chapter.6}% 69
\BOOKMARK [1][]{section.6.7}{6.7 Dual Linear Regression}{chapter.6}% 70
\BOOKMARK [1][]{section.6.8}{6.8 Relevance Vector Regression}{chapter.6}% 71
\BOOKMARK [0][]{chapter.7}{7 Logistic Regression}{}% 72
\BOOKMARK [1][]{section.7.1}{7.1 Binary Classification}{chapter.7}% 73
\BOOKMARK [1][]{section.7.2}{7.2 Bayesian Logistic Regression}{chapter.7}% 74
\BOOKMARK [1][]{section.7.3}{7.3 Non-linear Logistic Regression}{chapter.7}% 75
\BOOKMARK [1][]{section.7.4}{7.4 Kernel Trick \046 Gaussian Process Classification}{chapter.7}% 76
\BOOKMARK [1][]{section.7.5}{7.5 Relevance Vector Classification}{chapter.7}% 77
\BOOKMARK [1][]{section.7.6}{7.6 Incremental Fitting}{chapter.7}% 78
\BOOKMARK [1][]{section.7.7}{7.7 Multi-class Classification}{chapter.7}% 79
\BOOKMARK [0][]{chapter.8}{8 Support Vector Machines}{}% 80
\BOOKMARK [1][]{section.8.1}{8.1 Functional \046 Geometric Margins}{chapter.8}% 81
\BOOKMARK [1][]{section.8.2}{8.2 The Large Margin Principle}{chapter.8}% 82
\BOOKMARK [1][]{section.8.3}{8.3 Slack Variables}{chapter.8}% 83
\BOOKMARK [1][]{section.8.4}{8.4 Hinge Loss}{chapter.8}% 84
\BOOKMARK [1][]{section.8.5}{8.5 Non-linear SVMs \046 Kernel Trick}{chapter.8}% 85
\BOOKMARK [1][]{section.8.6}{8.6 Sequential Minimal Optimization \(SMO\)}{chapter.8}% 86
\BOOKMARK [0][]{chapter.9}{9 Decision Tree \046 Ensemble Methods}{}% 87
\BOOKMARK [1][]{section.9.1}{9.1 Decision Tree}{chapter.9}% 88
\BOOKMARK [2][]{subsection.9.1.1}{9.1.1 Growing a Tree}{section.9.1}% 89
\BOOKMARK [2][]{subsection.9.1.2}{9.1.2 Pruning a Tree}{section.9.1}% 90
\BOOKMARK [2][]{subsection.9.1.3}{9.1.3 Limitation of Decision Tree}{section.9.1}% 91
\BOOKMARK [1][]{section.9.2}{9.2 Bagging}{chapter.9}% 92
\BOOKMARK [2][]{subsection.9.2.1}{9.2.1 Bagging Basics}{section.9.2}% 93
\BOOKMARK [2][]{subsection.9.2.2}{9.2.2 Random Forest}{section.9.2}% 94
\BOOKMARK [1][]{section.9.3}{9.3 Boosting}{chapter.9}% 95
\BOOKMARK [2][]{subsection.9.3.1}{9.3.1 Boosting Basics}{section.9.3}% 96
\BOOKMARK [2][]{subsection.9.3.2}{9.3.2 Adaboost}{section.9.3}% 97
\BOOKMARK [2][]{subsection.9.3.3}{9.3.3 Gradient Boosting}{section.9.3}% 98
\BOOKMARK [0][]{chapter.10}{10 Clustering}{}% 99
\BOOKMARK [1][]{section.10.1}{10.1 K-Means}{chapter.10}% 100
\BOOKMARK [2][]{subsection.10.1.1}{10.1.1 The K-Means Algorithm}{section.10.1}% 101
\BOOKMARK [2][]{subsection.10.1.2}{10.1.2 K-Means Limitations}{section.10.1}% 102
\BOOKMARK [1][]{section.10.2}{10.2 Spectral Clustering}{chapter.10}% 103
\BOOKMARK [2][]{subsection.10.2.1}{10.2.1 Similarity Graphs}{section.10.2}% 104
\BOOKMARK [2][]{subsection.10.2.2}{10.2.2 Graph Laplacian}{section.10.2}% 105
\BOOKMARK [2][]{subsection.10.2.3}{10.2.3 Spectral Clustering Algorithms}{section.10.2}% 106
\BOOKMARK [2][]{subsection.10.2.4}{10.2.4 NCut and RatioCut Approximations}{section.10.2}% 107
\BOOKMARK [2][]{subsection.10.2.5}{10.2.5 Random Walk Viewpoint}{section.10.2}% 108
\BOOKMARK [0][]{chapter.11}{11 Graphical Models}{}% 109
\BOOKMARK [1][]{section.11.1}{11.1 Graph Definitions}{chapter.11}% 110
\BOOKMARK [2][]{subsection.11.1.1}{11.1.1 Graph}{section.11.1}% 111
\BOOKMARK [2][]{subsection.11.1.2}{11.1.2 Directed Graph}{section.11.1}% 112
\BOOKMARK [2][]{subsection.11.1.3}{11.1.3 Undirected Graph}{section.11.1}% 113
\BOOKMARK [2][]{subsection.11.1.4}{11.1.4 Connectivity}{section.11.1}% 114
\BOOKMARK [2][]{subsection.11.1.5}{11.1.5 Connectedness}{section.11.1}% 115
\BOOKMARK [1][]{section.11.2}{11.2 Belief Networks}{chapter.11}% 116
\BOOKMARK [2][]{subsection.11.2.1}{11.2.1 Definition}{section.11.2}% 117
\BOOKMARK [2][]{subsection.11.2.2}{11.2.2 Uncertain Evidence}{section.11.2}% 118
\BOOKMARK [2][]{subsection.11.2.3}{11.2.3 Independence}{section.11.2}% 119
\BOOKMARK [2][]{subsection.11.2.4}{11.2.4 General Rule for Independence in Belief Networks}{section.11.2}% 120
\BOOKMARK [2][]{subsection.11.2.5}{11.2.5 Markov Equivalence}{section.11.2}% 121
\BOOKMARK [1][]{section.11.3}{11.3 Markov Networks}{chapter.11}% 122
\BOOKMARK [2][]{subsection.11.3.1}{11.3.1 Definition}{section.11.3}% 123
\BOOKMARK [2][]{subsection.11.3.2}{11.3.2 Examples}{section.11.3}% 124
\BOOKMARK [2][]{subsection.11.3.3}{11.3.3 Independence}{section.11.3}% 125
\BOOKMARK [2][]{subsection.11.3.4}{11.3.4 Expressiveness of Markov and Belief Networks}{section.11.3}% 126
\BOOKMARK [2][]{subsection.11.3.5}{11.3.5 Factor Graphs}{section.11.3}% 127
\BOOKMARK [1][]{section.11.4}{11.4 Markov Models}{chapter.11}% 128
\BOOKMARK [2][]{subsection.11.4.1}{11.4.1 Basic Model}{section.11.4}% 129
\BOOKMARK [2][]{subsection.11.4.2}{11.4.2 Markov Chains}{section.11.4}% 130
\BOOKMARK [2][]{subsection.11.4.3}{11.4.3 Hidden Markov Models \(HMM\)}{section.11.4}% 131
\BOOKMARK [1][]{section.11.5}{11.5 Inference}{chapter.11}% 132
\BOOKMARK [2][]{subsection.11.5.1}{11.5.1 Sum-Product Algorithm}{section.11.5}% 133
\BOOKMARK [2][]{subsection.11.5.2}{11.5.2 Max-Product Algorithm}{section.11.5}% 134
\BOOKMARK [2][]{subsection.11.5.3}{11.5.3 The Junction Tree Algorithm}{section.11.5}% 135
\BOOKMARK [2][]{subsection.11.5.4}{11.5.4 Graph Cut}{section.11.5}% 136
\BOOKMARK [0][]{chapter.12}{12 Approximate Inference}{}% 137
\BOOKMARK [1][]{section.12.1}{12.1 Sampling}{chapter.12}% 138
\BOOKMARK [2][]{subsection.12.1.1}{12.1.1 Basic Monte Carlo}{section.12.1}% 139
\BOOKMARK [2][]{subsection.12.1.2}{12.1.2 Importance Sampling}{section.12.1}% 140
\BOOKMARK [2][]{subsection.12.1.3}{12.1.3 Rejection Sampling}{section.12.1}% 141
\BOOKMARK [2][]{subsection.12.1.4}{12.1.4 Markov Chain Monte Carlo \(MCMC\)}{section.12.1}% 142
\BOOKMARK [1][]{section.12.2}{12.2 Expectation Maximization \(EM\)}{chapter.12}% 143
\BOOKMARK [2][]{subsection.12.2.1}{12.2.1 Hidden Variables}{section.12.2}% 144
\BOOKMARK [2][]{subsection.12.2.2}{12.2.2 Variational EM}{section.12.2}% 145
\BOOKMARK [2][]{subsection.12.2.3}{12.2.3 EM Algorithm for Mixture Models}{section.12.2}% 146
\BOOKMARK [1][]{section.12.3}{12.3 Variational Inference}{chapter.12}% 147
\BOOKMARK [2][]{subsection.12.3.1}{12.3.1 Mean Field Approximation}{section.12.3}% 148
\BOOKMARK [2][]{subsection.12.3.2}{12.3.2 Variational Autoencoders}{section.12.3}% 149
\BOOKMARK [0][]{appendix.A}{A Bayesian Statistics}{}% 150
\BOOKMARK [1][]{section.A.1}{A.1 Bayesian Inference}{appendix.A}% 151
\BOOKMARK [1][]{section.A.2}{A.2 Prior Distributions}{appendix.A}% 152
\BOOKMARK [0][]{appendix.B}{B Statistical Assessment}{}% 153
\BOOKMARK [1][]{section.B.1}{B.1 Hypothesis Testing}{appendix.B}% 154
\BOOKMARK [2][]{subsection.B.1.1}{B.1.1 Testing Basics}{section.B.1}% 155
\BOOKMARK [2][]{subsection.B.1.2}{B.1.2 Testing Procedure}{section.B.1}% 156
\BOOKMARK [2][]{subsection.B.1.3}{B.1.3 Power Investigation}{section.B.1}% 157
\BOOKMARK [2][]{subsection.B.1.4}{B.1.4 Useful Tests}{section.B.1}% 158
\BOOKMARK [1][]{section.B.2}{B.2 Confidence Intervals}{appendix.B}% 159
\BOOKMARK [1][]{section.B.3}{B.3 Bootstrap}{appendix.B}% 160
