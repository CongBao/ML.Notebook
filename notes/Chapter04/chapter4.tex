%!TEX root = ../notebook.tex
% Chapter 4

\chapter{Matrix Factorization}
\label{chapter4}

\section{Principal Component Analysis (PCA)}

\subsection{The PCA Algorithm}

The goal of PCA is to use an orthogonal transformation $\mat{W}\T\in\bR^{L\times D}$ to convert the data $\mat{X}\in\bR^{N\times D}$ into a lower dimension set $\mat{Z}\in\bR^{N\times L}$. The matrix $\mat{W}$ is a set of orthogonal basis vectors of principle directions. Our goal is to minimize the reconstruction error $\cL(\mat{W},\mat{Z})=\norm{\mat{X}\T-\mat{W}\mat{Z}\T}_F^2$. The optimal solution is obtained by setting $\mat{W}=\mat{Q}_L$, where $\mat{Q}_L$ contains the $L$ eigenvectors with largest eigenvalues of the empirical covariance matrix $\mat{\Si}=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\vec{x}_i\TT$ (assume $\vec{x}_i$ has zero mean). Hence the optimal low-dimensional encoding of data is given by $\vec{z}_i=\mat{W}\T\vec{x}_i$.

The PCA algorithm can be concluded as following steps:
\begin{enumerate}
	\item Get the data $\mat{X}\in\bR^{N\times D}$.
	\item Zero-mean the data $\vec{x}_j\gets\vec{x}_j-\frac{1}{N}\sum_{i=1}^N\vec{x}_i,j=1,\dotsc,D$.
	\item Calculate the covariance matrix $\mat{\Si}=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\vec{x}_i\TT$.
	\item Apply eigendecomposition on covariance matrix $\mat{\Si}=\mat{Q}\mat{\La}\mat{Q}\T$.
	\item Select the eigenvectors with largest $L$ eigenvalues $\mat{Q}_L\in\bR^{L\times D}$ and apply to the data $\mat{Z}=\left(\mat{Q}_L\mat{X}\T\right)\T$.
\end{enumerate}

\subsection{PCA Interpretation}

We use $\vec{w}_j\in\bR^D$ to denote the $j\nth$ principal direction, $\vec{x}_i\in\bR^D$ to denote the $i\nth$ high-dimensional observation, $\vec{z}_i\in\bR^L$ to denote the $i\nth$ low-dimensional representation, and $\ti{\vec{z}}_1\in\bR^N$ to denote the $[z_{1j}\ \cdots\ z_{Nj}]$, which is the $j\nth$ component of all the low-dimensional vectors. Besides, since $\vec{w}_j$ is orthogonal basis vector, we have $\vec{w}_j\TT\vec{w}_j=1$ and $\norm{\vec{w}_j}=1$.

We start by estimating the best one-dimension solution, $\vec{w}_1\in\bR^D$, and the corresponding projected points $\ti{\vec{z}}_1\in\bR^N$. The reconstruction error is given by
\begin{align*}
	\cL(\vec{w}_1,\vec{z}_1)&=\frac{1}{N}\sum_{i=1}^N\norm{\vec{x}_i-z_{i1}\vec{w}_1}^2 \\
	&=\frac{1}{N}\sum_{i=1}^N(\vec{x}_i-z_{i1}\vec{w}_1)\T(\vec{x}_i-z_{i1}\vec{w}_1) \\
	&=\frac{1}{N}\sum_{i=1}^N\left(\vec{x}_i\TT\vec{x}_i-2z_{i1}\vec{w}_1\TT\vec{x}_i+z_{i1}^2\vec{w}_1\TT\vec{w}_1\right) \\
	&=\frac{1}{N}\sum_{i=1}^N\left(\vec{x}_i\TT\vec{x}_i-2z_{i1}\vec{w}_1\TT\vec{x}_i+z_{i1}^2\right)
\end{align*}
Taking derivatives w.r.t. $z_{i1}$ and equating to zero gives
\begin{align*}
	\frac{\pa}{\pa z_{i1}}\cL(\vec{w}_1,\vec{z}_1)=\frac{1}{N}\left(-2\vec{w}_1\TT\vec{x}_i+2z_{i1}\right)=0\impl z_{i1}=\vec{w}_1\TT\vec{x}_i
\end{align*}
So the optimal reconstruction weights are obtained by orthogonally projecting the data onto the first principle direction, $\vec{w}_1$. Plugging back in gives
\begin{align*}
	\cL(\vec{w}_1)&=\frac{1}{N}\sum_{i=1}^N\left(\vec{x}_i\TT\vec{x}_i-z_{i1}^2\right) \\
	&\prop-\frac{1}{N}\sum_{i=1}^Nz_{i1}^2
\end{align*}
The variance of the projected coordinates is given by
\begin{align*}
	\V{\ti{\vec{z}}_1}=\E{\ti{\vec{z}}_1^2}-\left(\E{\ti{\vec{z}}_1}\right)^2=\frac{1}{N}\sum_{i=1}^Nz_{i1}^2
\end{align*}
since $\E{z_{i1}}=\E{\vec{x}_1\TT\vec{w}_1}=\E{\vec{x}_i}\T\vec{w}_1=0$ as the data has zero mean. From this, we see that minimizing the reconstruction error is equivalent to maximizing the variance of the projected data, i.e.
\begin{align*}
	\argmin_{\vec{w}_1}\cL(\vec{w}_1)=\argmax_{\vec{w}_1}\V{\ti{\vec{z}}_1}
\end{align*}
Further, the variance of the projected data can be written as
\begin{align*}
	\V{\ti{\vec{z}}_1}=\frac{1}{N}\sum_{i=1}^Nz_{i1}^2=\frac{1}{N}\sum_{i=1}^N\vec{w}_1\TT\vec{x}_i\vec{x}_i\TT\vec{w}_1=\vec{w}_1\TT\mat{\Si}\vec{w}_1
\end{align*}
where $\mat{\Si}$ is the covariance matrix of the data. We then maximize the object function with the constraint $\norm{\vec{w}_1}=1$ with Lagrange multiplier, that is
\begin{align*}
	\ti{\cL}(\vec{w}_1)&=\vec{w}_1\TT\mat{\Si}\vec{w}_1+\la_1\left(\vec{w}_1\TT\vec{w}_1-1\right) \\
	\frac{\pa}{\pa\vec{w}_1}\ti{\cL}(\vec{w}_1)&=2\mat{\Si}\vec{w}_1-2\la_1\vec{w}_1=0 \\
	\mat{\Si}\vec{w}_1&=\la_1\vec{w}_1
\end{align*}
Hence the direction that maximize the variance is an eigenvector of the covariance matrix. We can calculate the variance of the projected data as $\vec{w}_1\TT\mat{\Si}\vec{w}_1=\la_1$. Since we want to maximize the variance, we pick the eigenvector which corresponds to the largest eigenvalue.

Now consider to find another direction $\vec{w}_2$ to further minimize the reconstruction error, subject to $\vec{w}_1\TT\vec{w}_2=0$ and $\vec{w}_2\TT\vec{w}_2=1$. The error is
\begin{align*}
	\cL(\vec{w}_1,\vec{z}_1,\vec{w}_2,\vec{z}_2)=\frac{1}{N}\sum_{i=1}^N\norm{\vec{x}_i-z_{i1}\vec{w}_1-z_{i2}\vec{w}_2}^2
\end{align*}
Optimizing w.r.t. $\vec{W}_1$ and $\vec{z}_1$ gives the same solution as before. In other words, the second principle encoding is gotten by projecting onto the second principal direction. Substituting in yields
\begin{align*}
	\cL(\vec{w}_2)&=\frac{1}{N}\sum_{i=1}^N\left(\vec{x}_i\TT\vec{x}_i-\vec{w}_1\TT\vec{x}_i\vec{x}_i\TT\vec{w}_1-\vec{w}_2\TT\vec{x}_i\vec{x}_i\TT\vec{w}_2\right) \\
	&\prop-\vec{w}_2\TT\mat{\Si}\vec{w}_2
\end{align*}
Adding the constraints and solving yields
\begin{align*}
	\ti{\cL}(\vec{w}_2)&=-\vec{w}_2\TT\mat{\Si}\vec{w}_2+\la_2\left(\vec{w}_2\TT\vec{w}_2-1\right)+\la_2'\left(\vec{w}_2\TT\vec{w}_1-0\right) \\
	\mat{\Si}\vec{w}_2&=\la_2\vec{w}_2
\end{align*}
The proof continues in this way.

\section{Singular Value Decomposition (SVD)}

\subsection{The SVD Algorithm}

SVD decomposes a data matrix $\mat{X}\in\bR^{N\times D}$ into $\mat{X}=\mat{U}\mat{\Si}\mat{V}\T$, where $\mat{U}\in\bR^{N\times N}$ is an orthogonal matrix (columns of $\mat{U}$ are called left singular vectors), $\mat{V}\in\bR^{D\times D}$ is an orthogonal matrix (columns of $\mat{V}$ are called right singular vectors), and $\mat{\Si}\in\bR^{N\times D}$ is a diagonal matrix, with non-negative diagonal entries, sorted from high to low (these are called singular values).

When using SVD for dimensionality reduction, we get the largest $L$ singular values from $\mat{\Si}$ to construct a diagonal matrix $\mat{\Si}_L\in\bR^{L\times L}$. Also, we get the corresponding left singular vectors from $\mat{U}$ to construct a matrix $\mat{U}_L\in\bR^{N\times L}$. Hence the low-dimensional matrix $\mat{Z}\in\bR^{N\times L}$ can be constructed by $\mat{Z}=\mat{U}_L\mat{\Si}_L$.

When using SVD for low-rank approximation, we still construct the same matrices $\mat{\Si}_L\in\bR^{L\times L}$ and $\mat{U}_L\in\bR^{N\times L}$. In addition, we construct another matrix $\mat{V}_L$ by getting corresponding right singular vectors from $\mat{V}$. Hence the reconstructed low-rank matrix can be constructed as $\mat{X}_L=\mat{U}_L\mat{\Si}_L\mat{V}_L\TT$.

\subsection{Relation to PCA}

In PCA, we have to compute eigenvectors and eigenvalues of the covariance matrix $\mat{X}\T\mat{X}$:
\begin{align*}
	\mat{X}\T\mat{X}=\mat{Q}\mat{\La}\mat{Q}\T
\end{align*}
This means if $\mat{X}=\mat{U}\mat{\Si}\mat{V}\T$, then
\begin{align*}
	\mat{X}\T\mat{X}=\mat{V}\mat{\Si}\T\mat{U}\T\mat{U}\mat{\Si}\mat{V}\T=\mat{V}\mat{\La}\mat{V}\T
\end{align*}
where $\mat{\La}$ is a diagonal matrix with diagonals equal to the squares of diagonal entries of $\mat{\Si}$. This means that the eigenvectors of $\mat{X}\T\mat{X}$ (principal dimensions) are the same as right singular vectors of $\mat{X}$.

\section{Non-negative Matrix Factorization ((N)NMF)}

\subsection{Standard NMF}

The idea of NMF is that for a matrix $\mat{X}$ of size $m\times n$ the goal is to express $\mat{X}$, at least approximately as a product of non-negative $m\times k$ and $k\times n$ matrices $\mat{W}$ and $\mat{H}$ that
\begin{align*}
	\mat{M}\approx\mat{W}\mat{H}
\end{align*}
The objective function to minimize is
\begin{align*}
	\cL(\mat{W},\mat{H})=\norm{\mat{X}-\mat{W}\mat{H}}_F^2,\mat{W}\geq 0,\mat{H}\geq 0
\end{align*}
The standard framework is to solve problems in $\mat{W}$ and $\mat{H}$ iteratively that fix $\mat{H}$, optimize for $\mat{W}$ or fix $\mat{W}$, optimize for $\mat{H}$. A simple iterative algorithm called \emph{multiplicative updates} is proposed as
\begin{align*}
	\mat{H}&\gets\mat{H}\odot\frac{\left[\mat{W}\T\mat{X}\right]}{\left[\mat{W}\T\mat{W}\mat{H}\right]} \\
	\mat{W}&\gets\mat{W}\odot\frac{\left[\mat{X}\mat{H}\T\right]}{\left[\mat{W}\mat{H}\mat{H}\T\right]}
\end{align*}
where $\frac{[]}{[]}$ is element-wise division and $\odot$ is element-wise multiplication. The updates can be viewed as rescaled gradient method. We can write the formulae as
\begin{align*}
	\mat{H}&\gets\mat{H}-\frac{\left[\mat{H}\right]}{\left[\mat{W}\T\mat{W}\mat{H}\right]}\odot\grad_{\vec{H}}\cL,\quad\grad_{\vec{H}}\cL=\mat{W}\T\mat{W}\mat{H}-\mat{W}\T\mat{X} \\
	\mat{W}&\gets\mat{W}-\frac{\left[\mat{W}\right]}{\left[\mat{W}\mat{H}\mat{H}\T\right]}\odot\grad_{\mat{W}}\cL,\quad\grad_{\mat{W}}\cL=\mat{W}\mat{H}\mat{H}\T-\mat{X}\mat{H}\T
\end{align*}

Another simple algorithm is \emph{alternating least squares (ALS)}:
\begin{align*}
	\mat{W}\gets\max\left(0,\frac{\mat{X}\mat{H}\T}{\mat{H}\mat{H}\T}\right)
\end{align*}
or the \emph{hierarchical alternating least squares (HALS)}:
\begin{align*}
	\mat{W}[:,i]\gets\max\left(0,\frac{\mat{X}\vec{h}_i\TT-\sum_{k\neq i}\mat{W}[:,i]\vec{h}_k\vec{h}_i\TT}{\norm{\vec{h}_i}}^2\right)
\end{align*}
where $\mat{W}[:,i]$ is the $i\nth$ column of $\mat{W}$ and $\vec{h}_i$ is the $i\nth$ row of $\mat{H}$.

\subsection{Anchor Words Assumption}
